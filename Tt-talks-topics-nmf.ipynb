{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEDtalk NMF Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also this for suggestions: http://scikit-learn.org/dev/auto_examples/applications/topics_extraction_with_nmf_lda.html#example-applications-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "More useful discussion of NMF-LDA on this [SO thread][so].\n",
    "\n",
    "[so]: http://stackoverflow.com/questions/35140117/how-to-interpret-lda-components-using-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "colnames = ['author', 'title', 'date' , 'length', 'text']\n",
    "data = pandas.read_csv('./data/talks-v1b.csv', names=colnames)\n",
    "\n",
    "# Creating 3 lists of relevant data.\n",
    "# Importing everything here. \n",
    "# If we want to test, we should import 2006-2015 and test on 2016.\n",
    "\n",
    "talks = data.text.tolist()\n",
    "authors = data.author.tolist()\n",
    "dates = data.date.tolist()\n",
    "\n",
    "# Getting only the years from dates list\n",
    "years = [re.sub('[A-Za-z ]', '', item) for item in dates]\n",
    "\n",
    "# Combining year with presenter for citation\n",
    "authordate = [author+\" \"+year for author, year in zip(authors, years)]\n",
    "\n",
    "# We need to remove the \"empty\" talks from both lists.\n",
    "\n",
    "# We establish which talks are empty\n",
    "i = 0\n",
    "no_good = []\n",
    "for talk in talks: \n",
    "    A = type(talk)\n",
    "    B = type('string or something')\n",
    "    if A != B:\n",
    "        no_good.append(i)\n",
    "    i = i + 1\n",
    "\n",
    "# Now we delete them in reverse order so as to preserve index order\n",
    "for index in sorted(no_good, reverse=True):\n",
    "    del talks[index]\n",
    "for index in sorted(no_good, reverse=True):\n",
    "    del authordate[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block of code below produces a list saved as an `np.array`, of 7254 words and a **document term matrix**, `dtm.shape`, of `(2106, 7254)`. The `CountVectorizer` does a lot of work, and it has the following parameters:\n",
    "\n",
    "* `stop_words` specifies which set to use. (The English words are the same as the Glasgow Information Retrieval Group. See link on [GitHub][].)\n",
    "* `lowercase` (default `True`) convert all text to lowercase before tokenizing\n",
    "* `min_df` (default 1) remove terms from the vocabulary that occur in fewer than min_df documents (in a large corpus this may be set to 15 or higher to eliminate very rare words)\n",
    "vocabulary ignore words that do not appear in the provided list of words\n",
    "* `token_pattern` (default `u'(?u)\\b\\w\\w+\\b'`) regular expression identifying tokens–by default words that consist of a single character (e.g., ‘a’, ‘2’) are ignored, setting `token_pattern` to `'(?u)\\b\\w+\\b'` will include these tokens\n",
    "* `tokenizer` (default unused) use a custom function for tokenizing\n",
    "\n",
    "[GitHub]: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Function for printing topic words (used later):\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_id, topic in enumerate(model.components_):\n",
    "        print('\\nTopic {}:'.format(int(topic_id))) \n",
    "        print(''.join([feature_names[i] + ' ' + str(round(topic[i], 2))\n",
    "              +', ' for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "n_samples = len(talks)\n",
    "n_features = 1000\n",
    "n_topics = 35\n",
    "n_top_words = 20\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "tfidf_vectorizer = text.TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(talks)\n",
    "tf_vectorizer = text.CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "tf = tf_vectorizer.fit_transform(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with tf-idf features, n_samples=2106 and n_features=1000...\n",
      "\n",
      "Topics in NMF model:\n",
      "\n",
      "Topic 0:\n",
      "actually 0.93, really 0.89, just 0.88, think 0.7, things 0.68, going 0.65, way 0.59, time 0.58, ve 0.57, make 0.52, little 0.48, kind 0.48, different 0.45, thing 0.45, look 0.45, right 0.45, new 0.44, world 0.43, work 0.42, use 0.39, \n",
      "\n",
      "Topic 1:\n",
      "said 0.7, life 0.67, story 0.5, day 0.47, man 0.45, time 0.43, father 0.43, years 0.43, family 0.39, went 0.39, mother 0.37, love 0.37, stories 0.36, did 0.35, home 0.34, told 0.34, people 0.34, old 0.33, didn 0.32, came 0.32, \n",
      "\n",
      "Topic 2:\n",
      "world 0.67, countries 0.62, country 0.5, global 0.47, percent 0.47, government 0.46, china 0.42, dollars 0.4, economic 0.39, economy 0.37, money 0.36, states 0.35, india 0.34, democracy 0.34, growth 0.33, billion 0.33, people 0.33, united 0.31, political 0.31, need 0.29, \n",
      "\n",
      "Topic 3:\n",
      "patients 1.11, health 0.98, patient 0.8, disease 0.63, care 0.6, medical 0.54, doctors 0.45, medicine 0.38, doctor 0.37, hospital 0.37, heart 0.37, treatment 0.3, diseases 0.25, blood 0.18, drugs 0.18, pain 0.16, drug 0.16, healthy 0.15, need 0.15, risk 0.14, \n",
      "\n",
      "Topic 4:\n",
      "universe 1.96, stars 0.44, space 0.44, light 0.4, theory 0.37, dark 0.37, physics 0.33, black 0.23, billion 0.21, energy 0.19, science 0.18, matter 0.18, sun 0.15, mass 0.14, big 0.14, structure 0.12, earth 0.12, field 0.11, star 0.08, years 0.08, \n",
      "\n",
      "Topic 5:\n",
      "women 2.74, men 1.1, girls 0.49, woman 0.46, female 0.28, sex 0.28, violence 0.18, girl 0.17, rights 0.12, young 0.11, talk 0.09, stories 0.08, media 0.08, daughter 0.08, country 0.08, issues 0.07, percent 0.07, mother 0.07, role 0.07, peace 0.06, \n",
      "\n",
      "Topic 6:\n",
      "cancer 2.46, drug 0.23, disease 0.2, body 0.19, blood 0.15, treatment 0.11, gene 0.09, percent 0.07, lab 0.07, drugs 0.07, genetic 0.07, research 0.05, patient 0.05, growth 0.05, genes 0.05, medicine 0.05, field 0.05, cell 0.05, doctor 0.04, normal 0.04, \n",
      "\n",
      "Topic 7:\n",
      "brain 2.82, brains 0.38, activity 0.19, mental 0.17, human 0.17, body 0.16, memory 0.14, mind 0.13, behavior 0.13, visual 0.11, pain 0.11, control 0.11, going 0.11, genes 0.1, normal 0.1, animal 0.1, study 0.1, region 0.1, areas 0.09, inside 0.09, \n",
      "\n",
      "Topic 8:\n",
      "building 1.42, buildings 0.81, architecture 0.78, space 0.65, project 0.36, built 0.31, build 0.28, air 0.25, site 0.22, house 0.22, materials 0.2, public 0.18, structure 0.17, light 0.16, material 0.15, center 0.15, floor 0.14, city 0.14, design 0.12, projects 0.12, \n",
      "\n",
      "Topic 9:\n",
      "ca 2.2, yeah 0.23, mean 0.18, chris 0.15, think 0.12, cause 0.1, got 0.08, ted 0.06, yes 0.05, ok 0.04, ve 0.04, thank 0.03, amazing 0.03, come 0.03, community 0.03, source 0.03, just 0.02, code 0.02, guess 0.02, happened 0.02, \n",
      "\n",
      "Topic 10:\n",
      "kids 1.32, school 1.2, children 0.97, students 0.77, education 0.71, teachers 0.68, schools 0.42, child 0.37, teacher 0.37, learning 0.35, parents 0.3, teach 0.29, girls 0.24, learn 0.23, class 0.23, kid 0.22, student 0.2, math 0.15, year 0.15, high 0.13, \n",
      "\n",
      "Topic 11:\n",
      "robot 1.89, robots 1.2, legs 0.12, video 0.09, lab 0.07, animal 0.06, build 0.06, control 0.06, foot 0.05, want 0.05, body 0.04, machines 0.04, small 0.04, intelligence 0.04, technology 0.04, moving 0.04, war 0.03, play 0.03, second 0.03, screen 0.03, \n",
      "\n",
      "Topic 12:\n",
      "music 2.57, play 0.3, playing 0.24, song 0.23, piece 0.22, hear 0.18, listen 0.1, thank 0.09, video 0.08, experience 0.07, yeah 0.06, heard 0.06, audience 0.06, beautiful 0.05, kind 0.05, child 0.05, sounds 0.05, performance 0.04, young 0.04, traditional 0.04, \n",
      "\n",
      "Topic 13:\n",
      "ocean 1.6, fish 1.0, sea 0.88, animals 0.3, deep 0.27, ice 0.25, blue 0.19, light 0.17, north 0.17, water 0.15, surface 0.13, climate 0.13, species 0.13, planet 0.13, miles 0.12, places 0.11, feet 0.11, area 0.1, land 0.1, areas 0.1, \n",
      "\n",
      "Topic 14:\n",
      "cells 2.29, cell 0.78, body 0.28, disease 0.21, drug 0.18, lab 0.17, blood 0.16, skin 0.14, diseases 0.13, heart 0.12, grow 0.11, patient 0.11, animal 0.1, patients 0.1, drugs 0.1, material 0.09, environment 0.09, actually 0.09, human 0.09, medicine 0.08, \n",
      "\n",
      "Topic 15:\n",
      "city 2.01, cities 1.37, map 0.32, new 0.31, york 0.31, cars 0.28, street 0.25, park 0.22, car 0.18, public 0.17, community 0.16, people 0.15, places 0.15, citizens 0.13, live 0.12, site 0.12, communities 0.11, network 0.11, square 0.1, built 0.1, \n",
      "\n",
      "Topic 16:\n",
      "play 1.38, game 1.28, games 1.11, video 0.52, playing 0.38, win 0.1, fun 0.1, thank 0.09, physical 0.09, world 0.07, real 0.07, activity 0.06, right 0.04, action 0.04, excited 0.04, social 0.04, online 0.04, seconds 0.04, computer 0.03, feel 0.03, \n",
      "\n",
      "Topic 17:\n",
      "language 1.58, english 1.08, word 0.61, words 0.58, speak 0.22, chinese 0.21, books 0.17, say 0.16, writing 0.15, example 0.14, read 0.14, write 0.13, learn 0.1, text 0.09, meaning 0.08, think 0.08, speaking 0.08, learning 0.08, reading 0.07, book 0.07, \n",
      "\n",
      "Topic 18:\n",
      "water 2.7, river 0.32, waste 0.23, bacteria 0.17, ice 0.13, use 0.12, surface 0.11, clean 0.11, material 0.11, ve 0.11, environment 0.11, india 0.11, air 0.1, south 0.09, plant 0.08, green 0.07, feet 0.07, sea 0.07, body 0.06, solution 0.06, \n",
      "\n",
      "Topic 19:\n",
      "africa 2.2, african 0.88, south 0.28, aid 0.21, countries 0.21, leaders 0.15, country 0.09, world 0.08, market 0.08, east 0.07, story 0.06, traditional 0.06, west 0.05, poverty 0.05, map 0.05, oil 0.05, percent 0.05, generation 0.05, bank 0.05, hiv 0.05, \n",
      "\n",
      "Topic 20:\n",
      "people 3.38, think 0.43, money 0.32, social 0.29, person 0.28, don 0.28, percent 0.25, want 0.25, things 0.24, group 0.24, moral 0.22, lot 0.18, somebody 0.18, say 0.18, study 0.16, trust 0.16, change 0.16, community 0.15, doing 0.14, lives 0.14, \n",
      "\n",
      "Topic 21:\n",
      "animals 1.12, species 1.05, forest 0.86, plants 0.67, trees 0.61, animal 0.43, tree 0.42, plant 0.41, nature 0.33, human 0.27, carbon 0.24, humans 0.23, land 0.2, planet 0.18, female 0.14, river 0.13, natural 0.13, years 0.13, grow 0.12, climate 0.12, \n",
      "\n",
      "Topic 22:\n",
      "hiv 1.69, drugs 0.63, drug 0.44, treatment 0.29, sex 0.22, countries 0.2, positive 0.11, developing 0.07, available 0.06, disease 0.06, risk 0.05, low 0.05, research 0.05, high 0.05, effective 0.05, diseases 0.05, current 0.05, new 0.04, africa 0.04, baby 0.03, \n",
      "\n",
      "Topic 23:\n",
      "data 2.84, web 0.17, map 0.16, information 0.12, points 0.11, numbers 0.1, look 0.09, text 0.08, patterns 0.08, using 0.07, google 0.07, percent 0.06, car 0.06, computer 0.06, light 0.06, science 0.05, rate 0.05, online 0.04, use 0.04, shows 0.04, \n",
      "\n",
      "Topic 24:\n",
      "compassion 1.84, god 0.42, beings 0.11, world 0.11, human 0.11, happy 0.11, love 0.1, says 0.09, self 0.08, happiness 0.07, person 0.07, word 0.06, zero 0.06, feel 0.05, dog 0.05, good 0.05, non 0.05, leaders 0.05, said 0.05, man 0.05, \n",
      "\n",
      "Topic 25:\n",
      "earth 1.23, planet 1.0, mars 0.9, solar 0.5, ice 0.47, sun 0.43, life 0.41, stars 0.4, surface 0.35, star 0.28, space 0.28, years 0.21, miles 0.18, climate 0.14, size 0.12, science 0.12, fly 0.12, light 0.12, billion 0.11, picture 0.11, \n",
      "\n",
      "Topic 26:\n",
      "energy 1.64, oil 1.0, nuclear 0.41, climate 0.39, solar 0.38, carbon 0.36, power 0.36, electricity 0.35, gas 0.35, wind 0.28, cars 0.21, use 0.2, percent 0.19, air 0.17, car 0.17, cost 0.15, billion 0.15, need 0.14, technology 0.13, stuff 0.13, \n",
      "\n",
      "Topic 27:\n",
      "sound 1.93, voice 0.67, listening 0.5, sounds 0.44, hear 0.35, song 0.27, listen 0.23, color 0.12, laughter 0.12, play 0.11, visual 0.1, just 0.09, eyes 0.08, video 0.08, time 0.07, body 0.07, heard 0.06, applause 0.06, health 0.05, use 0.05, \n",
      "\n",
      "Topic 28:\n",
      "know 1.43, going 1.01, said 0.9, don 0.85, just 0.69, got 0.68, right 0.57, say 0.55, oh 0.52, want 0.51, ve 0.49, yeah 0.47, think 0.46, ll 0.45, thing 0.42, laughter 0.4, guy 0.39, ok 0.39, really 0.38, little 0.36, \n",
      "\n",
      "Topic 29:\n",
      "food 2.17, eat 0.66, plant 0.4, plants 0.31, feed 0.3, waste 0.25, fish 0.18, healthy 0.14, kids 0.13, growing 0.12, percent 0.12, local 0.11, know 0.11, grow 0.1, produce 0.09, need 0.08, land 0.07, green 0.07, really 0.07, america 0.07, \n",
      "\n",
      "Topic 30:\n",
      "machine 1.85, computer 0.97, machines 0.62, computers 0.47, technology 0.24, human 0.21, intelligence 0.15, learning 0.14, built 0.12, memory 0.11, software 0.11, build 0.1, web 0.08, using 0.08, program 0.07, code 0.07, digital 0.06, robots 0.06, running 0.06, humans 0.06, \n",
      "\n",
      "Topic 31:\n",
      "dna 1.78, bacteria 0.61, genes 0.54, gene 0.51, genetic 0.41, cell 0.25, code 0.24, species 0.24, biology 0.17, evolution 0.12, science 0.1, years 0.08, technology 0.08, understand 0.08, bodies 0.07, human 0.07, program 0.07, long 0.06, million 0.06, billion 0.05, \n",
      "\n",
      "Topic 32:\n",
      "internet 1.52, media 0.73, web 0.65, online 0.63, page 0.36, digital 0.36, government 0.31, video 0.31, content 0.29, network 0.26, google 0.26, social 0.23, technology 0.21, news 0.15, website 0.14, software 0.14, phone 0.14, world 0.13, companies 0.13, computer 0.12, \n",
      "\n",
      "Topic 33:\n",
      "design 2.57, designed 0.3, products 0.15, product 0.15, beautiful 0.12, art 0.12, work 0.11, technology 0.1, materials 0.1, new 0.1, object 0.1, process 0.09, kind 0.07, objects 0.06, projects 0.05, working 0.05, ideas 0.05, approach 0.05, form 0.05, oh 0.05, \n",
      "\n",
      "Topic 34:\n",
      "information 2.39, phone 0.43, mobile 0.28, visual 0.21, intelligence 0.11, access 0.11, police 0.1, digital 0.09, device 0.06, technologies 0.06, genetic 0.05, data 0.05, th 0.05, google 0.04, computers 0.04, code 0.04, freedom 0.04, government 0.04, books 0.04, learning 0.03, \n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features, \"\n",
    "      \"n_samples={} and n_features={}...\".format(n_samples, n_features))\n",
    "nmf = NMF(n_components=n_topics, \n",
    "          random_state=1,\n",
    "          alpha=.1, \n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Scale component values so that they add up to 1 for any given document\n",
    "#doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF(alpha=0.1, beta=1, eta=0.1, init=None, l1_ratio=0.5, max_iter=200,\n",
      "  n_components=35, nls_max_iter=2000, random_state=1, shuffle=False,\n",
      "  solver='cd', sparseness=None, tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "\n",
      "Topic 0:\n",
      "people 0.17, going 0.12, know 0.12, talk 0.11, car 0.11, computer 0.11, way 0.1, thing 0.1, actually 0.1, able 0.1, world 0.1, different 0.09, film 0.09, device 0.09, did 0.09, use 0.09, think 0.09, really 0.09, got 0.09, power 0.09, \n",
      "\n",
      "Topic 1:\n",
      "world 0.1, violence 0.1, cells 0.09, patients 0.09, cancer 0.09, war 0.09, father 0.08, day 0.08, today 0.08, just 0.08, make 0.08, work 0.08, field 0.08, time 0.08, cell 0.08, stand 0.08, use 0.08, life 0.08, people 0.07, state 0.07, \n",
      "\n",
      "Topic 2:\n",
      "years 1272.46, energy 1044.08, planet 952.69, earth 921.22, world 814.61, water 762.98, ve 719.5, ocean 691.31, going 642.01, life 609.82, oil 572.08, just 563.98, time 556.73, need 533.48, climate 509.74, sea 507.03, fish 482.79, species 470.4, ice 454.26, year 430.16, \n",
      "\n",
      "Topic 3:\n",
      "light 1121.88, just 922.57, space 896.63, universe 869.77, time 728.21, look 625.82, different 567.79, way 542.17, world 490.07, science 486.24, actually 428.66, new 421.71, image 364.43, make 362.95, life 351.52, right 343.26, inside 334.95, ve 326.83, use 322.39, stars 320.79, \n",
      "\n",
      "Topic 4:\n",
      "natural 0.08, oil 0.08, gas 0.07, million 0.07, dollars 0.07, use 0.07, carbon 0.06, ca 0.06, going 0.06, don 0.06, make 0.06, product 0.06, cost 0.06, products 0.06, energy 0.06, countries 0.06, plan 0.06, machine 0.06, look 0.06, point 0.06, \n",
      "\n",
      "Topic 5:\n",
      "women 1980.24, men 1086.24, woman 577.72, compassion 391.21, girls 386.65, story 349.32, stories 324.73, sex 322.42, people 312.94, book 259.3, man 259.26, violence 255.47, world 250.97, said 242.0, books 240.23, mother 212.84, work 208.16, love 202.44, word 201.24, girl 188.86, \n",
      "\n",
      "Topic 6:\n",
      "just 0.14, ve 0.13, said 0.13, years 0.13, year 0.13, going 0.13, earth 0.12, think 0.12, need 0.11, time 0.11, people 0.11, problem 0.1, let 0.1, way 0.1, percent 0.1, end 0.09, come 0.09, change 0.09, energy 0.09, say 0.09, \n",
      "\n",
      "Topic 7:\n",
      "magic 229.91, audience 19.55, technology 5.61, try 5.15, digital 5.1, come 4.91, reality 4.88, hey 4.7, let 4.52, share 4.47, today 3.45, practice 3.07, tools 3.02, thank 2.79, right 2.65, research 2.63, form 2.41, oh 2.4, going 2.25, tell 2.06, \n",
      "\n",
      "Topic 8:\n",
      "world 0.41, cancer 0.39, people 0.38, know 0.32, make 0.29, ve 0.28, actually 0.28, just 0.25, need 0.24, really 0.24, ll 0.24, don 0.2, book 0.2, today 0.2, going 0.2, work 0.19, technology 0.19, things 0.19, little 0.19, drug 0.19, \n",
      "\n",
      "Topic 9:\n",
      "computer 742.0, machine 660.76, computers 335.41, machines 205.54, car 106.28, software 105.38, going 91.3, race 82.46, program 79.35, built 77.9, learning 77.84, technology 72.9, device 61.48, able 57.6, little 55.27, just 54.22, human 50.11, ve 49.41, thing 48.34, did 47.46, \n",
      "\n",
      "Topic 10:\n",
      "just 0.6, people 0.53, going 0.5, world 0.41, really 0.38, think 0.38, actually 0.36, years 0.27, ve 0.24, new 0.24, make 0.23, need 0.23, carbon 0.22, time 0.22, food 0.21, let 0.21, systems 0.21, kind 0.21, things 0.21, know 0.2, \n",
      "\n",
      "Topic 11:\n",
      "robot 456.37, robots 354.81, legs 141.13, want 97.21, fly 82.28, control 66.55, build 64.54, video 51.56, foot 47.96, just 45.8, body 43.45, actually 43.45, use 42.11, ll 41.89, second 41.15, doing 40.49, machines 40.38, human 36.61, look 36.23, really 35.87, \n",
      "\n",
      "Topic 12:\n",
      "people 2888.28, think 2416.03, actually 1744.33, just 1708.03, things 1597.93, world 1540.33, going 1523.06, really 1510.64, want 1249.61, way 1220.45, know 1205.6, don 1152.73, ve 1141.0, right 1018.53, thing 980.37, kind 961.07, time 910.32, say 869.94, make 835.44, use 782.02, \n",
      "\n",
      "Topic 13:\n",
      "animals 0.11, water 0.1, just 0.08, pretty 0.08, going 0.08, time 0.08, did 0.08, look 0.08, really 0.08, community 0.08, lot 0.08, right 0.08, life 0.08, feet 0.08, way 0.07, went 0.07, creating 0.07, fish 0.07, ve 0.07, surface 0.07, \n",
      "\n",
      "Topic 14:\n",
      "going 0.43, people 0.42, think 0.42, don 0.32, just 0.3, said 0.3, really 0.29, ve 0.29, know 0.28, years 0.28, cells 0.27, time 0.27, world 0.25, make 0.24, little 0.24, look 0.24, things 0.23, life 0.22, got 0.2, say 0.2, \n",
      "\n",
      "Topic 15:\n",
      "really 0.14, data 0.14, images 0.12, make 0.11, actually 0.11, time 0.1, just 0.1, use 0.1, work 0.09, science 0.09, numbers 0.09, art 0.09, single 0.09, information 0.09, way 0.09, come 0.09, lot 0.09, women 0.09, ve 0.08, read 0.08, \n",
      "\n",
      "Topic 16:\n",
      "world 0.27, going 0.22, used 0.2, know 0.2, years 0.19, ve 0.18, let 0.18, really 0.18, use 0.18, people 0.17, look 0.16, time 0.16, think 0.16, percent 0.16, points 0.15, dollars 0.15, thing 0.15, make 0.15, great 0.15, new 0.15, \n",
      "\n",
      "Topic 17:\n",
      "people 0.32, world 0.3, ve 0.27, really 0.23, book 0.19, time 0.19, just 0.18, way 0.17, years 0.17, story 0.17, film 0.17, going 0.17, things 0.16, work 0.15, images 0.14, life 0.14, make 0.14, didn 0.14, kind 0.14, tell 0.14, \n",
      "\n",
      "Topic 18:\n",
      "use 0.36, water 0.32, need 0.27, don 0.23, think 0.19, right 0.18, going 0.18, make 0.18, people 0.17, percent 0.16, green 0.16, plant 0.15, world 0.15, ve 0.15, species 0.15, just 0.15, lot 0.14, using 0.14, way 0.14, years 0.14, \n",
      "\n",
      "Topic 19:\n",
      "music 808.47, play 554.31, sound 489.05, listening 216.65, hear 183.47, sounds 163.44, playing 139.65, listen 129.3, video 94.73, way 47.8, experience 42.65, just 36.42, let 36.06, heard 32.47, really 26.42, going 23.74, ll 20.67, kind 19.42, song 18.53, little 16.6, \n",
      "\n",
      "Topic 20:\n",
      "world 0.23, people 0.15, percent 0.15, better 0.14, today 0.14, violence 0.13, father 0.12, technology 0.11, stand 0.11, human 0.11, make 0.11, want 0.11, day 0.1, hope 0.1, time 0.1, use 0.1, good 0.1, information 0.1, mobile 0.1, tell 0.1, \n",
      "\n",
      "Topic 21:\n",
      "people 0.31, women 0.26, know 0.2, thing 0.2, say 0.16, country 0.15, africa 0.15, think 0.14, business 0.13, just 0.13, world 0.13, ve 0.13, dollars 0.13, years 0.12, want 0.12, school 0.12, things 0.12, wonderful 0.12, love 0.11, universe 0.11, \n",
      "\n",
      "Topic 22:\n",
      "cancer 955.22, design 852.35, just 776.43, work 763.14, really 733.42, people 682.11, actually 668.48, need 619.6, way 608.49, patients 603.25, new 602.5, patient 544.61, time 540.39, students 526.62, make 525.33, things 517.93, ve 517.55, health 511.94, think 458.63, know 455.06, \n",
      "\n",
      "Topic 23:\n",
      "ca 592.18, ok 454.43, yeah 356.2, mean 169.0, chris 150.45, think 126.5, yes 111.33, right 93.86, ted 74.8, audience 71.22, good 61.94, got 61.49, thank 56.98, ve 48.74, did 41.64, ll 39.34, don 37.61, people 37.2, going 36.31, seven 30.8, \n",
      "\n",
      "Topic 24:\n",
      "food 1175.17, water 1056.43, eat 348.86, plants 343.73, plant 320.99, bacteria 308.97, ve 241.59, world 241.5, use 217.96, just 217.24, waste 215.43, need 192.32, think 184.19, grow 173.29, feed 164.3, environment 159.73, species 158.72, make 158.26, natural 153.9, way 151.46, \n",
      "\n",
      "Topic 25:\n",
      "brain 1002.72, self 328.02, sleep 259.93, mind 234.76, happiness 223.93, mental 182.88, body 154.36, going 109.26, think 84.67, minds 73.98, course 72.62, brains 68.2, memory 66.02, map 65.29, don 60.86, experience 60.01, region 53.06, day 50.13, life 49.57, night 48.86, \n",
      "\n",
      "Topic 26:\n",
      "people 3466.36, percent 1425.01, going 1173.96, ve 1109.82, data 1095.29, just 1039.23, think 1014.68, dollars 929.89, money 888.45, need 847.79, know 795.48, years 786.16, make 772.66, world 759.18, don 754.35, time 744.18, want 738.47, things 709.81, actually 701.35, business 688.05, \n",
      "\n",
      "Topic 27:\n",
      "world 0.09, actually 0.08, powerful 0.08, got 0.07, power 0.07, just 0.07, look 0.07, different 0.07, used 0.07, year 0.07, country 0.07, case 0.07, don 0.07, time 0.07, century 0.07, space 0.07, great 0.07, way 0.06, think 0.06, bring 0.06, \n",
      "\n",
      "Topic 28:\n",
      "black 414.65, song 266.48, love 60.36, baby 31.33, white 23.89, cause 21.0, hear 16.52, sun 16.39, hey 15.82, ll 15.04, ve 12.84, going 10.04, oh 8.65, police 8.35, men 8.21, life 7.77, young 7.61, just 7.02, older 6.96, blood 6.38, \n",
      "\n",
      "Topic 29:\n",
      "just 2288.76, going 2273.23, really 2129.24, actually 2126.5, think 1731.66, know 1584.15, little 1546.23, things 1321.03, look 1267.08, ve 1253.93, right 1188.04, brain 1176.42, different 1126.5, way 1101.53, cells 1044.21, thing 1043.15, don 969.8, make 945.46, kind 933.66, human 923.46, \n",
      "\n",
      "Topic 30:\n",
      "cells 0.24, environment 0.17, place 0.16, feeling 0.16, help 0.15, people 0.14, women 0.13, brain 0.13, change 0.12, wall 0.12, cell 0.12, car 0.11, knows 0.11, pattern 0.11, remember 0.11, think 0.1, memory 0.09, said 0.09, believe 0.09, animal 0.09, \n",
      "\n",
      "Topic 31:\n",
      "world 2095.75, people 1330.37, country 1229.41, countries 1019.62, children 822.07, india 789.48, years 788.44, china 737.65, today 674.62, states 632.53, war 583.6, global 583.42, government 574.55, united 557.72, percent 513.03, change 498.14, political 494.93, state 452.06, economic 446.6, chinese 422.42, \n",
      "\n",
      "Topic 32:\n",
      "people 1287.07, city 1251.01, just 911.09, new 887.53, building 825.09, really 824.86, going 730.16, cities 723.54, ve 617.79, make 616.21, think 603.35, car 569.9, way 542.11, time 506.1, work 503.15, little 480.37, kind 475.19, design 466.69, years 459.25, things 446.35, \n",
      "\n",
      "Topic 33:\n",
      "know 5554.6, people 5222.97, just 4589.86, said 4511.61, going 3605.3, don 3447.42, time 3176.95, ve 2831.94, think 2740.11, got 2712.46, life 2542.43, really 2533.77, want 2518.02, say 2359.3, did 2324.05, way 2134.6, years 2090.14, day 2045.77, things 2035.45, right 1982.55, \n",
      "\n",
      "Topic 34:\n",
      "brain 0.41, going 0.24, look 0.18, mind 0.17, need 0.17, climate 0.16, think 0.16, just 0.16, earth 0.16, energy 0.16, fact 0.15, course 0.15, make 0.14, self 0.14, don 0.13, years 0.13, people 0.13, really 0.13, body 0.13, know 0.13, \n"
     ]
    }
   ],
   "source": [
    "# Fit the LDA model\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 2106\n"
     ]
    }
   ],
   "source": [
    "# Now to associate topics to documents...\n",
    "doc_topic_distrib = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 2106\n"
     ]
    }
   ],
   "source": [
    "print(type(doc_topic_distrib), len(doc_topic_distrib))\n",
    "\n",
    "# Turn our list of authors and dates into an array:\n",
    "citations = np.asarray(authordate)\n",
    "\n",
    "num_groups = len(set(citations))\n",
    "\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))\n",
    "\n",
    "for i, name in enumerate(sorted(set(citations))):\n",
    "    doctopic_grouped[i, :] = np.mean(doctopic[citations == name, :], axis=0)\n",
    "\n",
    "for i in range(len(doctopic)):\n",
    "    top_topics = np.argsort(doctopic[i,:])[::-1][0:3]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(citations[i], top_topics_str))\n",
    "\n",
    "for t in range(len(topic_words)):\n",
    "    print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:15])))\n",
    "\n",
    "num_groups = len(set(au_dats))\n",
    "\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))\n",
    "\n",
    "for i, name in enumerate(sorted(set(au_dats))):\n",
    "    doctopic_grouped[i, :] = np.mean(doctopic[au_dats == name, :], axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
