{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words has 174 words \n",
      "the NLTK has 153 words \n",
      "the TedTalk list has 302 words \n",
      "the MALLET list has 525 words \n",
      "the Blei list has 297 words\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# the two embedded lists\n",
    "mod_stop = get_stop_words('en')\n",
    "nltk_stop = stopwords.words(\"english\")\n",
    "\n",
    "# external lists\n",
    "tt_list = re.split('\\s+', open('../data/stopwords_tt.txt', 'r').read().lower())\n",
    "mallet_list = re.split('\\s+', open('../data/stopwords_mallet.txt', 'r').read().lower())\n",
    "blei_list = re.split('\\s+', open('../data/stopwords_Blei.txt', 'r').read().lower())\n",
    "\n",
    "print(\"stop_words has {} words \\n\" \n",
    "      \"the NLTK has {} words \\n\" \n",
    "      \"the TedTalk list has {} words \\n\"\n",
    "      \"the MALLET list has {} words \\n\"\n",
    "      \"the Blei list has {} words\".format(len(mod_stop), \n",
    "                                            len(nltk_stop), \n",
    "                                            len(tt_list), \n",
    "                                            len(mallet_list),\n",
    "                                            len(blei_list)))\n",
    "\n",
    "# Having determined that the best place to start is with the \n",
    "# combination of the Blei list and the stop_word list:\n",
    "\n",
    "combo_list = mod_stop + blei_list\n",
    "tt_stopset = set(combo_list)\n",
    "tt_stoplist = sorted(list(tt_stop))\n",
    "outfile = open('../data/tt_stop.txt', 'w')\n",
    "outfile.write(\"\\n\".join(tt_stoplist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt_stop has 351 words\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tt_stoplist = re.split('\\s+', open('../data/tt_stop.txt', 'r').read().lower())\n",
    "print(\"tt_stop has {} words\".format(len(tt_stoplist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=\n",
    "# Clean and Tokenize, then Drop Stopwords\n",
    "# =-=-=-=-=-=\n",
    "\n",
    "# Load tokenizer, stopwords, and stemmer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "stopwords = re.split('\\s+', open('../data/tt_stop.txt', 'r').read().lower())\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# List for loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in test:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = re.sub(r\"[^\\w\\d'\\s]+\",'', i).lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in stopwords]\n",
    "    \n",
    "    # stem tokens\n",
    "    # stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(tokens)\n",
    "#    texts.append(stemmed_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
