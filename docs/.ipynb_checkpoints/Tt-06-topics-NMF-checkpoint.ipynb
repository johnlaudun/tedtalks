{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "# =-=-=-=-=-=\n",
    "# Consolidated imports for entire notebook\n",
    "# =-=-=-=-=-=\n",
    "\n",
    "import pandas\n",
    "import re\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import numpy as np\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=\n",
    "# Read CSV into DataFrame and then create lists\n",
    "# =-=-=-=-=-=\n",
    "\n",
    "# Create pandas dataframe & lists\n",
    "colnames = ['author', 'title', 'date' , 'length', 'text']\n",
    "df = pandas.read_csv('../data/talks_2.csv', names=colnames)\n",
    "talks = df.text.tolist()\n",
    "authors = df.author.tolist()\n",
    "dates = df.date.tolist()\n",
    "\n",
    "# Get years from date list and combing with author list for labels\n",
    "years = [re.sub('[A-Za-z ]', '', item) for item in dates]\n",
    "authordate = [author+\" \"+year for author, year in zip(authors, years)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=\n",
    "# Clean and Tokenize, then Drop Stopwords\n",
    "# =-=-=-=-=-=\n",
    "\n",
    "# Documentation: https://pypi.python.org/pypi/lda\n",
    "# LDA requires a DTM as input\n",
    "\n",
    "# From the Stopwords Notebook:\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "stopwords = re.split('\\s+', open('../data/tt_stop.txt', 'r').read().lower())\n",
    "\n",
    "# Loop to tokenize, stop, and stem (if needed) texts.\n",
    "texts = []\n",
    "for i in talks:   \n",
    "    # clean and tokenize document string\n",
    "    raw = re.sub(r\"[^\\w\\d'\\s]+\",'', i).lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in stopwords]\n",
    "    # stem tokens\n",
    "    # stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    # add tokens to list\n",
    "    texts.append(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Re-Assemble Texts as Strings from Lists of Words\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "strungs = []\n",
    "for text in texts:\n",
    "    strung = ' '.join(text)\n",
    "    strungs.append(strung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with 40 topics for 2092 documents with 2000 features.\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=\n",
    "# Get NMF topics\n",
    "# =-=-=-=-=-=\n",
    "\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "\n",
    "# All our variables are here to make it easier to make adjustments\n",
    "n_samples = len(strungs)\n",
    "n_features = 2000\n",
    "n_topics = 40\n",
    "n_top_words = 15\n",
    "# tt_stopwords = open('../data/stopwords_tt.txt', 'r').read().splitlines()\n",
    "\n",
    "# Get tf-idf features for NMF\n",
    "tfidf_vectorizer = sk_text.TfidfVectorizer(max_df = 0.95,\n",
    "                                        min_df = 2,\n",
    "                                        max_features = n_features)\n",
    "tfidf = tfidf_vectorizer.fit_transform(strungs)\n",
    "\n",
    "# Fit the NMF model\n",
    "nmf = NMF(n_components = n_topics,\n",
    "          random_state = 1,\n",
    "          alpha = 0.1,\n",
    "          l1_ratio = 0.5).fit(tfidf)\n",
    "print(\"Fitting the NMF model with {} topics for {} documents with {} features.\"\n",
    "      .format(n_topics, n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in NMF model:\n",
      "\n",
      "Topic 0:\n",
      "just 0.9, going 0.82, really 0.77, people 0.71, think 0.71, know 0.7, get 0.63, actually 0.62, things 0.62, see 0.59, right 0.51, little 0.48, time 0.48, thing 0.47, go 0.46, \n",
      "\n",
      "Topic 1:\n",
      "cells 1.81, stem 0.69, cell 0.55, tissue 0.28, bone 0.26, organs 0.23, disease 0.19, drug 0.18, actually 0.13, lab 0.12, drugs 0.12, heart 0.11, patient 0.1, blood 0.1, patients 0.09, \n",
      "\n",
      "Topic 2:\n",
      "energy 1.24, oil 0.87, climate 0.4, carbon 0.36, nuclear 0.35, coal 0.34, gas 0.29, solar 0.27, fuel 0.26, electricity 0.24, emissions 0.2, power 0.2, wind 0.2, percent 0.17, billion 0.16, \n",
      "\n",
      "Topic 3:\n",
      "women 2.5, men 1.01, woman 0.39, sex 0.26, female 0.26, gender 0.24, pm 0.19, male 0.17, violence 0.14, sexual 0.14, equal 0.1, girls 0.1, rights 0.09, boys 0.09, media 0.08, \n",
      "\n",
      "Topic 4:\n",
      "data 2.62, information 0.43, map 0.15, web 0.14, see 0.08, patterns 0.07, numbers 0.07, points 0.06, look 0.06, privacy 0.06, text 0.06, analysis 0.06, computer 0.05, satellite 0.05, collect 0.05, \n",
      "\n",
      "Topic 5:\n",
      "design 1.9, designers 0.55, building 0.47, architecture 0.32, designed 0.24, designing 0.24, designer 0.23, buildings 0.2, materials 0.2, project 0.14, product 0.13, products 0.13, process 0.12, technology 0.12, new 0.12, \n",
      "\n",
      "Topic 6:\n",
      "cancer 2.05, tumor 0.5, breast 0.25, disease 0.24, drug 0.19, protein 0.17, blood 0.16, molecule 0.1, body 0.1, cure 0.09, tissue 0.08, treatment 0.08, muscle 0.08, molecules 0.07, drugs 0.07, \n",
      "\n",
      "Topic 7:\n",
      "brain 2.42, neurons 0.54, brains 0.33, activity 0.16, mental 0.15, arm 0.15, human 0.13, memory 0.13, electrical 0.12, consciousness 0.11, behavior 0.11, signals 0.1, control 0.1, visual 0.1, mind 0.1, \n",
      "\n",
      "Topic 8:\n",
      "music 2.29, play 0.28, playing 0.23, song 0.22, piece 0.2, hear 0.18, played 0.13, listen 0.1, da 0.1, hearing 0.09, hall 0.08, sound 0.07, experience 0.07, dance 0.05, yeah 0.05, \n",
      "\n",
      "Topic 9:\n",
      "robot 1.77, robots 1.13, move 0.09, legs 0.09, see 0.07, lab 0.07, build 0.06, control 0.05, animal 0.05, leg 0.05, motion 0.04, foot 0.04, machines 0.04, small 0.04, intelligence 0.04, \n",
      "\n",
      "Topic 10:\n",
      "kids 1.19, school 1.04, students 0.74, children 0.71, education 0.67, teachers 0.65, schools 0.39, teacher 0.34, learning 0.33, classroom 0.28, teach 0.28, teaching 0.27, child 0.26, parents 0.22, class 0.21, \n",
      "\n",
      "Topic 11:\n",
      "dna 1.32, genome 0.57, bacteria 0.51, genes 0.44, species 0.4, gene 0.38, genetic 0.34, cell 0.28, molecular 0.25, code 0.22, virus 0.21, sequence 0.19, biology 0.17, organisms 0.17, evolution 0.16, \n",
      "\n",
      "Topic 12:\n",
      "patients 1.04, health 0.78, patient 0.71, care 0.53, medical 0.48, doctors 0.43, hospital 0.35, surgery 0.35, doctor 0.34, medicine 0.33, disease 0.32, healthcare 0.29, heart 0.28, treatment 0.23, clinical 0.16, \n",
      "\n",
      "Topic 13:\n",
      "city 1.62, cities 1.1, building 0.41, buildings 0.32, space 0.31, urban 0.29, new 0.28, york 0.26, spaces 0.26, map 0.24, public 0.22, architecture 0.2, park 0.2, neighborhood 0.2, street 0.2, \n",
      "\n",
      "Topic 14:\n",
      "machine 1.77, computer 0.74, machines 0.54, computers 0.38, human 0.13, learning 0.13, intelligence 0.1, memory 0.1, built 0.1, technology 0.09, build 0.08, web 0.08, dna 0.06, mathematical 0.06, running 0.05, \n",
      "\n",
      "Topic 15:\n",
      "girls 1.81, girl 0.67, boys 0.31, school 0.24, father 0.08, wish 0.08, afghanistan 0.08, dance 0.07, village 0.06, daughter 0.06, boy 0.06, sister 0.05, community 0.04, marriage 0.04, research 0.04, \n",
      "\n",
      "Topic 16:\n",
      "ocean 1.07, fish 0.9, sea 0.64, animals 0.56, coral 0.39, species 0.37, oceans 0.35, sharks 0.31, animal 0.21, deep 0.2, ice 0.18, planet 0.18, food 0.15, land 0.14, water 0.14, \n",
      "\n",
      "Topic 17:\n",
      "universe 1.56, galaxies 0.48, galaxy 0.45, space 0.32, stars 0.31, dark 0.26, theory 0.25, physics 0.23, telescope 0.22, gravity 0.18, particles 0.17, billion 0.16, black 0.15, energy 0.15, sky 0.14, \n",
      "\n",
      "Topic 18:\n",
      "language 1.37, english 1.04, word 0.49, words 0.49, languages 0.32, speak 0.18, sentence 0.13, books 0.12, speech 0.12, writing 0.1, say 0.1, read 0.08, autism 0.08, learn 0.07, learning 0.07, \n",
      "\n",
      "Topic 19:\n",
      "compassion 1.64, god 0.31, religious 0.18, suffering 0.14, beings 0.1, happy 0.1, religion 0.09, human 0.09, world 0.08, love 0.08, rule 0.07, empathy 0.07, self 0.06, says 0.06, einstein 0.06, \n",
      "\n",
      "Topic 20:\n",
      "mars 0.89, earth 0.86, planet 0.75, planets 0.64, solar 0.37, life 0.31, ice 0.31, stars 0.31, surface 0.28, sun 0.27, atmosphere 0.26, star 0.22, space 0.2, moon 0.19, telescope 0.19, \n",
      "\n",
      "Topic 21:\n",
      "plastic 1.47, bottle 0.27, ocean 0.17, waste 0.15, materials 0.13, bacteria 0.12, beach 0.11, material 0.09, steel 0.09, stuff 0.07, make 0.07, products 0.06, bits 0.06, island 0.05, mine 0.04, \n",
      "\n",
      "Topic 22:\n",
      "video 1.84, youtube 0.31, videos 0.29, content 0.26, games 0.22, camera 0.2, motion 0.16, objects 0.12, rights 0.1, cameras 0.09, show 0.08, virtual 0.07, see 0.07, bridge 0.07, object 0.07, \n",
      "\n",
      "Topic 23:\n",
      "ca 1.98, yeah 0.24, mean 0.18, think 0.13, chris 0.13, anderson 0.11, mr 0.09, got 0.08, people 0.07, ted 0.06, ok 0.05, bg 0.04, wow 0.03, source 0.03, report 0.03, \n",
      "\n",
      "Topic 24:\n",
      "game 1.24, play 1.08, games 1.03, playing 0.34, players 0.25, player 0.2, played 0.11, win 0.1, fun 0.07, social 0.06, physical 0.05, real 0.05, world 0.05, serious 0.05, worlds 0.04, \n",
      "\n",
      "Topic 25:\n",
      "africa 1.92, african 0.77, continent 0.37, south 0.23, countries 0.2, aid 0.19, kenya 0.19, leaders 0.13, farmers 0.1, sector 0.1, world 0.09, corruption 0.08, country 0.08, asia 0.07, sub 0.07, \n",
      "\n",
      "Topic 26:\n",
      "quantum 1.26, particles 0.36, field 0.21, metal 0.12, inside 0.11, theory 0.1, physics 0.09, energy 0.06, state 0.04, phenomenon 0.04, zero 0.04, single 0.03, place 0.03, object 0.03, weird 0.03, \n",
      "\n",
      "Topic 27:\n",
      "laughter 2.23, applause 0.33, okay 0.21, oh 0.12, yeah 0.12, mom 0.08, right 0.08, know 0.07, piece 0.06, mystery 0.06, hear 0.05, color 0.05, got 0.05, friends 0.05, just 0.03, \n",
      "\n",
      "Topic 28:\n",
      "light 1.83, see 0.4, lights 0.32, blue 0.2, eye 0.19, dark 0.18, eyes 0.18, camera 0.17, sun 0.17, color 0.14, stars 0.14, star 0.11, green 0.1, image 0.1, create 0.09, \n",
      "\n",
      "Topic 29:\n",
      "bees 1.58, plant 0.17, plants 0.1, insects 0.1, food 0.05, urban 0.05, species 0.04, nature 0.03, researchers 0.02, agriculture 0.02, right 0.02, sex 0.01, male 0.01, science 0.01, garden 0.01, \n",
      "\n",
      "Topic 30:\n",
      "chinese 1.26, china 1.2, india 0.57, indian 0.27, west 0.16, world 0.14, north 0.12, countries 0.11, asia 0.11, growth 0.11, political 0.11, democracy 0.1, east 0.09, state 0.09, united 0.08, \n",
      "\n",
      "Topic 31:\n",
      "said 0.91, life 0.47, father 0.4, know 0.39, people 0.39, man 0.38, mother 0.38, story 0.38, family 0.37, love 0.34, day 0.33, went 0.32, children 0.3, told 0.29, stories 0.28, \n",
      "\n",
      "Topic 32:\n",
      "water 2.34, river 0.25, food 0.24, plant 0.23, waste 0.22, plants 0.18, desert 0.13, bacteria 0.12, dry 0.11, organisms 0.11, india 0.1, rain 0.09, supply 0.08, farmers 0.08, air 0.08, \n",
      "\n",
      "Topic 33:\n",
      "internet 1.06, information 0.79, online 0.47, media 0.45, digital 0.45, web 0.44, phone 0.41, technology 0.4, mobile 0.31, twitter 0.27, page 0.26, government 0.25, facebook 0.23, network 0.22, computer 0.21, \n",
      "\n",
      "Topic 34:\n",
      "car 1.47, cars 1.0, driving 0.34, driver 0.3, road 0.26, vehicle 0.24, drive 0.22, miles 0.19, traffic 0.15, cities 0.13, going 0.11, sharing 0.1, mile 0.09, electric 0.09, hour 0.09, \n",
      "\n",
      "Topic 35:\n",
      "art 1.38, artist 0.58, artists 0.54, painting 0.44, museum 0.34, images 0.28, project 0.23, paint 0.23, work 0.22, image 0.18, book 0.18, space 0.18, arts 0.14, wall 0.12, books 0.11, \n",
      "\n",
      "Topic 36:\n",
      "people 0.8, world 0.5, countries 0.44, percent 0.42, country 0.4, global 0.37, government 0.36, money 0.33, dollars 0.33, social 0.31, economic 0.29, change 0.26, economy 0.26, democracy 0.25, states 0.25, \n",
      "\n",
      "Topic 37:\n",
      "sound 1.59, noise 0.52, voice 0.51, listening 0.48, sounds 0.36, hear 0.28, hearing 0.22, listen 0.2, voices 0.17, song 0.14, silence 0.11, waves 0.09, play 0.09, visual 0.08, time 0.07, \n",
      "\n",
      "Topic 38:\n",
      "hiv 1.3, virus 0.56, drugs 0.38, aids 0.38, vaccine 0.31, disease 0.28, epidemic 0.26, drug 0.23, flu 0.23, treatment 0.22, countries 0.19, sex 0.16, diseases 0.14, malaria 0.14, risk 0.1, \n",
      "\n",
      "Topic 39:\n",
      "body 1.74, skin 0.53, bodies 0.38, legs 0.38, space 0.26, human 0.24, death 0.16, blood 0.16, muscle 0.15, voice 0.13, arm 0.13, tissue 0.09, inside 0.09, evolution 0.09, technology 0.09, \n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=\n",
    "# Get NMF printing\n",
    "# =-=-=-=-=-=\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_id, topic in enumerate(model.components_):\n",
    "        print('\\nTopic {}:'.format(int(topic_id)))\n",
    "        print(''.join([feature_names[i] + ' ' + str(round(topic[i], 2))\n",
    "              +', ' for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "print(\"Topics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words) #n_top_words can be changed on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm = tfidf.toarray()\n",
    "doctopic = nmf.fit_transform(dtm) # This is an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "axis 1 out of bounds [0, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d530d224f207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mfileheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"citations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopsnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdocTopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthordate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdocTopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocTopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: axis 1 out of bounds [0, 1)"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=\n",
    "# Saving output to CSV\n",
    "# =-=-=-=-=-=\n",
    "\n",
    "# Since DOCTOPIC is an array, you can just do:\n",
    "#      np.savetxt(\"foo.csv\", doctopic, delimiter=\",\", fmt = \"%s\")\n",
    "# http://stackoverflow.com/questions/6081008/dump-a-numpy-array-into-a-csv-file\n",
    "#\n",
    "# The above won't give you the names of the files. Instead try this:\n",
    "\n",
    "topsnum = np.array([list(range(n_topics))])\n",
    "# topsnum = np.indices((1,n_topics))[1] <-- this is more than we need,\n",
    "#                                           but it's cool to know more tricks\n",
    "#\n",
    "# Two ways to get an array that is of the form [[0,1,2,3,...]].\n",
    "# It will have the desired dimensions of (1,35) which is what we want\n",
    "\n",
    "fileheader = np.concatenate((np.array([[\"citations\"]]), topsnum),axis = 1)\n",
    "\n",
    "docTopics = np.concatenate((authordate, doctopic), axis = 1)\n",
    "docTopics = np.concatenate((fileheader, docTopics), axis = 0)\n",
    "\n",
    "np.savetxt(\"../data/nmf_topics.csv\", docTopics, delimiter=\",\", fmt = \"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
