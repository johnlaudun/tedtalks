{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Pages that List Speakers\n",
    "\n",
    "After some rooting around on the TED website, examining the Speakers home page, it looks like there are 86 pages that list TED speakers, with URLS like this: `https://www.ted.com/speakers?page=1`. My plan is to:\n",
    "\n",
    "1. create a list of pages in a text file, \n",
    "2. download the 86 pages, \n",
    "3. parse the speakers out,\n",
    "4. create a second list of pages with the `ted.com/speakers/speaker_name\" format`\n",
    "5. download all those pages\n",
    "6. parse them into a CSV for KK.\n",
    "\n",
    "Oh, I hope this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's going to be a lot of files created in this process, so it's important to remember where I am:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/john/Code/tedtalks/data'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a list of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ted.com/speakers?page=1\n",
      "https://www.ted.com/speakers?page=2\n",
      "https://www.ted.com/speakers?page=3\n",
      "https://www.ted.com/speakers?page=4\n"
     ]
    }
   ],
   "source": [
    "# This is just proof of concept. I actually used range(1,87) to get\n",
    "# the list I needed and pasted it into a text document.\n",
    "for i in range(1,5):\n",
    "    print(\"https://www.ted.com/speakers?page=\" + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download the pages\n",
    "\n",
    "Okay, the text file is `speaker_index_pages.txt` in the data/speakers/directory. I used the following to download all 86 speaker pages to the indices directory:\n",
    "\n",
    "    wget -w 2 -i ../speaker_index_pages.txt\n",
    "\n",
    "Then, being lazy and not wanting to figure out how to parse all the files into one list (see below), I simply concatenated all the files into one, with the plan to use `BeautifulSoup` to run through it.\n",
    "\n",
    "    cat indices/* > speakers_all_pages.txt\n",
    "\n",
    "### Step 3: Parse the Speaker URLs Out of the Pages\n",
    "\n",
    "Inside the HTML, each speaker's profile can be found in the following line:\n",
    "\n",
    "    <a class=\"results__result media media--sm-v m4\" href=\"/speakers/ellen_t_hoen\">\n",
    "\n",
    "So we need the `href` attribute for the `results__result` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"results__result media media--sm-v m4\" href=\"/speakers/ellen_t_hoen\">\n",
      "<div class=\"media__image media__image--thumb\">\n",
      "<span class=\"thumb thumb--square\"><span class=\"thumb__sizer\"><span class=\"thumb__tugger\"><img alt=\"\" class=\" thumb__image\" play=\"false\" src=\"https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/bcd3208945bf8b311418b64f917b1f38e84a24e8_800x600.jpg?h=191&amp;w=254\"/><span class=\"thumb__aligner\"></span></span></span></span>\n",
      "</div>\n",
      "<div class=\"media__message\">\n",
      "<h4 class=\"h7 m5\">\n",
      "Ellen<br/>'t Hoen</h4>\n",
      "<p class=\"p4\">\n",
      "<strong>Medicine law expert</strong>\n",
      "</p>\n",
      "</div>\n",
      "</a>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "the_file = open(\"./speakers/speakers_all_pages.txt\", \"r\")\n",
    "the_soup = BeautifulSoup(the_file, \"lxml\")\n",
    "speaker_suffix = the_soup.find('a', {'class':'results__result'})\n",
    "print(speaker_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/speakers/ellen_t_hoen\n"
     ]
    }
   ],
   "source": [
    "the_file = open(\"./speakers/speakers_all_pages.txt\", \"r\")\n",
    "the_soup = BeautifulSoup(the_file, \"lxml\")\n",
    "speaker_suffix = the_soup.find('a', {'class':'results__result'})['href']\n",
    "print(speaker_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(speaker_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_file = open(\"./speakers/speakers_all_pages.txt\", \"r\")\n",
    "the_soup = BeautifulSoup(the_file, \"lxml\")\n",
    "speaker_suffixes = the_soup.find_all('a', {'class':'results__result'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speaker_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this only returning 30 items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"results__result media media--sm-v m4\" href=\"/speakers/marc_abrahams\">\n",
      "<div class=\"media__image media__image--thumb\">\n",
      "<span class=\"thumb thumb--square\"><span class=\"thumb__sizer\"><span class=\"thumb__tugger\"><img alt=\"\" class=\" thumb__image\" play=\"false\" src=\"https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/6ce52170e488499f44d4145454d54b247325bc8b_800x600.jpg?h=191&amp;w=254\"/><span class=\"thumb__aligner\"></span></span></span></span>\n",
      "</div>\n",
      "<div class=\"media__message\">\n",
      "<h4 class=\"h7 m5\">\n",
      "Marc<br/>Abrahams</h4>\n",
      "<p class=\"p4\">\n",
      "<strong>Science humorist</strong>\n",
      "</p>\n",
      "</div>\n",
      "</a>\n"
     ]
    }
   ],
   "source": [
    "print(speaker_suffixes[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/speakers/ellen_t_hoen', '/speakers/sandra_aamodt', '/speakers/trevor_aaronson']\n"
     ]
    }
   ],
   "source": [
    "suffixes = [i.attrs[\"href\"] for i in speaker_suffixes[0:3]]\n",
    "print(suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = [i.attrs[\"href\"] for i in speaker_suffixes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.ted.com/speakers/ellen_t_hoen', 'https://www.ted.com/speakers/sandra_aamodt']\n"
     ]
    }
   ],
   "source": [
    "urls = [str(\"https://www.ted.com\"+suffix) for suffix in suffixes[0:2]]\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [str(\"https://www.ted.com\"+suffix) for suffix in suffixes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./speakers/speaker_urls.txt', 'w') as f:\n",
    "    for item in urls:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# name: <h1 class=\"h2 profile-header__name\">\n",
    "# occupation: <div class=\"p2 profile-header__summary\">\n",
    "# intro: <div class=\"profile-intro\">\n",
    "# profile: <div class=\"section section--minor\">\n",
    "\n",
    "\n",
    "def parse(soup):\n",
    "    # both title and views are can be parsed in separate tags.\n",
    "    name = soup.find('h1', {'class' : \"h2 profile-header__name\"}).text.strip('\\n')\n",
    "    occupation = soup.find('div', {'class' : \"p2 profile-header__summary\"}).text.strip('\\n')\n",
    "    intro = soup.find('div', {'class' : \"profile-intro\"}).text.strip('\\n')\n",
    "    profile = soup.find('div', {'class' : \"section section--minor\"}).text.strip('\\n')\n",
    "    return name, occupation, intro, profile\n",
    "\n",
    "def to_csv(pth, out):\n",
    "    # open file to write to.\n",
    "    with open(out, \"w\") as out:\n",
    "        # create csv.writer.\n",
    "        wr = csv.writer(out)\n",
    "        # write our headers.\n",
    "        wr.writerow([\"title\", \"views\", \"descr\"])\n",
    "        # get all our html files.\n",
    "        for html in os.listdir(pth):\n",
    "            with open(os.path.join(pth, html)) as f:\n",
    "                print(html)\n",
    "                # parse the file and write the data to a row.\n",
    "                wr.writerow(parse(BeautifulSoup(f, \"lxml\")))\n",
    "\n",
    "# This is the ACTION:\n",
    "to_csv(\"./html_files/speakers/\",\"speakers.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
