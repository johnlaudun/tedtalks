{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Pages that List Speakers\n",
    "\n",
    "After some rooting around on the TED website, examining the Speakers home page, it looks like there are 86 pages that list TED speakers, with URLS like this: `https://www.ted.com/speakers?page=1`. My plan is to:\n",
    "\n",
    "1. create a list of pages in a text file, \n",
    "2. download the 86 pages, \n",
    "3. parse the speakers out,\n",
    "4. create a second list of pages with the `ted.com/speakers/speaker_name\" format`\n",
    "5. download all those pages\n",
    "6. parse them into a CSV for KK.\n",
    "\n",
    "Oh, I hope this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's going to be a lot of files created in this process, so it's important to remember where I am:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/john/Code/tedtalks/data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As always, the collected imports for this notebook are at the top:\n",
    "\n",
    "import re, csv, os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a list of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ted.com/speakers?page=1\n",
      "https://www.ted.com/speakers?page=2\n",
      "https://www.ted.com/speakers?page=3\n",
      "https://www.ted.com/speakers?page=4\n"
     ]
    }
   ],
   "source": [
    "# This is just proof of concept. I actually used range(1,87) to get\n",
    "# the list I needed and pasted it into a text document.\n",
    "for i in range(1,5):\n",
    "    print(\"https://www.ted.com/speakers?page=\" + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download the pages\n",
    "\n",
    "Okay, the text file is `speaker_index_pages.txt` in the data/speakers/directory. I used the following to download all 86 speaker pages to the indices directory:\n",
    "\n",
    "    wget -w 2 -i ../speaker_index_pages.txt\n",
    "\n",
    "Then, being lazy and not wanting to figure out how to parse all the files into one list (see below), I simply concatenated all the files into one, with the plan to use `BeautifulSoup` to run through it.\n",
    "\n",
    "    cat indices/* > speakers_all_pages.txt\n",
    "\n",
    "### Step 3: Parse the Speaker URLs Out of the Pages\n",
    "\n",
    "Inside the HTML, each speaker's profile can be found in the following line:\n",
    "\n",
    "    <a class=\"results__result media media--sm-v m4\" href=\"/speakers/ellen_t_hoen\">\n",
    "\n",
    "So we need the `href` attribute for the `results__result` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./speakers/speakers_all_pages.txt', 'r') as myfile:\n",
    "    data = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the_soup = BeautifulSoup(data, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(the_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"results__result media media--sm-v m4\" href=\"/speakers/ellen_t_hoen\">\n",
      "<div class=\"media__image media__image--thumb\">\n",
      "<span class=\"thumb thumb--square\"><span class=\"thumb__sizer\"><span class=\"thumb__tugger\"><img alt=\"\" class=\" thumb__image\" play=\"false\" src=\"https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/bcd3208945bf8b311418b64f917b1f38e84a24e8_800x600.jpg?h=191&amp;w=254\"/><span class=\"thumb__aligner\"></span></span></span></span>\n",
      "</div>\n",
      "<div class=\"media__message\">\n",
      "<h4 class=\"h7 m5\">\n",
      "Ellen<br/>'t Hoen</h4>\n",
      "<p class=\"p4\">\n",
      "<strong>Medicine law expert</strong>\n",
      "</p>\n",
      "</div>\n",
      "</a>\n"
     ]
    }
   ],
   "source": [
    "speaker_suffix = the_soup.find('a', {'class':'results__result media media--sm-v m4'})\n",
    "print(speaker_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> /speakers/ellen_t_hoen\n"
     ]
    }
   ],
   "source": [
    "speaker_suffix = the_soup.find('a', {'class':'results__result'})['href']\n",
    "print(type(speaker_suffix), speaker_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_suffixes = the_soup.findAll('a', {'class':'results__result media media--sm-v m4'})\n",
    "len(speaker_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this only returning 30 items? Using search in a plain text editor (Atom) on the same files above, there are 2572 occurrences of **`results__result`**.\n",
    "\n",
    "```python\n",
    "# Archived code that was meant to generate URLs from the soup above\n",
    "suffixes = [i.attrs[\"href\"] for i in speaker_suffixes]\n",
    "urls = [str(\"https://www.ted.com\"+suffix) for suffix in suffixes]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using regex to get the speaker suffix\n",
    "\n",
    "Okay, I'm still not sure what's going on with the truncated input above, but what I am going to try to do below is read the file in as a string and then use regex to grab the speaker suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2572"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffixes = re.findall(r'/speakers/(.*?)\\'>', data)\n",
    "len(suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = [str(\"https://www.ted.com/speakers/\"+suffix) for suffix in suffixes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.ted.com/speakers/ellen_t_hoen',\n",
       " 'https://www.ted.com/speakers/sandra_aamodt',\n",
       " 'https://www.ted.com/speakers/trevor_aaronson',\n",
       " 'https://www.ted.com/speakers/chris_abani',\n",
       " 'https://www.ted.com/speakers/yassmin_abdel_magied']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a second list with the speaker suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./speakers/speaker_urls.txt', 'w') as f:\n",
    "    for item in urls:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5** was done using **`wget`**: \n",
    "\n",
    "```bash\n",
    "wget -w 2 -i ../speaker_urls.txt\n",
    "```\n",
    "\n",
    "And here's the eventual report:\n",
    "\n",
    "```\n",
    "FINISHED --2018-11-15 22:05:00--\n",
    "Total wall clock time: 2h 0m 43s\n",
    "Downloaded: 2569 files, 91M in 7.0s (13.1 MB/s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Parse the speaker profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/john/Code/tedtalks/data/speakers\n"
     ]
    }
   ],
   "source": [
    "% cd speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mindices\u001b[m\u001b[m/                 speaker_index_pages.txt  speakers.csv\r\n",
      "\u001b[1m\u001b[34mprofiles\u001b[m\u001b[m/                speaker_urls.txt         speakers_all_pages.txt\r\n"
     ]
    }
   ],
   "source": [
    "% ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the HTML where things are found:\n",
    "\n",
    "* **Name**: `<meta name=\"author\" content=\"Aala El-Khani\" />` or `<h1 class=\"h2 profile-header__name\">`\n",
    "* **Occupation**: `<div class=\"p2 profile-header__summary\">`\n",
    "* **Intro**: `<div class=\"profile-intro\">`\n",
    "* **Profile**: `<div class=\"section section--minor\">`\n",
    "\n",
    "These are all converted into **`BS4`** searches below. (Fingers crossed that these work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(soup):\n",
    "    name = soup.find('h1', {'class' : 'h2 profile-header__name'}).text.strip('\\n')\n",
    "    occupation = soup.find('div', {'class' : 'p2 profile-header__summary'}).text.strip('\\n')\n",
    "    intro = soup.find('div', {'class' : 'profile-intro'}).text.strip('\\n')\n",
    "    profile = soup.find('div', {'class' : 'section section--minor'}).text.strip('\\n')\n",
    "    return name, occupation, intro, profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_csv(pth, out):\n",
    "    # open file to write to.\n",
    "    with open(out, \"w\") as out:\n",
    "        # create csv.writer\n",
    "        wr = csv.writer(out)\n",
    "        # write headers\n",
    "        wr.writerow([\"name\", \"occupation\", \"introduction\", \"profile\"])\n",
    "        # get all our html files\n",
    "        for html in os.listdir(pth):\n",
    "            with open(os.path.join(pth, html)) as f:\n",
    "                # print(html) # prints off name as it goes?\n",
    "                # parse the file and write the data to a row.\n",
    "                wr.writerow(parse(BeautifulSoup(f, \"lxml\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gary_haugen\n",
      "rabbi_lord_jonathan_sacks\n",
      "joe_kowan\n",
      "allan_adams\n",
      "peter_tyack\n",
      "heather_brooke\n",
      "reed_hastings\n",
      "oscar_schwartz\n",
      "marwa_al_sabouni\n",
      "steven_addis\n",
      "nizar_ibrahim\n",
      "ian_bremmer\n",
      "lisa_harouni\n",
      "shashi_tharoor\n",
      "sergey_brin\n",
      "candy_chang\n",
      "susan_solomon\n",
      "jk_rowling\n",
      "diane_kelly\n",
      "thelma_golden\n",
      "john_kasaona\n",
      "lee_smolin\n",
      "elon_musk\n",
      "yossi_vardi\n",
      "david_logan\n",
      "christiane_amanpour\n",
      "ndidi_nwuneli\n",
      "jonas_gahr_store\n",
      "larry_brilliant\n",
      "zaria_forman\n",
      "marc_abrahams\n",
      "adam_driver\n",
      "halla_tomasdottir\n",
      "sarah_donnelly\n",
      "courtney_e_martin\n",
      "joshua_klein\n",
      "john_gable\n",
      "john_la_grou\n",
      "christine_sun_kim\n",
      "deborah_gordon\n",
      "bart_knols\n",
      "einstein_the_parrot\n",
      "mike_degruy\n",
      "peter_gabriel\n",
      "jill_heinerth\n",
      "haas_hahn\n",
      "benjamin_wallace\n",
      "ethan_zuckerman\n",
      "marco_tempest\n",
      "pam_warhurst\n",
      "neri_oxman\n",
      "cynthia_kenyon\n",
      "chimamanda_ngozi_adichie\n",
      "jonathan_marks\n",
      "hannah_brencher\n",
      "birke_baehr\n",
      "abraham_verghese\n",
      "katie_hinde\n",
      "sheila_patek\n",
      "nirmalya_kumar\n",
      "raj_panjabi\n",
      "kate_stafford\n",
      "lauren_sallan\n",
      "sian_leah_beilock\n",
      "jimmy_lin\n",
      "elizabeth_lindsey\n",
      "manwar_ali\n",
      "peter_fankhauser\n",
      "sue_desmond_hellman\n",
      "anindya_kundu\n",
      "jack_choi\n",
      "stuart_duncan\n",
      "peter_molyneux\n",
      "damon_davis\n",
      "ingrid_betancourt\n",
      "lidia_yuknavitch\n",
      "molly_stevens\n",
      "patrick_awuah\n",
      "keith_schacht\n",
      "jason_mccue\n",
      "diane_wolk_rogers\n",
      "evgeny_morozov\n",
      "damon_horowitz\n",
      "adrianne_haslet_davis\n",
      "simon_anholt\n",
      "sendhil_mullainathan\n",
      "roger_mcnamee\n",
      "rose_goslinga\n",
      "joshua_foer\n",
      "stephanie_busari\n",
      "mitchell_besser\n",
      "christiana_figueres\n",
      "rei\n",
      "nic_marks\n",
      "donald_hoffman\n",
      "james_watson\n",
      "chris_bangle\n",
      "edwidge_danticat\n",
      "krista_donaldson\n",
      "luke_syson\n",
      "rives\n",
      "ramsey_musallam\n",
      "bryan_stevenson\n",
      "michael_archer\n",
      "michael_green_spi\n",
      "sam_kass\n",
      "sangu_delle\n",
      "janet_iwasa\n",
      "stanley_mcchrystal\n",
      "cathy_o_neil\n",
      "kate_darling\n",
      "linda_hill\n",
      "fred_jansen\n",
      "they_might_be_giants\n",
      "trevor_timm\n",
      "alexis_ohanian\n",
      "elif_shafak\n",
      "ric_elias\n",
      "carol_dweck\n",
      "graham_hawkes\n",
      "amy_tan\n",
      "alex_wissner_gross\n",
      "supasorn_suwajanakorn\n",
      "kayla_briet\n",
      "william_mcdonough\n",
      "laura_l_dunn\n",
      "vikram_sharma\n",
      "sydney_chaffee\n",
      "casey_gerald\n",
      "jinha_lee\n",
      "norman_lear\n",
      "dread_scott\n",
      "ruth_chang\n",
      "kristin_poinar\n",
      "eric_haseltine\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-407c80542bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This is the ACTION:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./profiles/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"speakers.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-25b6008d0cae>\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(pth, out)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;31m# parse the file and write the data to a row.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mwr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-f409d930d8ba>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(soup)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'h2 profile-header__name'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moccupation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'p2 profile-header__summary'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mintro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'profile-intro'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'section section--minor'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# This is the ACTION:\n",
    "to_csv(\"./profiles/\",\"speakers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('speakers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>occupation</th>\n",
       "      <th>introduction</th>\n",
       "      <th>profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gary Haugen</td>\n",
       "      <td>Human rights attorney</td>\n",
       "      <td>As founder of International Justice Mission, G...</td>\n",
       "      <td>While a member of the 1994 United Nations te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rabbi Lord Jonathan Sacks</td>\n",
       "      <td>Religious leader</td>\n",
       "      <td>In a world violently polarized by extremists, ...</td>\n",
       "      <td>Rabbi Lord Sacks is one of Judaism's spiritua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joe Kowan</td>\n",
       "      <td>Musician and graphic designer</td>\n",
       "      <td>By day he's a graphic designer, and by night J...</td>\n",
       "      <td>Joe Kowan is a Boston-based musician and grap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Allan Adams</td>\n",
       "      <td>Theoretical physicist</td>\n",
       "      <td>Allan Adams is a theoretical physicist working...</td>\n",
       "      <td>Allan Adams is a theoretical physicist workin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peter Tyack</td>\n",
       "      <td>Behavioral ecologist</td>\n",
       "      <td>Peter Tyack studies the social behavior and ac...</td>\n",
       "      <td>Peter Tyack, a senior scientist in biology at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name                     occupation  \\\n",
       "0                Gary Haugen          Human rights attorney   \n",
       "1  Rabbi Lord Jonathan Sacks               Religious leader   \n",
       "2                  Joe Kowan  Musician and graphic designer   \n",
       "3                Allan Adams          Theoretical physicist   \n",
       "4                Peter Tyack           Behavioral ecologist   \n",
       "\n",
       "                                        introduction  \\\n",
       "0  As founder of International Justice Mission, G...   \n",
       "1  In a world violently polarized by extremists, ...   \n",
       "2  By day he's a graphic designer, and by night J...   \n",
       "3  Allan Adams is a theoretical physicist working...   \n",
       "4  Peter Tyack studies the social behavior and ac...   \n",
       "\n",
       "                                             profile  \n",
       "0    While a member of the 1994 United Nations te...  \n",
       "1   Rabbi Lord Sacks is one of Judaism's spiritua...  \n",
       "2   Joe Kowan is a Boston-based musician and grap...  \n",
       "3   Allan Adams is a theoretical physicist workin...  \n",
       "4   Peter Tyack, a senior scientist in biology at...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
