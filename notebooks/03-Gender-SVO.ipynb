{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject-Verb-Objects\n",
    "\n",
    "In this notebook, we conduct a series of experiments in order, first, toisolate the subject-verb-object (SVO) triples in the texts of speakers we have gendered male or female, then to explore the *character spaces* they establish for gendered entities within their speech as well as the nature of the *character space* they create for themselves as speakers. \n",
    "\n",
    "**Note**: We are not excluding parentheticals in this notebook.\n",
    "\n",
    "**Next Steps**: Work on code to compile / visualize this as a network graph (?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From our 992x14 CSV, we have a list of 992 talks: 260 by women and 720 by men.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import re, spacy, textacy\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "talks_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "talks_all = pd.concat([talks_m, talks_f, talks_nog])\n",
    "\n",
    "# And then grabbing on the texts of the talks:\n",
    "texts_all = talks_all.text.tolist()\n",
    "texts_women = talks_f.text.tolist()\n",
    "texts_men = talks_m.text.tolist()\n",
    "\n",
    "print(f\"From our {talks_all.shape[0]}x{talks_all.shape[1]} CSV, \\\n",
    "we have a list of {len(texts_all)} talks: {len(texts_women)} by women and \\\n",
    "{len(texts_men)} by men.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing everything upfront because we don't care whether it is *She* or *she*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase everything before we create spaCy doc and Textacy SVO triple\n",
    "texts_w = [text.lower() for text in texts_women]\n",
    "texts_m = [text.lower() for text in texts_men]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has three different English language models: small, medium, and large. We use the large model here because our corpus is small and the syntax may be a bit more involved. \n",
    "\n",
    "`TO DO`: determine the difference between the models. \n",
    "\n",
    "After determining telling spaCy which model to use, we then use its conventions for feeding a set of texts as a list of strings, to it. \n",
    "\n",
    "The preview simply checks that everything went as planned: it gives us a word count and the first 50 characters -- which is weird because in theory it has converted the string to a series of spacy objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(2690 tokens: \"  thank you so much, chris. and it\\'s truly a gr...\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Use the pipe method to feed documents \n",
    "docs_w = list(nlp.pipe(texts_w))\n",
    "docs_m = list(nlp.pipe(texts_m))\n",
    "\n",
    "# A quick check of our work:\n",
    "docs_m[0]._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gendered SVOs Dataframes\n",
    "\n",
    "We now have two sets of spaCy documents, one for the women speakers and one for the men speakers -- there is a third set, composed of groups of speakes, but we are leaving them aside for now. \n",
    "\n",
    "In order to examine how each set of texts imagines the agency and actions of gendered persons in their speech, we are going to go through all the clauses in each talk, identify those clauses that have a pronoun as a subject. We are going to compile all the clauses that feature all the subject case pronouns: *first person* (I and we), *second person* (you), and *third person* (she, he, they, it).\n",
    "\n",
    "`TO DO`: Identify the clauses where gendered entities are acted upon: her, him, me, us, them. (Convert the **spaCy object** in the function below to strings so that we can then match for the objective pronouns?) This may or may not be possibly best done as a separate section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells we create the list of pronouns for whom we seek SVO triples and then we have the function that will grab SVOs starting with those pronouncs and append them to a list each item of which is a dictionary. Each dictionary consists of three entries: one for the subject, one for the verb, and one for the object. This structure was chosen because it makes it easier to instantiate a pandas dataframe in the work that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the lists of gendered pronouns\n",
    "pronouns = ['i', 'we', 'she', 'he', 'they', 'it', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function which will get the SVOs\n",
    "def actions(terms, doc, svo_list):\n",
    "    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))\n",
    "    for term in terms:\n",
    "        for item in svotriples:\n",
    "            if str(item[0][-1]) == term:\n",
    "                svo_list.append(\n",
    "                    {\n",
    "                        'subject': str(item[0][-1]), \n",
    "                        'verb': str(item[1][-1]), \n",
    "                        'object': str(item[2])\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the two lists of dictionaries, one each for talks we have gendered as female and male based on the speaker, we create two dataframes each with three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56781, 3) (18602, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create the two lists\n",
    "svos_m = []\n",
    "svos_w = []\n",
    "\n",
    "# Populate the lists with SVO triples\n",
    "for doc in docs_m:\n",
    "    actions(pronouns, doc, svos_m)\n",
    "\n",
    "for doc in docs_w:\n",
    "    actions(pronouns, doc, svos_w)\n",
    "\n",
    "# Convert the lists to dataframes\n",
    "df_w = pd.DataFrame(svos_w)\n",
    "df_m = pd.DataFrame(svos_m)\n",
    "\n",
    "print(df_m.shape, df_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is simply survey the pronouns: make sure they are present and then to count the number of verbs associated with each one. The total here should match the total length of the dataframe, 18,602. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb_w</th>\n",
       "      <th>percentage_w</th>\n",
       "      <th>verb_m</th>\n",
       "      <th>percentage_m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>739</td>\n",
       "      <td>0.039727</td>\n",
       "      <td>2529</td>\n",
       "      <td>0.044540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>6220</td>\n",
       "      <td>0.334373</td>\n",
       "      <td>15502</td>\n",
       "      <td>0.273014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1342</td>\n",
       "      <td>0.072143</td>\n",
       "      <td>4646</td>\n",
       "      <td>0.081823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>636</td>\n",
       "      <td>0.034190</td>\n",
       "      <td>842</td>\n",
       "      <td>0.014829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>1919</td>\n",
       "      <td>0.103161</td>\n",
       "      <td>5780</td>\n",
       "      <td>0.101795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>4645</td>\n",
       "      <td>0.249704</td>\n",
       "      <td>15517</td>\n",
       "      <td>0.273278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>3101</td>\n",
       "      <td>0.166703</td>\n",
       "      <td>11965</td>\n",
       "      <td>0.210722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         verb_w  percentage_w  verb_m  percentage_m\n",
       "subject                                            \n",
       "he          739      0.039727    2529      0.044540\n",
       "i          6220      0.334373   15502      0.273014\n",
       "it         1342      0.072143    4646      0.081823\n",
       "she         636      0.034190     842      0.014829\n",
       "they       1919      0.103161    5780      0.101795\n",
       "we         4645      0.249704   15517      0.273278\n",
       "you        3101      0.166703   11965      0.210722"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pf = pronoun frequency\n",
    "\n",
    "# Count the rows with each pronoun as the subject:\n",
    "procount_m = df_m.groupby([\"subject\"]).count()\n",
    "procount_w = df_w.groupby([\"subject\"]).count()\n",
    "\n",
    "# Drop the OBJECT column\n",
    "procount_w.drop('object', axis=1, inplace=True)\n",
    "# Create PERCENTAGE column\n",
    "procount_w['percentage'] = procount_w['verb'] /  procount_w['verb'].sum()\n",
    "\n",
    "# Repeat above for men speakers\n",
    "procount_m.drop('object', axis=1, inplace=True)\n",
    "procount_m['percentage'] = procount_m['verb'] /  procount_m['verb'].sum()\n",
    "\n",
    "# Merge the two dataframes\n",
    "pf_compare = procount_w.merge(procount_m, \n",
    "                        left_on='subject', \n",
    "                        right_on='subject',\n",
    "                        suffixes=('_w', '_m'))\n",
    "\n",
    "# See the results\n",
    "pf_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> The code below works, but it uses raw counts and it probably needs to be a percentage so that one can compare the mens' and womens' subcorpora. </div>\n",
    "\n",
    "Apologies for the awkward line wraps: the pandas code got long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the top 20 verbs for each pronoun\n",
    "pv_w = df_w.groupby([\"subject\", \"verb\"])\\\n",
    ".size()\\\n",
    ".groupby(level=0).nlargest(20)\\\n",
    ".reset_index(level=0, drop=True)\\\n",
    ".reset_index(name='Count')\n",
    "\n",
    "# Save to CSV for easier viewing\n",
    "# pv_w.to_csv('../output/pv_w.csv')\n",
    "\n",
    "# Repeat for the men\n",
    "pv_m = df_m.groupby([\"subject\", \"verb\"])\\\n",
    ".size()\\\n",
    ".groupby(level=0).nlargest(20)\\\n",
    ".reset_index(level=0, drop=True)\\\n",
    ".reset_index(name='Count')\n",
    "# pv_m.to_csv('../output/pv_m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he': [48, 49, 141, 142, 143, 144, 145, 146, 147, 148, 305, 306, 391, 433, 493, 494, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 866, 974, 975, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1466, 1467, 1468, 1469, 1470, 1571, 1572, 1573, 1574, 1575, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1736, 1737, 1738, 1739, 1878, 1879, 1880, 1881, 1927, 2119, 2296, 2297, 2365, 2366, 2367, 2368, 2369, 2492, 2493, 2494, 2495, 2496, 2497, 2733, 2734, 2735, 2736, 2737, 2738, ...], 'i': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 191, 192, 193, ...], 'it': [53, 54, 172, 321, 322, 323, 324, 325, 326, 327, 392, 393, 434, 435, 436, 518, 519, 520, 521, 522, 523, 524, 525, 711, 712, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 1006, 1007, 1008, 1009, 1010, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1586, 1587, 1588, 1589, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, ...], 'she': [44, 45, 46, 47, 138, 139, 140, 390, 680, 681, 682, 683, 684, 685, 686, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 972, 973, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, ...], 'they': [50, 51, 52, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 976, 977, 978, ...], 'we': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 137, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, ...], 'you': [55, 56, 57, 58, 59, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 437, 438, 439, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, ...]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w.groupby(\"subject\").groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>he</td>\n",
       "      <td>does</td>\n",
       "      <td>[which, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>he</td>\n",
       "      <td>married</td>\n",
       "      <td>[mom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>he</td>\n",
       "      <td>led</td>\n",
       "      <td>[them]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>he</td>\n",
       "      <td>stopped</td>\n",
       "      <td>[nephites]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>he</td>\n",
       "      <td>visited</td>\n",
       "      <td>[nephites]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>he</td>\n",
       "      <td>having</td>\n",
       "      <td>[conversation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>he</td>\n",
       "      <td>doing</td>\n",
       "      <td>[it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>he</td>\n",
       "      <td>goes</td>\n",
       "      <td>[you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>he</td>\n",
       "      <td>turns</td>\n",
       "      <td>[you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>he</td>\n",
       "      <td>tells</td>\n",
       "      <td>[you]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>739 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject     verb          object\n",
       "48         he     does   [which, time]\n",
       "49         he  married           [mom]\n",
       "141        he      led          [them]\n",
       "142        he  stopped      [nephites]\n",
       "143        he  visited      [nephites]\n",
       "...       ...      ...             ...\n",
       "18244      he   having  [conversation]\n",
       "18245      he    doing            [it]\n",
       "18246      he     goes           [you]\n",
       "18247      he    turns           [you]\n",
       "18248      he    tells           [you]\n",
       "\n",
       "[739 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w.groupby(\"subject\").get_group('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives you a dataframe with just the index\n",
    "# and the verb\n",
    "df2 = df_w.groupby(['subject'])[['verb']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df_w.groupby(['subject', 'verb'])\\\n",
    ".size()\\\n",
    ".groupby(level=0).nlargest(5)\\\n",
    ".reset_index(level=0, drop=True)\\\n",
    ".reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All SVOs to Dataframe\n",
    "\n",
    "Since we create SVOs for every sentence in the two subcorpora, why not save both to two dataframes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions(doc, svo_list):\n",
    "    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))\n",
    "    for item in svotriples:\n",
    "        svo_list.append(\n",
    "            {\n",
    "                'subject': str(item[0][-1]), \n",
    "                'verb': str(item[1][-1]), \n",
    "                'object': str(item[2])\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80331, 3) (26527, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create the two lists\n",
    "all_svos_m = []\n",
    "all_svos_w = []\n",
    "\n",
    "# Populate the lists with SVO triples\n",
    "for doc in docs_m:\n",
    "    actions(doc, all_svos_m)\n",
    "\n",
    "for doc in docs_w:\n",
    "    actions(doc, all_svos_w)\n",
    "\n",
    "# Convert the lists to dataframes\n",
    "df_all_svos_w = pd.DataFrame(all_svos_w)\n",
    "df_all_svos_m = pd.DataFrame(all_svos_m)\n",
    "\n",
    "print(df_all_svos_m.shape, df_all_svos_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. If the counts above are accurate, that there are 80,331 SVOs in total in the male speaker subcorpora and 56,781 begin with on of the pronouns listed above and 26,527 total SVOs for the female speaker subcorpus with 18,602 beginning with pronouns, then the preponderance of sentences in TED talks begin with a rather small set of pronouns:\n",
    "\n",
    "```\n",
    "56,781 / 80,331 = .706\n",
    "18,602 / 26,527 = .701\n",
    "```\n",
    "Is that possible? Or is there some flaw in the SVO code base or in my code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving the Number of Sentences\n",
    "\n",
    "The code above suggests that 70% of the SVOs in TED talks have `'i', 'we', 'she', 'he', 'they', 'it', 'you'` as their subject. It's not clear, however, how much the SVO pattern represents all sentences in the talks. In this section we explore counting sentences, both through NLTK and spaCy, but also a hand count of a few sample texts to see how well our code is reflecting underlying realities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "sents_w = [ sent_tokenize(text) for text in texts_w ]    \n",
    "sents_m = [ sent_tokenize(text) for text in texts_m ]\n",
    "\n",
    "print(len(sents_w[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30799\n"
     ]
    }
   ],
   "source": [
    "sent_count_w = 0\n",
    "sent_per_talk = []\n",
    "\n",
    "for in_text in sents_w:\n",
    "    num_sents = len(in_text)\n",
    "    sent_count_w += num_sents\n",
    "    sent_per_talk.append(num_sents)\n",
    "    \n",
    "print(sent_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96342\n"
     ]
    }
   ],
   "source": [
    "sent_count_m = 0\n",
    "for text in texts_m:\n",
    "    sent_count_m += len(sent_tokenize(text))\n",
    "print(sent_count_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30799\n"
     ]
    }
   ],
   "source": [
    "sent_count_w = 0\n",
    "for text in texts_w:\n",
    "    sent_count_w += len(sent_tokenize(text))\n",
    "print(sent_count_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two blocks of code give us NLTK's answer to total number of sentences for all the talks, which is larger than the total number of SVOs:\n",
    "\n",
    "```\n",
    "26,527 SVOs in 30,799 sentences in female speaker subcorpus\n",
    "80,331 SVOs in 96,342 sentences in male speaker subcorpus\n",
    "```\n",
    "That makes for the following percentages of pronoun SVOs out of the total number of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6039806487223611\n",
      "0.5893691225010899\n"
     ]
    }
   ],
   "source": [
    "print(f\"{18602 / 30799}\")\n",
    "print(f\"{56781 / 96342}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Further Experiments with the NLTK Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_w = sent_count/len(sents_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_per_talk.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 12,\n",
       " 15,\n",
       " 18,\n",
       " 18,\n",
       " 22,\n",
       " 24,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 30,\n",
       " 31,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 33,\n",
       " 33,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 38,\n",
       " 39,\n",
       " 39,\n",
       " 40,\n",
       " 40,\n",
       " 42,\n",
       " 42,\n",
       " 43,\n",
       " 43,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 46,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 51,\n",
       " 53,\n",
       " 55,\n",
       " 55,\n",
       " 55,\n",
       " 56,\n",
       " 56,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 63,\n",
       " 63,\n",
       " 65,\n",
       " 68,\n",
       " 69,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 71,\n",
       " 72,\n",
       " 72,\n",
       " 73,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 78,\n",
       " 79,\n",
       " 79,\n",
       " 79,\n",
       " 80,\n",
       " 80,\n",
       " 82,\n",
       " 82,\n",
       " 83,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 90,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 94,\n",
       " 94,\n",
       " 95,\n",
       " 95,\n",
       " 96,\n",
       " 96,\n",
       " 96,\n",
       " 96,\n",
       " 96,\n",
       " 97,\n",
       " 97,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 110,\n",
       " 110,\n",
       " 112,\n",
       " 115,\n",
       " 117,\n",
       " 118,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 133,\n",
       " 135,\n",
       " 137,\n",
       " 137,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 139,\n",
       " 141,\n",
       " 141,\n",
       " 141,\n",
       " 143,\n",
       " 143,\n",
       " 144,\n",
       " 144,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 150,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 155,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 159,\n",
       " 160,\n",
       " 164,\n",
       " 165,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 170,\n",
       " 171,\n",
       " 171,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 180,\n",
       " 180,\n",
       " 181,\n",
       " 181,\n",
       " 181,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 185,\n",
       " 187,\n",
       " 190,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 196,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 203,\n",
       " 208,\n",
       " 213,\n",
       " 214,\n",
       " 218,\n",
       " 218,\n",
       " 220,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 231,\n",
       " 233,\n",
       " 236,\n",
       " 239,\n",
       " 242,\n",
       " 248,\n",
       " 252,\n",
       " 255,\n",
       " 258,\n",
       " 260,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 269,\n",
       " 348]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_per_talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([27., 41., 55., 42., 41., 30., 14.,  9.,  0.,  1.]),\n",
       " array([  1. ,  35.7,  70.4, 105.1, 139.8, 174.5, 209.2, 243.9, 278.6,\n",
       "        313.3, 348. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sent_per_talk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  if you're here today — and i'm\n",
      "however, when we're not at ted, we\n",
      "and that's because most people with decision-making\n",
      "   the reason why i'm here today, in\n",
      "she turned out to be a much bigger dog than\n",
      "when she came into my life, we were fighting\n",
      "the area also has one of the lowest ratios of\n",
      "   so when i was contacted by the parks department\n",
      "i'd lived in this area all my life,\n",
      "then, while jogging with my dog one morning,\n",
      "there were weeds and piles of garbage and other stuff\n",
      "i knew that this forgotten little street-end,\n",
      "and i knew it would grow to become the proud\n",
      "we garnered much support along the way, and the\n",
      "we leveraged that $10,000 seed grant more than 300\n",
      "   and in the fall, i'm going to\n",
      "   (audience whistles)\n",
      "   thank you very much.\n",
      "   (applause)    that's him pressing my\n",
      "   (laughter)    (applause)    but\n",
      "we feel the problems right now, and have for\n",
      "environmental justice, for those of you who may not\n",
      "   unfortunately, race and class are extremely reliable indicators\n",
      "as a black person in america, i am twice\n",
      "i am five times more likely to live within walking\n",
      "these land-use decisions created the hostile conditions that\n",
      "why would someone leave their home to go for a\n",
      "our 27 percent obesity rate is high even for this\n",
      "one out of four south bronx children has asthma.\n",
      "our asthma hospitalization rate is seven times higher than the\n",
      "these impacts are coming everyone's way.\n",
      "and we all pay dearly for solid waste costs,\n",
      "fifty percent of our residents live at or below the\n",
      "low-income citizens often use emergency-room visits\n",
      "this comes at a high cost to taxpayers and produces\n",
      "poor people are not only still poor, they are\n",
      "   fortunately, there are many people like me who\n",
      "none of us want that, and we all have\n",
      "so what else do we have in common?\n",
      "   well, first of all, we're all\n",
      "   (laughter)    graduated high school, college\n",
      "ok. good.\n",
      "   (laughter)    but, besides being a\n",
      "i watched nearly half of the buildings in my neighborhood\n",
      "my big brother lenny fought in vietnam, only to\n",
      "jesus.\n",
      "i grew up with a crack house across the street\n",
      "yeah,\n",
      "i'm a poor black child from the ghetto.\n",
      "these things make me different from you.\n",
      "but the things we have in common set me apart\n",
      "   so how did things get so different for us\n",
      "in the late '40s, my dad — a\n",
      "at the time, the community was a mostly white\n",
      "my dad was not alone.\n",
      "and as others like him pursued their own version of\n",
      "red-lining was used by banks, wherein certain\n",
      "many landlords believed it was more profitable to torch their\n",
      "   hunts point was formerly a walk-to-\n",
      "a national highway construction boom was added to our problems\n",
      "in new york state, robert moses spearheaded an aggressive\n",
      "one of its primary goals was to make it easier\n",
      "the south bronx, which lies in between, did\n",
      "residents were often given less than a month's notice\n",
      "600,000 people were displaced.\n",
      "the common perception was that only pimps and pushers and\n",
      "and if you are told from your earliest days that\n",
      "so now, my family's property was worthless,\n",
      "and luckily for me, that home and the love\n",
      "   now, why is this story important?\n",
      "because from a planning perspective, economic degradation begets environmental\n",
      "the disinvestment that began in the 1960s set the stage\n",
      "antiquated zoning and land-use regulations are still used\n",
      "are these factors taken into consideration when land-use\n",
      "what costs are associated with these decisions?\n",
      "and who pays?\n",
      "who profits?\n",
      "does anything justify what the local community goes through?\n",
      "this was \"planning\" — in quotes — that\n",
      "   once we realized that, we decided it was\n",
      "that small park i told you about earlier was the\n",
      "i wrote a one-and-a-quarter\n",
      "physical improvements help inform public policy regarding traffic safety,\n",
      "they provide opportunities to be more physically active, as\n",
      "think bike shops, juice stands.\n",
      "we secured 20 million dollars to build first-phase\n",
      "this is lafayette avenue — and that's redesigned by\n",
      "and once this path is constructed, it'll connect\n",
      "right now we're separated by about 25 feet of\n",
      "   as we nurture the natural environment, its abundance\n",
      "we run a project called the bronx [environmental]\n",
      "little by little, we're seeding the area with\n",
      "the sheridan expressway is an underutilized relic of the robert\n",
      "even during rush hour, it goes virtually unused.\n",
      "the community created an alternative transportation plan that allows for\n",
      "we have the opportunity now to bring together all the\n",
      "   we also built new york city's first green\n",
      "cool roofs are highly-reflective surfaces that don't\n",
      "green roofs are soil and living plants.\n",
      "both can be used instead of petroleum-based roofing\n",
      "green roofs also retain up to 75 percent of rainfall\n",
      "and they provide habitats for our little friends!\n",
      "   [butterfly]    (laughter)    so\n",
      "   anyway, the demonstration project is a springboard for\n",
      "   [green is the new black ...]    \n",
      "   anyway, i know chris told us not to\n",
      "end of pitch.\n",
      "it's better to ask for forgiveness than permission.\n",
      "anyway —    (laughter)    (applause)\n",
      "ok. katrina.\n",
      "   prior to katrina, the south bronx and new\n",
      "both were largely populated by poor people of color,\n",
      "both are waterfront communities that host both industries and residents\n",
      "in the post-katrina era, we have still\n",
      "we're at best ignored, and maligned and abused\n",
      "neither the destruction of the ninth ward nor the south\n",
      "but we have emerged with valuable lessons about how to\n",
      "we are more than simply national symbols of urban blight\n",
      "now will we let the gulf coast languish for a\n",
      "or will we take proactive steps and learn from the\n",
      "   now listen, i do not expect individuals,\n",
      "this presentation today only represents some of what i've\n",
      "like a tiny little bit.\n",
      "you've no clue.\n",
      "but i'll tell you later, if you want\n",
      "   (laughter)    but — i know it\n",
      "i'm interested in what i like to call the\n",
      "developments that have the potential to create positive returns for\n",
      "   at present, that's not happening in new\n",
      "and we are operating with a comprehensive urban-planning\n",
      "a parade of government subsidies is going to propose big\n",
      "and their approaches to local economic and job development are\n",
      "because on top of that, the world's richest\n",
      "now, we'll have even less than that stat\n",
      "and although less than 25 percent of south bronx residents\n",
      "now, what's missing from the larger debate is\n",
      "my agency is working closely with columbia university and others\n",
      "   now let's get this straight: i am\n",
      "ours is a city, not a wilderness preserve.\n",
      "and i've embraced my inner capitalist.\n",
      "and, but i don't have —    (\n",
      "   (laughter)    so i don't have\n",
      "there's enough precedent out there to show that a\n",
      "fellow tedsters bill mcdonough and amory lovins — both heroes\n",
      "i do have a problem with developments that hyper-\n",
      "that it continues is a shame upon us all,\n",
      "but one of the things i do to remind myself\n",
      "this is my version of globalization.\n",
      "   let's take bogota.\n",
      "poor, latino, surrounded by runaway gun violence and\n",
      "however, this city was blessed in the late 1990s\n",
      "he looked at the demographics.\n",
      "few bogotanos own cars, yet a huge portion of\n",
      "if you're a mayor, you can do something\n",
      "his administration narrowed key municipal thoroughfares from five lanes to\n",
      "for his brilliant efforts, he was nearly impeached.\n",
      "but as people began to see that they were being\n",
      "people stopped littering.\n",
      "crime rates dropped, because the streets were alive with\n",
      "his administration attacked several typical urban problems at one time\n",
      "we have no excuse in this country, i'm\n",
      "but the bottom line is: their people-first\n",
      "that development should not come at the expense of the\n",
      "   you, however, are blessed with the gift\n",
      "that's why you're here and why you value\n",
      "use your influence in support of comprehensive, sustainable change\n",
      "don't just talk about it at ted.\n",
      "this is a nationwide policy agenda i'm trying to\n",
      "help me make green the new black.\n",
      "help me make sustainability sexy.\n",
      "make it a part of your dinner and cocktail conversations\n",
      "help me fight for environmental and economic justice.\n",
      "support investments with a triple-bottom-line return\n",
      "help me democratize sustainability by bringing everyone to the table\n",
      "oh good, glad i have a little more time\n",
      "   listen — when i spoke to mr. gore\n",
      "his response was a grant program.\n",
      "i don't think he understood that i wasn't\n",
      "i was making him an offer.\n",
      "   (applause)    what troubled me was that\n",
      "now, don't get me wrong, we need\n",
      "   (laughter)    but grassroots groups are needed\n",
      "of the 90 percent of the energy that mr.\n",
      "   (applause)    i have come from so\n",
      "please don't waste me.\n",
      "by working together, we can become one of those\n",
      "we might have come to this conference from very,\n",
      "we have nothing to lose and everything to gain.\n",
      "   ciao, bellos!\n",
      "   (applause)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# If you append [0:10] to sent in the print statement\n",
    "# it gives you just the first part of a sentence:\n",
    "# useful for scanning for subjects\n",
    "\n",
    "for sent in docs_w[0].sents:\n",
    "    print(sent[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "print(len(list(docs_w[0].sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31673\n"
     ]
    }
   ],
   "source": [
    "snt_cnt_w = 0\n",
    "for doc in docs_w:\n",
    "    snt_cnt_w += len(list(doc.sents))\n",
    "print(snt_cnt_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99039\n"
     ]
    }
   ],
   "source": [
    "snt_cnt_m = 0\n",
    "for doc in docs_m:\n",
    "    snt_cnt_m += len(list(doc.sents))\n",
    "print(snt_cnt_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total sentence counts are:\n",
    "```\n",
    "Women - NLTK : 30,799\n",
    "        spaCy: 31,673\n",
    "Men -   NLTK : 96,342\n",
    "        spaCy: 99,039\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
