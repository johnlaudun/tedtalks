{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject-Verb-Objects\n",
    "\n",
    "In this notebook, we conduct a series of experiments in order, first, toisolate the subject-verb-object (SVO) triples in the texts of speakers we have gendered male or female, then to explore the *character spaces* they establish for gendered entities within their speech as well as the nature of the *character space* they create for themselves as speakers. \n",
    "\n",
    "**Note**: We are not excluding parentheticals in this notebook.\n",
    "\n",
    "**Next Steps**: Work on code to compile / visualize this as a network graph (?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From our 992x14 CSV, we have a list of 992 talks: 260 by women and 720 by men.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import re, spacy, textacy\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "talks_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "talks_all = pd.concat([talks_m, talks_f, talks_nog])\n",
    "\n",
    "# And then grabbing on the texts of the talks:\n",
    "texts_all = talks_all.text.tolist()\n",
    "texts_women = talks_f.text.tolist()\n",
    "texts_men = talks_m.text.tolist()\n",
    "\n",
    "print(f\"From our {talks_all.shape[0]}x{talks_all.shape[1]} CSV, \\\n",
    "we have a list of {len(texts_all)} talks: {len(texts_women)} by women and \\\n",
    "{len(texts_men)} by men.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing everything upfront because we don't care whether it is *She* or *she*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase everything before we create spaCy doc and Textacy SVO triple\n",
    "texts_w = [text.lower() for text in texts_women]\n",
    "texts_m = [text.lower() for text in texts_men]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has three different English language models: small, medium, and large. We use the large model here because our corpus is small and the syntax may be a bit more involved. \n",
    "\n",
    "`TO DO`: determine the difference between the models. \n",
    "\n",
    "After determining telling spaCy which model to use, we then use its conventions for feeding a set of texts as a list of strings, to it. \n",
    "\n",
    "The preview simply checks that everything went as planned: it gives us a word count and the first 50 characters -- which is weird because in theory it has converted the string to a series of spacy objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(2690 tokens: \"  thank you so much, chris. and it\\'s truly a gr...\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Use the pipe method to feed documents \n",
    "docs_w = list(nlp.pipe(texts_w))\n",
    "docs_m = list(nlp.pipe(texts_m))\n",
    "\n",
    "# A quick check of our work:\n",
    "docs_m[0]._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gendered SVOs Dataframes\n",
    "\n",
    "We now have two sets of spaCy documents, one for the women speakers and one for the men speakers -- there is a third set, composed of groups of speakes, but we are leaving them aside for now. \n",
    "\n",
    "In order to examine how each set of texts imagines the agency and actions of gendered persons in their speech, we are going to go through all the clauses in each talk, identify those clauses that have a pronoun as a subject. We are going to compile all the clauses that feature all the subject case pronouns: *first person* (I and we), *second person* (you), and *third person* (she, he, they, it).\n",
    "\n",
    "`TO DO`: Identify the clauses where gendered entities are acted upon: her, him, me, us, them. (Convert the **spaCy object** in the function below to strings so that we can then match for the objective pronouns?) This may or may not be possibly best done as a separate section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells we create the list of pronouns for whom we seek SVO triples and then we have the function that will grab SVOs starting with those pronouncs and append them to a list each item of which is a dictionary. Each dictionary consists of three entries: one for the subject, one for the verb, and one for the object. This structure was chosen because it makes it easier to instantiate a pandas dataframe in the work that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the lists of gendered pronouns\n",
    "pronouns = ['i', 'we', 'she', 'he', 'they', 'it', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function which will get the SVOs\n",
    "def actions(terms, doc, svo_list):\n",
    "    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))\n",
    "    for term in terms:\n",
    "        for item in svotriples:\n",
    "            if str(item[0][-1]) == term:\n",
    "                svo_list.append(\n",
    "                    {\n",
    "                        'subject': str(item[0][-1]), \n",
    "                        'verb': str(item[1][-1]), \n",
    "                        'object': str(item[2])\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the two lists of dictionaries, one each for talks we have gendered as female and male based on the speaker, we create two dataframes each with three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56781, 3) (18602, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create the two lists\n",
    "svos_m = []\n",
    "svos_w = []\n",
    "\n",
    "# Populate the lists with SVO triples\n",
    "for doc in docs_m:\n",
    "    actions(pronouns, doc, svos_m)\n",
    "\n",
    "for doc in docs_w:\n",
    "    actions(pronouns, doc, svos_w)\n",
    "\n",
    "# Convert the lists to dataframes\n",
    "df_w = pd.DataFrame(svos_w)\n",
    "df_m = pd.DataFrame(svos_m)\n",
    "\n",
    "print(df_m.shape, df_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is simply survey the pronouns: make sure they are present and then to count the number of verbs associated with each one. The total here should match the total length of the dataframe, 18,602. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb_w</th>\n",
       "      <th>percentage_w</th>\n",
       "      <th>verb_m</th>\n",
       "      <th>percentage_m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>739</td>\n",
       "      <td>0.039727</td>\n",
       "      <td>2529</td>\n",
       "      <td>0.044540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>6220</td>\n",
       "      <td>0.334373</td>\n",
       "      <td>15502</td>\n",
       "      <td>0.273014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1342</td>\n",
       "      <td>0.072143</td>\n",
       "      <td>4646</td>\n",
       "      <td>0.081823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>636</td>\n",
       "      <td>0.034190</td>\n",
       "      <td>842</td>\n",
       "      <td>0.014829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>1919</td>\n",
       "      <td>0.103161</td>\n",
       "      <td>5780</td>\n",
       "      <td>0.101795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>4645</td>\n",
       "      <td>0.249704</td>\n",
       "      <td>15517</td>\n",
       "      <td>0.273278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>3101</td>\n",
       "      <td>0.166703</td>\n",
       "      <td>11965</td>\n",
       "      <td>0.210722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         verb_w  percentage_w  verb_m  percentage_m\n",
       "subject                                            \n",
       "he          739      0.039727    2529      0.044540\n",
       "i          6220      0.334373   15502      0.273014\n",
       "it         1342      0.072143    4646      0.081823\n",
       "she         636      0.034190     842      0.014829\n",
       "they       1919      0.103161    5780      0.101795\n",
       "we         4645      0.249704   15517      0.273278\n",
       "you        3101      0.166703   11965      0.210722"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pf = pronoun frequency\n",
    "\n",
    "# Count the rows with each pronoun as the subject:\n",
    "procount_m = df_m.groupby([\"subject\"]).count()\n",
    "procount_w = df_w.groupby([\"subject\"]).count()\n",
    "\n",
    "# Drop the OBJECT column\n",
    "procount_w.drop('object', axis=1, inplace=True)\n",
    "# Create PERCENTAGE column\n",
    "procount_w['percentage'] = procount_w['verb'] /  procount_w['verb'].sum()\n",
    "\n",
    "# Repeat above for men speakers\n",
    "procount_m.drop('object', axis=1, inplace=True)\n",
    "procount_m['percentage'] = procount_m['verb'] /  procount_m['verb'].sum()\n",
    "\n",
    "# Merge the two dataframes\n",
    "pf_compare = procount_w.merge(procount_m, \n",
    "                        left_on='subject', \n",
    "                        right_on='subject',\n",
    "                        suffixes=('_w', '_m'))\n",
    "\n",
    "# See the results\n",
    "pf_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> The code below works, but it uses raw counts and it probably needs to be a percentage so that one can compare the mens' and womens' subcorpora. </div>\n",
    "\n",
    "Apologies for the awkward line wraps: the pandas code got long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the top 20 verbs for each pronoun\n",
    "pv_w = df_w.groupby([\"subject\", \"verb\"])\\\n",
    ".size()\\\n",
    ".groupby(level=0).nlargest(20)\\\n",
    ".reset_index(level=0, drop=True)\\\n",
    ".reset_index(name='Count')\n",
    "\n",
    "# Save to CSV for easier viewing\n",
    "# pv_w.to_csv('../output/pv_w.csv')\n",
    "\n",
    "# Repeat for the men\n",
    "pv_m = df_m.groupby([\"subject\", \"verb\"])\\\n",
    ".size()\\\n",
    ".groupby(level=0).nlargest(20)\\\n",
    ".reset_index(level=0, drop=True)\\\n",
    ".reset_index(name='Count')\n",
    "# pv_m.to_csv('../output/pv_m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he': [48, 49, 141, 142, 143, 144, 145, 146, 147, 148, 305, 306, 391, 433, 493, 494, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 866, 974, 975, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1466, 1467, 1468, 1469, 1470, 1571, 1572, 1573, 1574, 1575, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1736, 1737, 1738, 1739, 1878, 1879, 1880, 1881, 1927, 2119, 2296, 2297, 2365, 2366, 2367, 2368, 2369, 2492, 2493, 2494, 2495, 2496, 2497, 2733, 2734, 2735, 2736, 2737, 2738, ...], 'i': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 191, 192, 193, ...], 'it': [53, 54, 172, 321, 322, 323, 324, 325, 326, 327, 392, 393, 434, 435, 436, 518, 519, 520, 521, 522, 523, 524, 525, 711, 712, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 1006, 1007, 1008, 1009, 1010, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1586, 1587, 1588, 1589, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, ...], 'she': [44, 45, 46, 47, 138, 139, 140, 390, 680, 681, 682, 683, 684, 685, 686, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 972, 973, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, ...], 'they': [50, 51, 52, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 976, 977, 978, ...], 'we': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 137, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, ...], 'you': [55, 56, 57, 58, 59, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 437, 438, 439, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, ...]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w.groupby(\"subject\").groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>he</td>\n",
       "      <td>does</td>\n",
       "      <td>[which, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>he</td>\n",
       "      <td>married</td>\n",
       "      <td>[mom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>he</td>\n",
       "      <td>led</td>\n",
       "      <td>[them]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>he</td>\n",
       "      <td>stopped</td>\n",
       "      <td>[nephites]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>he</td>\n",
       "      <td>visited</td>\n",
       "      <td>[nephites]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>he</td>\n",
       "      <td>having</td>\n",
       "      <td>[conversation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>he</td>\n",
       "      <td>doing</td>\n",
       "      <td>[it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>he</td>\n",
       "      <td>goes</td>\n",
       "      <td>[you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>he</td>\n",
       "      <td>turns</td>\n",
       "      <td>[you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>he</td>\n",
       "      <td>tells</td>\n",
       "      <td>[you]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>739 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject     verb          object\n",
       "48         he     does   [which, time]\n",
       "49         he  married           [mom]\n",
       "141        he      led          [them]\n",
       "142        he  stopped      [nephites]\n",
       "143        he  visited      [nephites]\n",
       "...       ...      ...             ...\n",
       "18244      he   having  [conversation]\n",
       "18245      he    doing            [it]\n",
       "18246      he     goes           [you]\n",
       "18247      he    turns           [you]\n",
       "18248      he    tells           [you]\n",
       "\n",
       "[739 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w.groupby(\"subject\").get_group('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives you a dataframe with just the index\n",
    "# and the verb\n",
    "df2 = df_w.groupby(['subject'])[['verb']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df_w.groupby(['subject', 'verb'])\\\n",
    ".size()\\\n",
    ".groupby(level=0).nlargest(5)\\\n",
    ".reset_index(level=0, drop=True)\\\n",
    ".reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All SVOs to Dataframe\n",
    "\n",
    "Since we create SVOs for every sentence in the two subcorpora, why not save both to two dataframes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions(doc, svo_list):\n",
    "    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))\n",
    "    for item in svotriples:\n",
    "        svo_list.append(\n",
    "            {\n",
    "                'subject': str(item[0][-1]), \n",
    "                'verb': str(item[1][-1]), \n",
    "                'object': str(item[2])\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80331, 3) (26527, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create the two lists\n",
    "all_svos_m = []\n",
    "all_svos_w = []\n",
    "\n",
    "# Populate the lists with SVO triples\n",
    "for doc in docs_m:\n",
    "    actions(doc, all_svos_m)\n",
    "\n",
    "for doc in docs_w:\n",
    "    actions(doc, all_svos_w)\n",
    "\n",
    "# Convert the lists to dataframes\n",
    "df_all_svos_w = pd.DataFrame(all_svos_w)\n",
    "df_all_svos_m = pd.DataFrame(all_svos_m)\n",
    "\n",
    "print(df_all_svos_m.shape, df_all_svos_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. If the counts above are accurate, that there are 80,331 SVOs in total in the male speaker subcorpora and 56,781 begin with on of the pronouns listed above and 26,527 total SVOs for the female speaker subcorpus with 18,602 beginning with pronouns, then the preponderance of sentences in TED talks begin with a rather small set of pronouns:\n",
    "\n",
    "56,781 / 80,331 = .706\n",
    "18,602 / 26,527 = .701\n",
    "\n",
    "Is that possible? Or is there some flaw in the SVO code base or in my code?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
