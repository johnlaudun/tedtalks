{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From our 992x14 CSV, we have a list of 992 talks: 260 by women and 720 by men.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import re, spacy, textacy\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# If needed\n",
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\",  \n",
    "                  \"\\(video\\)\", \"\\(laughs\\)\", \"\\(applause ends\\)\", \n",
    "                  \"\\(audio\\)\", \"\\(singing\\)\", \"\\(music ends\\)\", \n",
    "                  \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \n",
    "                  \"\\(marimba sounds\\)\", \"\\(drum sounds\\)\" ]\n",
    "\n",
    "def remove_parentheticals(text):\n",
    "    global parentheticals\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), \n",
    "                          flags=re.IGNORECASE)\n",
    "    return new_text\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "talks_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "talks_all = pd.concat([talks_m, talks_f, talks_nog])\n",
    "\n",
    "# And then grabbing on the texts of the talks:\n",
    "texts = talks_all.text.tolist()\n",
    "texts_f = talks_f.text.tolist()\n",
    "texts_m = talks_m.text.tolist()\n",
    "\n",
    "print(f\"From our {talks_all.shape[0]}x{talks_all.shape[1]} CSV, \\\n",
    "we have a list of {len(texts)} talks: {len(texts_f)} by women and \\\n",
    "{len(texts_m)} by men.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textacy\n",
    "\n",
    "Textacy is fussy about the size of texts being fed it, responding with `ValueError`s for `nlp.maxlength`. The workaround here is to create a `docs` object which is a list of spaCy `doc`s. The preview below demonstrates that each item in the list has the characteristics of a spaCy doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Use the pipe method to feed documents \n",
    "docs = list(nlp.pipe(texts_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = textacy.Corpus(\"en_core_web_sm\", data=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(3740 tokens: \"  If you\\'re here today — and I\\'m very happy tha...\")'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   _SP SPACE\n",
      "If IN SCONJ\n",
      "you PRP PRON\n",
      "'re VBP AUX\n",
      "here RB ADV\n",
      "today NN NOUN\n",
      "— : PUNCT\n",
      "and CC CCONJ\n",
      "I PRP PRON\n",
      "'m VBP AUX\n",
      "very RB ADV\n",
      "happy JJ ADJ\n",
      "that IN SCONJ\n",
      "you PRP PRON\n",
      "are VBP AUX\n",
      "— : PUNCT\n",
      "you PRP PRON\n",
      "'ve VBP AUX\n",
      "all DT PRON\n",
      "heard VBN VERB\n"
     ]
    }
   ],
   "source": [
    "for token in docs[0][0:20]:\n",
    "    print (token, token.tag_, token.pos_) # spacy.explain(token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   _SP heard dep\n",
      "If IN 're mark\n",
      "you PRP 're nsubj\n",
      "'re VBP heard advcl\n",
      "here RB 're advmod\n",
      "today NN 're npadvmod\n",
      "— : 're punct\n",
      "and CC 're cc\n",
      "I PRP 'm nsubj\n",
      "'m VBP 're conj\n",
      "very RB happy advmod\n",
      "happy JJ 'm acomp\n",
      "that IN are mark\n",
      "you PRP are nsubj\n",
      "are VBP happy ccomp\n",
      "— : heard punct\n",
      "you PRP heard nsubj\n",
      "'ve VBP heard aux\n",
      "all DT heard dep\n",
      "heard VBN heard ROOT\n"
     ]
    }
   ],
   "source": [
    "for token in docs[0][0:20]:\n",
    "    print (token.text, token.tag_, token.head.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'(<VBN>)'\n",
    "verb_phrases = textacy.extract.matches.regex_matches(docs[0], pattern)\n",
    "\n",
    "len(list(verb_phrases))\n",
    "# for chunk in verb_phrases:\n",
    "#     print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "patterns = [{\"POS\": {\"IN\": [\"ADJ\", \"DET\"]}, \"OP\": \"+\"}, {\"ORTH\": {\"REGEX\": \"workers?\"}}]\n",
    "print(list(textacy.extract.token_matches(docs[0], patterns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVOs = list(textacy.extract.triples.subject_verb_object_triples(docs[0]))\n",
    "len(SVOs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVOTriple(subject=[development], verb=[will, save], object=[us])\n",
      "SVOTriple(subject=[She], verb=[turned], object=[to, be, a, much, bigger, dog, than, I, 'd, anticipated])\n",
      "SVOTriple(subject=[part], verb=[handled], object=[percent])\n",
      "SVOTriple(subject=[that], verb=[bring], object=[truck, trips])\n",
      "SVOTriple(subject=[area], verb=[has], object=[one])\n",
      "SVOTriple(subject=[I], verb=[was, contacted], object=[Parks, Department])\n",
      "SVOTriple(subject=[I], verb=[mentioned], object=[that])\n",
      "SVOTriple(subject=[she], verb=[pulled], object=[me])\n",
      "SVOTriple(subject=[she], verb=[were], object=[dragging, me])\n",
      "SVOTriple(subject=[I], verb=[wo, n't, mention], object=[that])\n"
     ]
    }
   ],
   "source": [
    "for item in SVOs[0:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[development]\n",
      "[She]\n",
      "[part]\n",
      "[that]\n",
      "[area]\n",
      "[I]\n",
      "[I]\n",
      "[she]\n",
      "[she]\n",
      "[I]\n"
     ]
    }
   ],
   "source": [
    "for item in SVOs[0:10]:\n",
    "    print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVOTriple(subject=[development], verb=[will, save], object=[us])\n"
     ]
    }
   ],
   "source": [
    "svo = SVOs[0]\n",
    "print(svo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[development]\n"
     ]
    }
   ],
   "source": [
    "s = (svo[0])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, spacy.tokens.token.Token found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mz/k5p3f5wj0czgd_5m4dw59py00000gn/T/ipykernel_29073/2914014531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, spacy.tokens.token.Token found"
     ]
    }
   ],
   "source": [
    "''.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mz/k5p3f5wj0czgd_5m4dw59py00000gn/T/ipykernel_29073/3806252375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "s.text"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
