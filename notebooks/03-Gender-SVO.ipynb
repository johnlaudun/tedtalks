{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject-Verb-Objects\n",
    "\n",
    "In this notebook, we conduct a series of experiments in order, first, toisolate the subject-verb-object (SVO) triples in the texts of speakers we have gendered male or female, then to explore the *character spaces* they establish for gendered entities within their speech as well as the nature of the *character space* they create for themselves as speakers. \n",
    "\n",
    "**Note**: We are not excluding parentheticals in this notebook.\n",
    "\n",
    "**Next Steps**: Work on code to compile / visualize this as a network graph (?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From our 992x14 CSV, we have a list of 992 talks: 260 by women and 720 by men.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import re, spacy, textacy\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "talks_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "talks_all = pd.concat([talks_m, talks_f, talks_nog])\n",
    "\n",
    "# And then grabbing on the texts of the talks:\n",
    "texts_all = talks_all.text.tolist()\n",
    "texts_women = talks_f.text.tolist()\n",
    "texts_men = talks_m.text.tolist()\n",
    "\n",
    "print(f\"From our {talks_all.shape[0]}x{talks_all.shape[1]} CSV, \\\n",
    "we have a list of {len(texts_all)} talks: {len(texts_women)} by women and \\\n",
    "{len(texts_men)} by men.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing everything upfront because we don't care whether it is *She* or *she*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase everything before we create spaCy doc and Textacy SVO triple\n",
    "texts_w = [text.lower() for text in texts_women]\n",
    "texts_m = [text.lower() for text in texts_men]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has three different English language models: small, medium, and large. We use the large model here because our corpus is small and the syntax may be a bit more involved. \n",
    "\n",
    "`TO DO`: determine the difference between the models. \n",
    "\n",
    "After determining telling spaCy which model to use, we then use its conventions for feeding a set of texts as a list of strings, to it. \n",
    "\n",
    "The preview simply checks that everything went as planned: it gives us a word count and the first 50 characters -- which is weird because in theory it has converted the string to a series of spacy objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(2690 tokens: \"  thank you so much, chris. and it\\'s truly a gr...\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Use the pipe method to feed documents \n",
    "docs_w = list(nlp.pipe(texts_w))\n",
    "docs_m = list(nlp.pipe(texts_m))\n",
    "\n",
    "# A quick check of our work:\n",
    "docs_m[0]._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gendered SVOs Dataframes\n",
    "\n",
    "We now have two sets of spaCy documents, one for the women speakers and one for the men speakers -- there is a third set, composed of groups of speakes, but we are leaving them aside for now. \n",
    "\n",
    "In order to examine how each set of texts imagines the agency and actions of gendered persons in their speech, we are going to go through all the clauses in each talk, identify those clauses that have a pronoun as a subject. We are going to compile all the clauses that feature all the subject case pronouns: *first person* (I and we), *second person* (you), and *third person* (she, he, they, it).\n",
    "\n",
    "`TO DO`: Identify the clauses where gendered entities are acted upon: her, him, me, us, them. (Convert the **spaCy object** in the function below to strings so that we can then match for the objective pronouns?) This may or may not be possibly best done as a separate section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells we create the list of pronouns for whom we seek SVO triples and then we have the function that will grab SVOs starting with those pronouncs and append them to a list each item of which is a dictionary. Each dictionary consists of three entries: one for the subject, one for the verb, and one for the object. This structure was chosen because it makes it easier to instantiate a pandas dataframe in the work that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the lists of gendered pronouns\n",
    "pronouns = ['i', 'we', 'she', 'he', 'they', 'it', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function which will get the SVOs\n",
    "def actions(terms, doc, svo_list):\n",
    "    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))\n",
    "    for term in terms:\n",
    "        for item in svotriples:\n",
    "            if str(item[0][-1]) == term:\n",
    "                svo_list.append(\n",
    "                    {\n",
    "                        'subject': str(item[0][-1]), \n",
    "                        'verb': str(item[1][-1]), \n",
    "                        'object': str(item[2])\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the two lists of dictionaries, one each for talks we have gendered as female and male based on the speaker, we create two dataframes each with three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two lists\n",
    "svos_m = []\n",
    "svos_w = []\n",
    "\n",
    "# Populate the lists with SVO triples\n",
    "for doc in docs_m:\n",
    "    actions(pronouns, doc, svos_m)\n",
    "\n",
    "for doc in docs_w:\n",
    "    actions(pronouns, doc, svos_w)\n",
    "\n",
    "# Convert the lists to dataframes\n",
    "df_w = pd.DataFrame(svos_w)\n",
    "df_m = pd.DataFrame(svos_m)\n",
    "\n",
    "print(df_m.shape, df_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is simply survey the pronouns: make sure they are present and then to count the number of verbs associated with each one. The total here should match the total length of the dataframe, 18,602. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf = pronoun frequency\n",
    "\n",
    "# Count the rows with each pronoun as the subject:\n",
    "procount_m = df_m.groupby([\"subject\"]).count()\n",
    "procount_w = df_w.groupby([\"subject\"]).count()\n",
    "\n",
    "# Drop the OBJECT column\n",
    "procount_w.drop('object', axis=1, inplace=True)\n",
    "# Create PERCENTAGE column\n",
    "procount_w['percentage'] = procount_w['verb'] /  procount_w['verb'].sum()\n",
    "\n",
    "# Repeat above for men speakers\n",
    "procount_m.drop('object', axis=1, inplace=True)\n",
    "procount_m['percentage'] = procount_m['verb'] /  procount_m['verb'].sum()\n",
    "\n",
    "# Merge the two dataframes\n",
    "pf_compare = procount_w.merge(procount_m, \n",
    "                        left_on='subject', \n",
    "                        right_on='subject',\n",
    "                        suffixes=('_w', '_m'))\n",
    "\n",
    "# See the results\n",
    "pf_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> The code below works, but it uses raw counts and it probably needs to be a percentage so that one can compare the mens' and womens' subcorpora. </div>\n",
    "\n",
    "Apologies for the awkward line wraps: the pandas code got long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the top 20 verbs for each pronoun\n",
    "pv_w = df_w.groupby([\"subject\", \"verb\"])\\\n",
    ".size()\\\n",
    ".groupby(level=0).nlargest(20)\\\n",
    ".reset_index(level=0, drop=True)\\\n",
    ".reset_index(name='Count')\n",
    "\n",
    "# Save to CSV for easier viewing\n",
    "# pv_w.to_csv('../output/pv_w.csv')\n",
    "\n",
    "# Repeat for the men\n",
    "pv_m = df_m.groupby([\"subject\", \"verb\"])\\\n",
    ".size()\\\n",
    ".groupby(level=0).nlargest(20)\\\n",
    ".reset_index(level=0, drop=True)\\\n",
    ".reset_index(name='Count')\n",
    "# pv_m.to_csv('../output/pv_m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_w.groupby(\"subject\").groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_w.groupby(\"subject\").get_group('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives you a dataframe with just the index\n",
    "# and the verb\n",
    "df2 = df_w.groupby(['subject'])[['verb']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df_w.groupby(['subject', 'verb'])\\\n",
    ".size()\\\n",
    ".groupby(level=0).nlargest(5)\\\n",
    ".reset_index(level=0, drop=True)\\\n",
    ".reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All SVOs to Dataframe\n",
    "\n",
    "Since we create SVOs for every sentence in the two subcorpora, why not save both to two dataframes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions(doc, svo_list):\n",
    "    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))\n",
    "    for item in svotriples:\n",
    "        svo_list.append(\n",
    "            {\n",
    "                'subject': str(item[0][-1]), \n",
    "                'verb': str(item[1][-1]), \n",
    "                'object': str(item[2])\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two lists\n",
    "all_svos_m = []\n",
    "all_svos_w = []\n",
    "\n",
    "# Populate the lists with SVO triples\n",
    "for doc in docs_m:\n",
    "    actions(doc, all_svos_m)\n",
    "\n",
    "for doc in docs_w:\n",
    "    actions(doc, all_svos_w)\n",
    "\n",
    "# Convert the lists to dataframes\n",
    "df_all_svos_w = pd.DataFrame(all_svos_w)\n",
    "df_all_svos_m = pd.DataFrame(all_svos_m)\n",
    "\n",
    "print(df_all_svos_m.shape, df_all_svos_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. If the counts above are accurate, that there are 80,331 SVOs in total in the male speaker subcorpora and 56,781 begin with on of the pronouns listed above and 26,527 total SVOs for the female speaker subcorpus with 18,602 beginning with pronouns, then the preponderance of sentences in TED talks begin with a rather small set of pronouns:\n",
    "\n",
    "```\n",
    "56,781 / 80,331 = .706\n",
    "18,602 / 26,527 = .701\n",
    "```\n",
    "Is that possible? Or is there some flaw in the SVO code base or in my code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving the Number of Sentences\n",
    "\n",
    "The code above suggests that 70% of the SVOs in TED talks have `'i', 'we', 'she', 'he', 'they', 'it', 'you'` as their subject. It's not clear, however, how much the SVO pattern represents all sentences in the talks. In this section we explore counting sentences, both through NLTK and spaCy, but also a hand count of a few sample texts to see how well our code is reflecting underlying realities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_w = [ sent_tokenize(text) for text in texts_w ]    \n",
    "sents_m = [ sent_tokenize(text) for text in texts_m ]\n",
    "\n",
    "print(len(sents_w[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_count_w = 0\n",
    "sent_per_talk = []\n",
    "\n",
    "for in_text in sents_w:\n",
    "    num_sents = len(in_text)\n",
    "    sent_count_w += num_sents\n",
    "    sent_per_talk.append(num_sents)\n",
    "    \n",
    "print(sent_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_count_m = 0\n",
    "for text in texts_m:\n",
    "    sent_count_m += len(sent_tokenize(text))\n",
    "print(sent_count_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_count_w = 0\n",
    "for text in texts_w:\n",
    "    sent_count_w += len(sent_tokenize(text))\n",
    "print(sent_count_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two blocks of code give us NLTK's answer to total number of sentences for all the talks, which is larger than the total number of SVOs:\n",
    "\n",
    "```\n",
    "26,527 SVOs in 30,799 sentences in female speaker subcorpus\n",
    "80,331 SVOs in 96,342 sentences in male speaker subcorpus\n",
    "```\n",
    "That makes for the following percentages of pronoun SVOs out of the total number of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{18602 / 30799}\")\n",
    "print(f\"{56781 / 96342}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Further Experiments with the NLTK Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_w = sent_count/len(sents_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_per_talk.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_per_talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sent_per_talk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you append [0:10] to sent in the print statement\n",
    "# it gives you just the first part of a sentence:\n",
    "# useful for scanning for subjects\n",
    "\n",
    "for sent in docs_w[0].sents:\n",
    "    print(sent[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(docs_w[0].sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt_cnt_w = 0\n",
    "for doc in docs_w:\n",
    "    snt_cnt_w += len(list(doc.sents))\n",
    "print(snt_cnt_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt_cnt_m = 0\n",
    "for doc in docs_m:\n",
    "    snt_cnt_m += len(list(doc.sents))\n",
    "print(snt_cnt_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total sentence counts are:\n",
    "```\n",
    "Women - NLTK : 30,799\n",
    "        spaCy: 31,673\n",
    "Men -   NLTK : 96,342\n",
    "        spaCy: 99,039\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
