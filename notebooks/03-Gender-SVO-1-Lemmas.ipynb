{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVO Verbs  Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re, spacy, textacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "talks_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "talks_all = pd.concat([talks_m, talks_f, talks_nog])\n",
    "\n",
    "# And then grabbing on the texts of the talks:\n",
    "texts_all = talks_all.text.tolist()\n",
    "texts_women = talks_f.text.tolist()\n",
    "texts_men = talks_m.text.tolist()\n",
    "\n",
    "# Lowercase everything before we create spaCy doc and Textacy SVO triple\n",
    "texts_w = [text.lower() for text in texts_women]\n",
    "texts_m = [text.lower() for text in texts_men]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Texts to SVOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# nlp.add_pipe('sentencizer')\n",
    "# nlp.remove_pipe(\"lemmatizer\")\n",
    "# nlp.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"}).initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(2690 tokens: \"  thank you so much, chris. and it\\'s truly a gr...\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the pipe method to feed documents \n",
    "docs_w = list(nlp.pipe(texts_w))\n",
    "docs_m = list(nlp.pipe(texts_m))\n",
    "\n",
    "# A quick check of our work:\n",
    "docs_m[0]._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [iPython documentation](https://ipython.readthedocs.io/en/stable/config/extensions/storemagic.html), we can `%store` variables so that if the notebook crashes, we need not run the code above again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'docs_w' (list)\n",
      "Stored 'docs_m' (list)\n"
     ]
    }
   ],
   "source": [
    "%store docs_w docs_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  thank you so much, chris. and it's truly a great honor to have the opportunity to come to this stage twice; i'm extremely grateful. i have been blown away by this conference, and i want to thank all of you for the many nice comments about what i had to say the other night. and i say that sincerely, partly because (mock sob) i need that.    (laughter)    put yourselves in my position.    (laughter)    i flew on air force"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_m[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  ', 'thank', 'you', 'so', 'much', ',', 'chris', '.', 'and', 'it', 'be', 'truly', 'a', 'great', 'honor', 'to', 'have', 'the', 'opportunity', 'to', 'come', 'to', 'this', 'stage', 'twice', ';', 'I', 'be', 'extremely', 'grateful', '.', 'I', 'have', 'be', 'blow', 'away', 'by', 'this', 'conference', ',', 'and', 'I', 'want', 'to', 'thank', 'all', 'of', 'you', 'for', 'the', 'many', 'nice', 'comment', 'about', 'what', 'I', 'have', 'to', 'say', 'the', 'other', 'night', '.', 'and', 'I', 'say', 'that', 'sincerely', ',', 'partly', 'because', '(', 'mock', 'sob', ')', 'I', 'need', 'that', '.', '   ', '(', 'laughter', ')', '   ', 'put', 'yourself', 'in', 'my', 'position', '.', '   ', '(', 'laughter', ')', '   ', 'I', 'fly', 'on', 'air', 'force']\n"
     ]
    }
   ],
   "source": [
    "lemmatas = [token.lemma_ for token in docs_m[0][0:100]]\n",
    "print(lemmatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [token.lemma_ for token in docs_w[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_w = []\n",
    "for doc in docs_w:\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    lemmas_w.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_m = []\n",
    "for doc in docs_m:\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    lemmas_m.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   thank you so much , chris . and it be truly a great honor to have the opportunity to come to this stage twice ; I be extremely grateful . I have be blow away by this conference , and I want to thank all of you for the many nice comment about what I have to say the other night . and I say that sincerely , partly because ( mock sob ) I need that .     ( laughter )     put yourself in my position .     ( laughter )     I fly on air force'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(lemmas_m[0][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = ' '.join(lemmas_m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'sents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mz/k5p3f5wj0czgd_5m4dw59py00000gn/T/ipykernel_6090/2522562233.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz_svos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject_verb_object_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/site-packages/textacy/extract/triples.py\u001b[0m in \u001b[0;36msubject_verb_object_triples\u001b[0;34m(doclike)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoclike\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoclike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'sents'"
     ]
    }
   ],
   "source": [
    "z_svos = list(textacy.extract.triples.subject_verb_object_triples(joined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'lemma_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mz/k5p3f5wj0czgd_5m4dw59py00000gn/T/ipykernel_6090/341782573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts_m\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/mz/k5p3f5wj0czgd_5m4dw59py00000gn/T/ipykernel_6090/341782573.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts_m\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'lemma_'"
     ]
    }
   ],
   "source": [
    "lemmas = [token.lemma_ for token in texts_m[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVOs to Dataframe\n",
    "\n",
    "Since we create SVOs for every sentence in the two subcorpora, why not save both to two dataframes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSVOs(doc, svo_list):\n",
    "    # Create the list of tuples for the document\n",
    "    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))\n",
    "    # Convert to list of dictionaries\n",
    "    for item in svotriples:\n",
    "        svo_list.append(\n",
    "            {\n",
    "                'subject': str(item[0][-1]), \n",
    "                'verb': str(item[1][-1]), \n",
    "                'object': str(item[2])\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Small Experiment to get to Lemmas\n",
    "\n",
    "With Z standing in for *docs_m[0]* above, which is Al Gore's TED talk, we are trying to get the lemma of the verbs in the SVOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zsvos = list(textacy.extract.triples.subject_verb_object_triples(docs_m[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SVOTriple(subject=[i], verb=[have, been, blown], object=[conference]), SVOTriple(subject=[i], verb=[want], object=[to, thank, all, of, you, for, the, many, nice, comments, about, what, i, had, to, say, the, other, night]), SVOTriple(subject=[i], verb=[need], object=[that]), SVOTriple(subject=[laughter], verb=[put], object=[yourselves]), SVOTriple(subject=[i], verb=[flew], object=[two])]\n"
     ]
    }
   ],
   "source": [
    "print(Zsvos[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Zsvos' (list) to file 'Zsvos.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Zsvos >Zsvos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = pd.DataFrame(Zsvos)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(item):\n",
    "    token = str(item[1][-1])\n",
    "    lemma = token.lemma_\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textacy.extract.triples.SVOTriple"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Zsvos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z['verb'] = Z['verb'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two lists\n",
    "svos_m = []\n",
    "svos_w = []\n",
    "\n",
    "# Populate the lists with SVO triples\n",
    "for doc in docs_m:\n",
    "    createSVOs(doc, svos_m)\n",
    "\n",
    "for doc in docs_w:\n",
    "    createSVOs(doc, svos_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80460 26610\n"
     ]
    }
   ],
   "source": [
    "# Convert the lists to dataframes\n",
    "svos_w = pd.DataFrame(svos_w)\n",
    "svos_m = pd.DataFrame(svos_m)\n",
    "\n",
    "print(svos_m.shape[0], svos_w.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-SVO Lemmatizing\n",
    "\n",
    "Two possible approaches to lemmatizing verbs in a dataframe:\n",
    "* [How to lemmatise a dataframe column Python - Stack Overflow](https://stackoverflow.com/questions/61987040/how-to-lemmatise-a-dataframe-column-python)\n",
    "* [dataframe - lemmatizing a verb list in a data frame in Python - Stack Overflow](https://stackoverflow.com/questions/72394840/lemmatizing-a-verb-list-in-a-data-frame-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "wnl = WordNetLemmatizer()\n",
    "svos_w.verb = svos_w.verb.map(lambda word: wnl.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26610, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svos_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "svos_m.verb = svos_m.verb.map(lambda word: wnl.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files \n",
    "# >>> Commented out once run\n",
    "svos_w.to_csv(\"../output/svos_w_lem.csv\")\n",
    "svos_m.to_csv(\"../output/svos_m_lem.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
