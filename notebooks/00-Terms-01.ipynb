{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Term Frequencies\n",
    "\n",
    "With the TEDtalks-all dataset created, we have 1747 talks with which to work. This is a small corpus, and so the usual reasons for shrinking the feature set for the texts do not apply, but as we begin our survey of the contents of the TED talks we wanted to be mindful of standards that had emerged both so that our results would be comparable to the work of others but also so that we could potentially scale up the work here without having to re-think the foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we load the complete dataset of both TED and TED+ talks. (No TEDx talks.) We use Python's `Sci-Kit Learn` library to create a document - term frequency matrix with a shape of 1747 x 50379. Summing the words to get a total for each word across all talks in the dataset, we then hand-inspect the totals and discover that there is a small list of numbers that recur. We approach mapping those numbers in two trials before returning to the task of getting a clean frequency list with no repetitions or other oddities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd, re, csv, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 27)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Data\n",
    "df = pd.read_csv('../output/TEDall_speakers.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do**: Edit the CSV to remove the vestigial index column at the start of each line. Then use `df.set_index('Talk_ID')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies\n",
    "\n",
    "The sole purpose of this notebook is to establish how we are going to elicit our features, our words, from the collection of talks. Thus, the only column we are interested in is the one with the texts of the talks. As we move forward, however, we will want to decide if we are simply going to append ~30,000 columns to a version of the extant CSV or create a separate CSV for each experiment. \n",
    "\n",
    "For this first experiment, we will keep it simple, creating two lists, one of the URLs and one of the texts. The URLs are unique, human-friendly identifiers for the talks. (We can, perhaps, make them a bit more friendly by modifying them a bit, subtracting `https://www.ted.com/talks/` from each.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls  = df.public_url.tolist()\n",
    "texts = df.text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to get term frequencies, but **SciKit-Learn**'s `CountVectorizer` is, I think, the way to go, since it will work well with the other vectorizers and models also available in `sklearn`.\n",
    "\n",
    "In our first experiment, we run `CountVectorizer` unadorned. The default options are: lowercase everything, get rid of all punctuation, make a word out of anything more than two characters long. The only thing that might not be welcome is the splitting of contractions. For now, we will leave things as they are. (Also, please note, no stopwords were used, so we have an unfiltered word list.)\n",
    "\n",
    "For this current work, we are running `fit()` and `transform()` separately, but since `fit()` just calculates the parameters and saves them as an internal objects state `transform()`  applies the transformation to a particular set of examples (the ones we just fitted), the two operations are usually simply done at the same time as `fit_transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 50379)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to pass options, pass them here:\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the model to the data \n",
    "vecs = vectorizer.fit(texts)\n",
    "\n",
    "# transform the data according to the fitted model\n",
    "bow = vecs.transform(texts)\n",
    "\n",
    "# see how many features we have\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Totals per Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can total up our columns for each feature (word), which is something we will be doing per year, per gender, per discipline. Here, we take the vector describing a word and sum it. We then pair the sum with the word in a tuple, which we then sort by frequency. \n",
    "\n",
    "(I'm doing it this way because it appears to be the way to do it, but it also strikes me that there should be a way to do this within the array itself, or, perhaps, to do it through **pandas**.)\n",
    "\n",
    "We save the results to a CSV file so that we can hand-check the words: are these the results we expected? (We don't want any weirdness affecting our overall results.) The hand inspection looks good. I didn't see anything in words 4 or above in frequency that looked off. (So, the simplest solution works!) What I did note was the frequency of certain **numbers**: **100**, **12**, etc. This might be worth taking a closer look: are there *power numbers*? (I am thinking here of Alan Dundes' essay on the \"power\" of three in American culture.)\n",
    "\n",
    "**To Do**: It would be nice to be able grab all words of a certain frequency, or range of frequencies.\n",
    "\n",
    "---\n",
    "**Follow-up**: whenever I attempt some version of\n",
    "```python\n",
    "for item in vecs:\n",
    "    if vecs.vocabulary_.get(item) == 1691:\n",
    "        print(item)\n",
    "```\n",
    "I get **`TypeError: 'CountVectorizer' object is not iterable`**. My best guess, for now, is that we need to use the tuple above to get this information.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 166093), ('and', 118989), ('to', 102276), ('of', 92416), ('that', 76268), ('in', 62673), ('it', 59191), ('you', 56296), ('we', 54458), ('is', 50072), ('this', 38510), ('so', 29001), ('they', 25157), ('was', 24582), ('for', 24445), ('are', 22592), ('have', 21965), ('but', 21804), ('on', 20978), ('what', 20907)]\n"
     ]
    }
   ],
   "source": [
    "# summing up the counts for each word\n",
    "sum_words = bow.sum(axis=0)\n",
    "\n",
    "# create a tuple\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vecs.vocabulary_.items()]\n",
    "\n",
    "# sort the tuple\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# check the results of our work by printing the top 20 more frequent words\n",
    "print(words_freq[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../output/word_freq.csv','w') as out:\n",
    "#     csv_out = csv.writer(out)\n",
    "#     csv_out.writerow(['word','count'])\n",
    "#     for row in words_freq:\n",
    "#         csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently Occurring Numbers\n",
    "\n",
    "One of the dimensions of the corpus that arises out of a hand inspection of the terms if the frequency with which some numbers appear. The follow table captures the top ten numbers:\n",
    "\n",
    "| TERM | FREQUENCY |\n",
    "|------|-----------|\n",
    "| 000  | 2098 |\n",
    "| 10   | 1691 |\n",
    "|  20  | 1107 |\n",
    "| 100  |  902 |\n",
    "|  30  |  827 |\n",
    "|  50  |  784 |\n",
    "|  15  |  659 | \n",
    "|  40  |  494 |\n",
    "|  12  |  460 | \n",
    "|  25  |  410 |\n",
    "\n",
    "Other frequently occurring numbers: 60, 500, 200, 11, 18, 80, 14 (241 times!). \n",
    "\n",
    "In order to examine the appearance of the numbers in context, we make a giant string out of the list of strings, `texts`: in which text a number appears is less important than its immediate context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a quick reminder of what the `texts` look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this sta\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally I would use `words = re.sub(\"[^a-zA-Z']\",\" \", mdg).lower().split()` but in this case we want to keep the non-letter numbers, so we'll keep it simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank', 'you', 'so', 'much,', 'Chris.', 'And', \"it's\", 'truly', 'a', 'great']\n"
     ]
    }
   ],
   "source": [
    "onetext = nltk.Text('\\n'.join(texts).split())\n",
    "# And here's what an NLTK text object looks like: a list of words, really\n",
    "print(onetext[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 1216 matches:\n",
      "Thank you very much. (Applause) About 10 years ago, I took on the task to teac\n",
      "tion of income of people. One dollar, 10 dollars or 100 dollars per day. There\n",
      " a long time, but they come out after 10 years very, very differently. And the\n",
      "at drives you in your life today? Not 10 years ago. Are you running the same p\n",
      "really heavy, but in the last five or 10 years, have there been some decisions\n",
      ". (Laughter) Are you sure? (Laughter) 10 seconds! (Laughter and applause) 10 s\n",
      ") 10 seconds! (Laughter and applause) 10 seconds, I want to be respectful. All\n",
      "principle in the Bible that says give 10 percent of what you get back to chari\n",
      "ional shelter that would last five to 10 years, that would be placed next to t\n",
      "tandards of five billion people? With 10 million solutions. So I wish to devel\n",
      " to go see Central Command, which was 10 minutes away. And that way, I could g\n",
      " will not launch this without five to 10 million units in the first run. And t\n",
      " down, and that's why I said seven to 10 million there. And we're doing it wit\n",
      " your laptop costs, in rough numbers, 10 dollars a diagonal inch. That can dro\n",
      "s; I can just go right up and use all 10 fingers if I wanted to. You know, lik\n",
      "ade by hand, and where, when you work 10 hours a day, you're still only earnin\n",
      "years; you have to be looking five or 10 years ahead. But I think with the vis\n",
      "ll from Earthlink that said, due to a 10 cents per megabyte overage charge, I \n",
      "umcised against her will when she was 10 years old, and she really made a deci\n",
      "and they simply point and they go for 10 percent, right in the middle. Howard \n",
      "ss in this country. And over the next 10 years, they made 600 million dollars \n",
      " you today is that, in fact, based on 10 years of research, a unique opportuni\n",
      "in the housing project on and off for 10 years, hanging out in crack houses, g\n",
      " marathons back to back. 800 miles in 10 weeks. And I was dragging all the foo\n",
      "week. He described this expedition as 10 times as dangerous as Everest. So for\n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 320 matches:\n",
      "ts live at or below the poverty line; 25 percent of us are unemployed. Low-inc\n",
      "k. Right now we're separated by about 25 feet of water, but this link will cha\n",
      "about earlier. And although less than 25 percent of South Bronx residents own \n",
      "en I started Saddleback Church, I was 25 years old. I started it with one othe\n",
      " the church had paid me over the last 25 years, and I gave it back. And I gave\n",
      "now it. But I think the man was maybe 25 years too early. So let's do it. Than\n",
      "reports in the world came from GPHIN, 25 percent of all the reports in the wor\n",
      " years, imagine them coming out every 25 seconds. So, imagine we could do that\n",
      "nd his wife gave birth to her baby at 25 weeks. And he never expected this. On\n",
      " and the other, certainly of the last 25 years, that are going to have an impa\n",
      "ccording to Howard, somewhere between 25 and 27 percent of you. Most of you li\n",
      "bout it, I would say that in the last 25 years, of every invention or innovati\n",
      "years, you expect to die with about a 25 percent likelihood. That is about as \n",
      "lling drugs for $3.50 an hour, with a 25 percent chance of dying over the next\n",
      "se. These things tend to happen every 25 years. 40 years long, with an overlap\n",
      " of a chest pain. Then, approximately 25 percent of all patients never have an\n",
      "s I've been involved in over the past 25 years. In 1981, I went to Northern Ir\n",
      "these low prices are available in the 25 countries where we work, and in a tot\n",
      "an who is now sponsoring a village of 25 Vietnamese girls for their education,\n",
      "irst state in this country to mandate 25 percent reduction of greenhouse gases\n",
      "ness. We consolidated them from 89 to 25 banks by requiring that they increase\n",
      "share capital. And it went from about 25 million dollars to 150 million dollar\n",
      " AIDS. You probably know that roughly 25 million people in Africa are infected\n",
      "not be quite right. If you think that 25 million people are infected, if you t\n",
      "e, it suggests that somewhere between 25 percent and 50 percent of the decline\n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to note here:\n",
    "\n",
    "First, there is a discrepancy in the count between `sklearn` and the NLTK: the former counted 2098 occurrences of `000`, the latter none. In all the counts that follow, there is a similar mismatch:\n",
    "\n",
    "| TERM | `sklearn` | `nltk` |\n",
    "|------|-----------|--------|\n",
    "| 000  | 2098 | \"no match\" |\n",
    "|  10  | 1691 | 1216 |\n",
    "|  20  | 1107 | 879 |\n",
    "| 100  |  902 | 647 |\n",
    "|  30  |  827 | 650 |\n",
    "|  50  |  784 | 594 | \n",
    "|  15  |  659 | 512 | \n",
    "|  40  |  494 | 387 | \n",
    "| ...               | \n",
    "|  14  |  241 | 148 | \n",
    "\n",
    "I don't have a ready explanation for this.\n",
    "\n",
    "Second, the frequency of some numbers are readily explained:\n",
    "\n",
    "* Round numbers like 10, 20, 30, 50, and 100 are approximations -- though it would be interesting to explore how often they are attached to large scalars like \"thousand\" or million.\" \n",
    "* Some numbers seem to represent alternate ways of counting: 25 reagularly stands in for \"one-quarter\" -- though not as often as we might imagine -- and 18 is regularly paired with *month* as a more precise way to say \" a year and a half.\"\n",
    "* There are some numbers, like 11 and 14 which seem to have power all their own, perhaps tied to particular ages in humans. \n",
    "\n",
    "Next up is some code to explore the most common occurring words with these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All my searches for \"collocations with specific words\" took me to the NLTK, which means, so far as I can tell, generating all the bigrams and then filtering to get the one(s) you want. This seems backwards to me: wouldn't it be faster simply to find the word and then what comes after it? I'll take a look at regex for this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bigrams\n",
    "finder = BigramCollocationFinder.from_words(onetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('14', 'years'), ('14', 'billion'), ('was', '14'), ('14', 'years,'), ('14', 'hours'), ('14', 'orders'), ('14', 'million'), ('14', 'percent'), ('14', 'feet'), ('14', 'times')]\n"
     ]
    }
   ],
   "source": [
    "## Here's the filter operation:\n",
    "the_number = lambda *w: '14' not in w\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain the number\n",
    "finder.apply_ngram_filter(the_number)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not return a count. *Oi!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank', 'you', 'so', 'much', 'chris', 'and', \"it's\", 'truly', 'a', 'great']\n"
     ]
    }
   ],
   "source": [
    "the_one = nltk.Text(re.sub(\"[^a-zA-Z0-9']\",\" \",'\\n'.join(texts)).lower().split())\n",
    "# And here's what an NLTK text object looks like: a list of words, really\n",
    "print(the_one[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 2098 matches:\n",
      "u for 99 i know one guy who's spent 4 000 just on photoshop over the years and \n",
      "er industries that bring more than 60 000 diesel truck trips to the area each w\n",
      "ed by the parks department about a 10 000 seed grant initiative to help develop\n",
      "re than 60 years we leveraged that 10 000 seed grant more than 300 times into a\n",
      "before their buildings were razed 600 000 people were displaced the common perc\n",
      "p by showing the internet users per 1 000 in this software we access about 500 \n",
      "ll tell you about emotion there are 6 000 emotions that we have words for in th\n",
      "f your dominant emotions if i have 20 000 people or 1 000 and i have them write\n",
      "emotions if i have 20 000 people or 1 000 and i have them write down all the em\n",
      "ith this i was in hawaii i was with 2 000 people from 45 countries we were tran\n",
      " vertebrate landmass that was just 10 000 years ago yesterday in biological ter\n",
      "rillion facts your mind can handle 15 000 decisions a second well it would be i\n",
      "d in the first couple of days i had 4 000 emails from people needing help so we\n",
      "ssrooms and rainwater collection is 5 000 dollars this was raised by hot chocol\n",
      " chris somehow decided to give me 100 000 so why not this many people open sour\n",
      " time in buses and trains walking 500 000 villages 120 million households and n\n",
      "t's what we did in india alone my 150 000 best friends and i went door to door \n",
      "ations history until the iraq war 150 000 people from all over the world doctor\n",
      "will give back sight to more than 300 000 people in tamil nadu india applause b\n",
      " the websites that they crawl from 20 000 to 20 million let's increase the lang\n",
      "gan to crawl the web only crawling 20 000 different websites mostly periodicals\n",
      "ese guys in ottawa on a budget of 800 000 dollars a year got cracking 75 percen\n",
      "augh was moscow cars are made from 30 000 components how ridiculous is that cou\n",
      "is thing for my studio it cost me 250 000 dollars to build this most people go \n",
      "sic her material is now used over 600 000 times per month tremendous use in fac\n"
     ]
    }
   ],
   "source": [
    "the_one.concordance(\"000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there's the missing `000`! It's in the idiomatic transcription practices of TED wherein a number like \"sixty thousand\" is rendered as \"60,000.\" \n",
    "\n",
    "One thing we know now: reporting large numbers is a part of TED talks.\n",
    "\n",
    "**TO DO**: How to keep the comma marker between numbers? (Or should we just look to 000 as a possible collocate with the other numbers?) One solution from the [Regex Cookbook][]:\n",
    "\n",
    "```python\n",
    "\\b[0-9]{1,3}(,[0-9]{3})*(\\.[0-9]+)?\\b|\\.[0-9]+\\b\n",
    "```\n",
    "\n",
    "[Regex Cookbook]: https://www.oreilly.com/library/view/regular-expressions-cookbook/9781449327453/ch06s11.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Improved Term Frequency Mapping\n",
    "\n",
    "With the numbers revealing some problems with the way the default tokenization works in `sklearn`, we need to tweak things a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
