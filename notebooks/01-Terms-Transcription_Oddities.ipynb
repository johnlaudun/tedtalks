{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED Talk Transcription Oddities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re \n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import tokenize, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From our 992 x 14 CSV, we have a list of 992 talks.\n"
     ]
    }
   ],
   "source": [
    "# Loading the data in a gendered partitioned fashion: \n",
    "df_male = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "df_female = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "df_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "\n",
    "df_all = pd.concat([df_male, df_female, df_nog])\n",
    "\n",
    "texts = df_all.text.tolist()\n",
    "\n",
    "print(f\"From our {all_talks.shape[0]} x {all_talks.shape[1]} CSV, \\\n",
    "we have a list of {len(texts)} talks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 39515)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default vectorizer = lowercase, remove punctuation, \n",
    "# tokens > 2 char, split contractions, no stopwords\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame(X.toarray(), \n",
    "                      columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "wc_all = df_all_words.sum()\n",
    "wc_all.head(10)\n",
    "wc_all.to_csv('../output/gender-part-word_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pause Fillers and Interjections: Variations on \"ah\"\n",
    "\n",
    "The words from a search for \"ah\" were: \n",
    "\n",
    "    aaaah, aah, ah, ahh, ahhh\n",
    "    \n",
    "The words from a search for \"aa\" were: \n",
    "\n",
    "    aa, aaa, aaaa, aaaaa, aaaah, aah\n",
    "    \n",
    "Code used:\n",
    "\n",
    "```python\n",
    "for match in wc_all.index:\n",
    "    if \"aa\" in match:\n",
    "        print(match)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa        11\n",
       "aaa        6\n",
       "aaaa       2\n",
       "aaaaa      1\n",
       "aah       10\n",
       "aaaah      2\n",
       "aah       10\n",
       "ah       116\n",
       "ahh        6\n",
       "ahhh       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ahs_list = [\"aa\", \"aaa\", \"aaaa\", \"aaaaa\", \"aah\", \n",
    "            \"aaaah\", \"aah\", \"ah\", \"ahh\", \"ahhh\" ] \n",
    "\n",
    "ahs = wc_all.filter(items = ahs_list, axis=0)\n",
    "ahs.head(len(ahs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  thank you so much, chris. and it's truly a great honor to have the opportunity to come to this sta\n"
     ]
    }
   ],
   "source": [
    "alltexts = ' '.join(texts).lower()\n",
    "t = tokenize.WhitespaceTokenizer()\n",
    "corpse = text.Text(t.tokenize(alltexts))\n",
    "print(alltexts[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "corpse.concordance(\"ahh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "There appears to be a tokenizer mis-match between SciKit-Learn, from which the list of <b>ah</b>s above is drawn and the NLTK tokenizer which is not finding the same tokens. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count - Sorted\n",
    "\n",
    "While we have a series with a word count, it's easy enough to sort the list to get a quick overview of the total count for the words in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the     93853\n",
       "and     67710\n",
       "to      57089\n",
       "of      52313\n",
       "that    44087\n",
       "it      35339\n",
       "in      34728\n",
       "you     34162\n",
       "we      30407\n",
       "is      28569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_sorted = wc_all.sort_values(ascending=False, inplace=False)\n",
    "wc_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performatives and Other Parenthetical Expressions <a class=\"anchor\" id=\"performatives\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier explorations of the corpus revealed something we knew but had not realized could affect our work: some TED talks are not talks but musical performances. Generally, the text of such performances are rather short. Using an arbitrary length of `500` characters, we can see what these texts look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 183, 297, 388, 602, 983, 991]\n"
     ]
    }
   ],
   "source": [
    "shorts = [ texts.index(text) for text in texts if len(text) < 500 ]\n",
    "print(shorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84:\n",
      "  (Applause)    (Music)    (Applause)  \n",
      "\n",
      "183:\n",
      "  Let's just get started here.    Okay, just a moment.    (Whirring)    All right. (Laughter) Oh, sorry.    (Music) (Beatboxing)    Thank you.    (Applause)  \n"
     ]
    }
   ],
   "source": [
    "print(f\"{shorts[0]}:\\n{texts[shorts[0]]}\\n\\n{shorts[1]}:\\n{texts[shorts[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the two examples above reveal, some talks are not talks at all and do not actually contain any text, except for the performatives, while other talks contain non-textual materials, some of which is part of the performance and some of which is the audience's response to the performance. \n",
    "\n",
    "When it comes time to process words in a text, we think the best bet is to remove the performatives. We do think, however, that having them means we can possibly explore sentiment using `(Applause)` and `(Laughter)` as contextual valuations.\n",
    "\n",
    "For now, we will need some regex to remove the parentheses and their contents from our texts. An examination of `113` above reveals that it is only three parenthetical expressions:\n",
    "\n",
    "    (Applause)    (Music)    (Applause)\n",
    "\n",
    "We need a sample text that is a mix, and so we will use `183` from above:\n",
    "\n",
    "> Let's just get started here.    Okay, just a moment.    (Whirring)    All right. (Laughter) Oh, sorry.    (Music) (Beatboxing)    Thank you.    (Applause)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "A quick check using two regexes give us one list without the parentheses and one with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whirring', 'Laughter', 'Music', 'Beatboxing', 'Applause']\n",
      "['(Whirring)', '(Laughter)', '(Music)', '(Beatboxing)', '(Applause)']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'(?<=\\().*?(?=\\))', texts[183]))\n",
    "print(re.findall(r'\\([^)]*\\)', texts[183]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `CountVectorizer` to inventory all the parenthetical expressions in the corpus to see if we are missing anything. \n",
    "\n",
    "First, we test is on a known text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>applause</th>\n",
       "      <th>beatboxing</th>\n",
       "      <th>laughter</th>\n",
       "      <th>music</th>\n",
       "      <th>whirring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     applause  beatboxing  laughter  music  whirring\n",
       "183         1           1         1      1         1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(token_pattern = r'(?<=\\().*?(?=\\))')\n",
    "X183 = vec.fit_transform(texts[183:184])\n",
    "df183 = pd.DataFrame(X183.toarray(), columns=vec.get_feature_names_out())\n",
    "df.index = [ texts.index(text) for text in texts[183:184] ]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 449)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parentheticals = vec.fit_transform(texts)\n",
    "parentheticals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"although it's nothing serious, let's keep an eye on it to make sure it doesn't turn into a major lawsuit.\"</th>\n",
       "      <th>\"close it!\"</th>\n",
       "      <th>\"i sold my soul for about a tenth of what the damn things are going for now.\"</th>\n",
       "      <th>\"in order to remain competitive in today's marketplace, i'm afraid we're going to have to replace you with a sleezeball.\"</th>\n",
       "      <th>\"intrigue and murder among 16th century ottoman court painters.\"</th>\n",
       "      <th>\"michael crichton responds by fax:\"</th>\n",
       "      <th>\"the end\" by the doors</th>\n",
       "      <th>\"what's a jurassic park?\"</th>\n",
       "      <th>\"wow! fucking fantastic jacket\"</th>\n",
       "      <th>\"yes, books. you know, the bound volumes with ink on paper. you cannot turn them off with a switch. tell your kids.\"</th>\n",
       "      <th>...</th>\n",
       "      <th>whistles</th>\n",
       "      <th>whistling</th>\n",
       "      <th>whoosh</th>\n",
       "      <th>woman screaming</th>\n",
       "      <th>woman: have you ever done a kissing test before?</th>\n",
       "      <th>woman: okay.</th>\n",
       "      <th>woo-hoo-hoo-hoo</th>\n",
       "      <th>xylophone</th>\n",
       "      <th>yelling more loudly</th>\n",
       "      <th>your fathers bristles white and stiff now</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 449 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   \"although it's nothing serious, let's keep an eye on it to make sure it doesn't turn into a major lawsuit.\"  \\\n",
       "0                                                  0                                                             \n",
       "1                                                  0                                                             \n",
       "2                                                  0                                                             \n",
       "3                                                  0                                                             \n",
       "4                                                  0                                                             \n",
       "\n",
       "   \"close it!\"  \\\n",
       "0            0   \n",
       "1            0   \n",
       "2            0   \n",
       "3            0   \n",
       "4            0   \n",
       "\n",
       "   \"i sold my soul for about a tenth of what the damn things are going for now.\"  \\\n",
       "0                                                  0                               \n",
       "1                                                  0                               \n",
       "2                                                  0                               \n",
       "3                                                  0                               \n",
       "4                                                  0                               \n",
       "\n",
       "   \"in order to remain competitive in today's marketplace, i'm afraid we're going to have to replace you with a sleezeball.\"  \\\n",
       "0                                                  0                                                                           \n",
       "1                                                  0                                                                           \n",
       "2                                                  0                                                                           \n",
       "3                                                  0                                                                           \n",
       "4                                                  0                                                                           \n",
       "\n",
       "   \"intrigue and murder among 16th century ottoman court painters.\"  \\\n",
       "0                                                  0                  \n",
       "1                                                  0                  \n",
       "2                                                  0                  \n",
       "3                                                  0                  \n",
       "4                                                  0                  \n",
       "\n",
       "   \"michael crichton responds by fax:\"  \"the end\" by the doors  \\\n",
       "0                                    0                       0   \n",
       "1                                    0                       0   \n",
       "2                                    0                       0   \n",
       "3                                    0                       0   \n",
       "4                                    0                       0   \n",
       "\n",
       "   \"what's a jurassic park?\"  \"wow! fucking fantastic jacket\"  \\\n",
       "0                          0                                0   \n",
       "1                          0                                0   \n",
       "2                          0                                0   \n",
       "3                          0                                0   \n",
       "4                          0                                0   \n",
       "\n",
       "   \"yes, books. you know, the bound volumes with ink on paper. you cannot turn them off with a switch. tell your kids.\"  \\\n",
       "0                                                  0                                                                      \n",
       "1                                                  0                                                                      \n",
       "2                                                  0                                                                      \n",
       "3                                                  0                                                                      \n",
       "4                                                  0                                                                      \n",
       "\n",
       "   ...  whistles  whistling  whoosh  woman screaming  \\\n",
       "0  ...         0          0       0                0   \n",
       "1  ...         0          0       0                0   \n",
       "2  ...         0          0       0                0   \n",
       "3  ...         0          0       0                0   \n",
       "4  ...         0          0       0                0   \n",
       "\n",
       "   woman: have you ever done a kissing test before?  woman: okay.  \\\n",
       "0                                                 0             0   \n",
       "1                                                 0             0   \n",
       "2                                                 0             0   \n",
       "3                                                 0             0   \n",
       "4                                                 0             0   \n",
       "\n",
       "   woo-hoo-hoo-hoo  xylophone  yelling more loudly  \\\n",
       "0                0          0                    0   \n",
       "1                0          0                    0   \n",
       "2                0          0                    0   \n",
       "3                0          0                    0   \n",
       "4                0          0                    0   \n",
       "\n",
       "   your fathers bristles white and stiff now  \n",
       "0                                          0  \n",
       "1                                          0  \n",
       "2                                          0  \n",
       "3                                          0  \n",
       "4                                          0  \n",
       "\n",
       "[5 rows x 449 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parens = pd.DataFrame(parentheticals.toarray(), \n",
    "                         columns=vec.get_feature_names_out())\n",
    "df_parens.index = [ texts.index(text) for text in texts ]\n",
    "df_parens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the parenthetical expressions captured, we can determine the most frequent, take a look at their numbers, and then decide what's the best path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "laughter           4625\n",
       "applause           2446\n",
       "music               297\n",
       "video               185\n",
       "audio                33\n",
       "music ends           29\n",
       "singing              29\n",
       "applause ends        21\n",
       "laughs               16\n",
       "cheers               14\n",
       "guitar strum         14\n",
       "guitar               13\n",
       "sighs                13\n",
       "drum sounds          10\n",
       "marimba sounds       10\n",
       "sings                 8\n",
       "ball squeaks          7\n",
       "clicking              7\n",
       "drum sound            6\n",
       "drum roll             6\n",
       "cheering              6\n",
       "audio: laughing       6\n",
       "piano                 6\n",
       "beep                  6\n",
       "ss's voice            5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums = df_parens.sum(axis = 0)\n",
    "sums.sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the top 20 parentheticals could be inserted into a stopword list and we would remove, in the case of the top 4 especially, words or clauses that might affect results.\n",
    "\n",
    "The code below can be included in `CountVectorizer` as a preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Refined Preprocessor --\n",
    "# This one removes two-word phrases/clauses\n",
    "\n",
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\",  \n",
    "                  \"\\(video\\)\", \"\\(laughs\\)\", \"\\(applause ends\\)\", \n",
    "                  \"\\(audio\\)\", \"\\(singing\\)\", \"\\(music ends\\)\", \n",
    "                  \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \n",
    "                  \"\\(marimba sounds\\)\", \"\\(drum sounds\\)\" ]\n",
    "\n",
    "def remove_parentheticals(text):\n",
    "    global parentheticals\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), \n",
    "                          flags=re.IGNORECASE)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laughter is the best medicine.   \n",
      "hold your applause; i'm not done yet.  \n"
     ]
    }
   ],
   "source": [
    "test = \"\"\"Laughter is the best medicine. (Laughter) \n",
    "Hold your applause; I'm not done yet. (Applause ends)\"\"\"\n",
    "\n",
    "print(remove_parentheticals(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 22389)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_vec = CountVectorizer( preprocessor = remove_parentheticals,\n",
    "                          max_df = 0.9, min_df = 2 )\n",
    "the_X = the_vec.fit_transform(texts)\n",
    "the_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers\n",
    "\n",
    "One of the dimensions of the corpus that arises out of a hand inspection of the terms is the frequency with which some numbers appear. The follow table captures the top ten numbers:\n",
    "\n",
    "| TERM | FREQUENCY |\n",
    "|------|-----------|\n",
    "| 000  | 2098 |\n",
    "| 10   | 1691 |\n",
    "|  20  | 1107 |\n",
    "| 100  |  902 |\n",
    "|  30  |  827 |\n",
    "|  50  |  784 |\n",
    "|  15  |  659 | \n",
    "|  40  |  494 |\n",
    "|  12  |  460 | \n",
    "|  25  |  410 |\n",
    "\n",
    "Other frequently occurring numbers: 60, 500, 200, 11, 18, 80, 14 (241 times!). \n",
    "\n",
    "In order to examine the appearance of the numbers in context, we make a giant string out of the list of strings, `texts`: in which text a number appears is less important than its immediate context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank', 'you', 'so', 'much,', 'Chris.', 'And', \"it's\", 'truly', 'a', 'great']\n"
     ]
    }
   ],
   "source": [
    "onetext = nltk.Text('\\n'.join(texts).split())\n",
    "# And here's what an NLTK text object looks like: a list of words, really\n",
    "print(onetext[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 1216 matches:\n",
      "Thank you very much. (Applause) About 10 years ago, I took on the task to teac\n",
      "tion of income of people. One dollar, 10 dollars or 100 dollars per day. There\n",
      " a long time, but they come out after 10 years very, very differently. And the\n",
      "at drives you in your life today? Not 10 years ago. Are you running the same p\n",
      "really heavy, but in the last five or 10 years, have there been some decisions\n",
      ". (Laughter) Are you sure? (Laughter) 10 seconds! (Laughter and applause) 10 s\n",
      ") 10 seconds! (Laughter and applause) 10 seconds, I want to be respectful. All\n",
      "principle in the Bible that says give 10 percent of what you get back to chari\n",
      "ional shelter that would last five to 10 years, that would be placed next to t\n",
      "tandards of five billion people? With 10 million solutions. So I wish to devel\n",
      " to go see Central Command, which was 10 minutes away. And that way, I could g\n",
      " will not launch this without five to 10 million units in the first run. And t\n",
      " down, and that's why I said seven to 10 million there. And we're doing it wit\n",
      " your laptop costs, in rough numbers, 10 dollars a diagonal inch. That can dro\n",
      "s; I can just go right up and use all 10 fingers if I wanted to. You know, lik\n",
      "ade by hand, and where, when you work 10 hours a day, you're still only earnin\n",
      "years; you have to be looking five or 10 years ahead. But I think with the vis\n",
      "ll from Earthlink that said, due to a 10 cents per megabyte overage charge, I \n",
      "umcised against her will when she was 10 years old, and she really made a deci\n",
      "and they simply point and they go for 10 percent, right in the middle. Howard \n",
      "ss in this country. And over the next 10 years, they made 600 million dollars \n",
      " you today is that, in fact, based on 10 years of research, a unique opportuni\n",
      "in the housing project on and off for 10 years, hanging out in crack houses, g\n",
      " marathons back to back. 800 miles in 10 weeks. And I was dragging all the foo\n",
      "week. He described this expedition as 10 times as dangerous as Everest. So for\n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 387 matches:\n",
      "w York City already handled more than 40 percent of the entire city's commerci\n",
      "ing rooms, whose evolution in 20, 30, 40 years we can't predict. So that liter\n",
      "nd all the other teams have done this 40 Days of Purpose, based on the book. A\n",
      "nternet tools, and we ended up having 40 chapters starting up, thousands of ar\n",
      "cumented the Lower Ninth for the last 40 years. That was their home, and these\n",
      "me. And a long time ago — well, about 40 years ago — my mom had an exchange st\n",
      " world where women and children spend 40 billion hours a year fetching water. \n",
      " age category of 76 to 85, as much as 40 percent of people have nothing really\n",
      "things tend to happen every 25 years. 40 years long, with an overlap. You can \n",
      " all high-rises. So they'll put 20 or 40 up at a time, and they just go up in \n",
      "te, we've seen no side effects in the 40 or so patients in whom it's been impl\n",
      " terms of price performance, that's a 40 to 50 percent deflation rate. And eco\n",
      " people may increase their volume 30, 40 percent, but they won't keep up with \n",
      "r hunters could smell animal urine at 40 paces and tell you what species left \n",
      "percent of all the pregnancies. About 40 percent of all the people — I said 40\n",
      "I said 400,000. I meant 40,000. About 40 percent of all the people who need TB\n",
      "tt-hours per kilogram nickel-cadmium, 40 watt-hours per kilogram in nickel-met\n",
      "n 60 in captivity, so we've only done 40 years in the wild so far. And we find\n",
      "ater. Imagine what the world was like 40 years ago, with just the Saturday Eve\n",
      " to India. India and China used to be 40 percent of the global economy just at\n",
      "ongo River. Canopied trees as tall as 40 meters, 130 feet, grow densely in the\n",
      "Hong Kong, with six million people in 40 square miles. During the dry season, \n",
      " farming the same piece of ground for 40 centuries. You can't farm the same pi\n",
      "n't farm the same piece of ground for 40 centuries without understanding nutri\n",
      "o measure it — but it's anywhere from 40 to 60 years. It goes on a long time. \n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"40\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to note here:\n",
    "\n",
    "First, there is a discrepancy in the count between `sklearn` and the NLTK: the former counted 2098 occurrences of `000`, the latter none. In all the counts that follow, there is a similar mismatch:\n",
    "\n",
    "| TERM | `sklearn` | `nltk` |\n",
    "|------|-----------|--------|\n",
    "| 000  | 2098 | \"no match\" |\n",
    "|  10  | 1691 | 1216 |\n",
    "|  20  | 1107 | 879 |\n",
    "| 100  |  902 | 647 |\n",
    "|  30  |  827 | 650 |\n",
    "|  50  |  784 | 594 | \n",
    "|  15  |  659 | 512 | \n",
    "|  40  |  494 | 387 | \n",
    "| ...               | \n",
    "|  14  |  241 | 148 | \n",
    "\n",
    "I don't have a ready explanation for this.\n",
    "\n",
    "Second, the frequency of some numbers are readily explained:\n",
    "\n",
    "* Round numbers like 10, 20, 30, 50, and 100 are approximations -- though it would be interesting to explore how often they are attached to large scalars like \"thousand\" or million.\" \n",
    "* Some numbers seem to represent alternate ways of counting: 25 reagularly stands in for \"one-quarter\" -- though not as often as we might imagine -- and 18 is regularly paired with *month* as a more precise way to say \" a year and a half.\"\n",
    "* There are some numbers, like 11 and 14 which seem to have power all their own, perhaps tied to particular ages in humans. \n",
    "\n",
    "Next up is some code to explore the most common occurring words with these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All my searches for \"collocations with specific words\" took me to the NLTK, which means, so far as I can tell, generating all the bigrams and then filtering to get the one(s) you want. This seems backwards to me: wouldn't it be faster simply to find the word and then what comes after it? I'll take a look at regex for this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bigrams\n",
    "finder = BigramCollocationFinder.from_words(onetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('14', 'years'), ('14', 'billion'), ('was', '14'), ('14', 'years,'), ('14', 'hours'), ('14', 'orders'), ('14', 'million'), ('14', 'percent'), ('14', 'feet'), ('14', 'times')]\n"
     ]
    }
   ],
   "source": [
    "## Here's the filter operation:\n",
    "the_number = lambda *w: '14' not in w\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain the number\n",
    "finder.apply_ngram_filter(the_number)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not return a count. *Oi!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank', 'you', 'so', 'much', 'chris', 'and', \"it's\", 'truly', 'a', 'great']\n"
     ]
    }
   ],
   "source": [
    "the_one = nltk.Text(re.sub(\"[^a-zA-Z0-9']\",\" \",'\\n'.join(texts)).lower().split())\n",
    "# And here's what an NLTK text object looks like: a list of words, really\n",
    "print(the_one[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 494 matches:\n",
      "oking for a place to eat we were on i 40 we got to exit 238 lebanon tennessee \n",
      "w york city already handled more than 40 percent of the entire city's commerci\n",
      "eading rooms whose evolution in 20 30 40 years we can't predict so that litera\n",
      "nd all the other teams have done this 40 days of purpose based on the book and\n",
      "internet tools and we ended up having 40 chapters starting up thousands of arc\n",
      "cumented the lower ninth for the last 40 years that was their home and these a\n",
      "e time and a long time ago well about 40 years ago my mom had an exchange stud\n",
      " world where women and children spend 40 billion hours a year fetching water t\n",
      "be someone coming to rescue me cut to 40 some odd years later we go to kenya a\n",
      "t age category of 76 to 85 as much as 40 percent of people have nothing really\n",
      " is how do you go to the loo at minus 40 ben i've read somewhere that at minus\n",
      "ben i've read somewhere that at minus 40 exposed skin becomes frostbitten in l\n",
      "ou answer the call of nature at minus 40 the answer of course to which is a tr\n",
      " things tend to happen every 25 years 40 years long with an overlap you can pu\n",
      "e all high rises so they'll put 20 or 40 up at a time and they just go up in t\n",
      "se my prize so i would take 30 000 or 40 000 dollars of the winnings and the r\n",
      "ate we've seen no side effects in the 40 or so patients in whom it's been impl\n",
      "n terms of price performance that's a 40 to 50 percent deflation rate and econ\n",
      "n people may increase their volume 30 40 percent but they won't keep up with i\n",
      "r hunters could smell animal urine at 40 paces and tell you what species left \n",
      " percent of all the pregnancies about 40 percent of all the people i said 400 \n",
      "all the people i said 400 000 i meant 40 000 about 40 percent of all the peopl\n",
      "e i said 400 000 i meant 40 000 about 40 percent of all the people who need tb\n",
      "att hours per kilogram nickel cadmium 40 watt hours per kilogram in nickel met\n",
      "lture what we find is that over these 40 odd years that i and others have been\n"
     ]
    }
   ],
   "source": [
    "the_one.concordance(\"40\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there's the missing `000`! It's in the idiomatic transcription practices of TED wherein a number like \"sixty thousand\" is rendered as \"60,000.\" \n",
    "\n",
    "One thing we know now: reporting large numbers is a part of TED talks.\n",
    "\n",
    "**TO DO**: How to keep the comma marker between numbers? (Or should we just look to 000 as a possible collocate with the other numbers?) One solution from the [Regex Cookbook][]:\n",
    "\n",
    "```python\n",
    "\\b[0-9]{1,3}(,[0-9]{3})*(\\.[0-9]+)?\\b|\\.[0-9]+\\b\n",
    "```\n",
    "\n",
    "[Regex Cookbook]: https://www.oreilly.com/library/view/regular-expressions-cookbook/9781449327453/ch06s11.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
