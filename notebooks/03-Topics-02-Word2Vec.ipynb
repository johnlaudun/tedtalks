{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec feels like an incantation. In this notebook we are going to try out `gensim`'s word2vec implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, Functions, Stopwords\n",
    "import pandas as pd, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "import gensim\n",
    "import operator\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Load the Data\n",
    "df = pd.read_csv('../output/TEDall.csv')\n",
    "\n",
    "# Grab the text of the talks\n",
    "talks = df.text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Word Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In his discussion of the `gensim` implementation, Radim Řehůřek notes that it expects a list of sentences as input.[[1](https://rare-technologies.com/word2vec-tutorial/)]. Because individual texts do not matter to/for `word2vec`, we are going to bundle all our sentences into a single string, then break it into sentences using the NLTK sentence tokenizer. We will preprocess the sentences to make them lowercase and to remove stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencer(sentence):\n",
    "    global stopwords\n",
    "    tokens = word_tokenize(sentence)\n",
    "    sentenced = [token for token in tokens if token not in stopwords and  len(token)>2]\n",
    "    return sentenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_talks = ' '.join(talks).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220118 ['antiquated zoning and land-use regulations are still used to this day to continue putting polluting facilities in my neighborhood.', 'are these factors taken into consideration when land-use policy is decided?', 'what costs are associated with these decisions?']\n"
     ]
    }
   ],
   "source": [
    "raw = sent_tokenize(all_talks)\n",
    "# Check our work by getting the number of sentences and three sentences\n",
    "print(len(raw), raw[500:503])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220118 [['antiquated', 'zoning', 'land-use', 'regulations', 'still', 'used', 'day', 'continue', 'putting', 'polluting', 'facilities', 'neighborhood'], ['factors', 'taken', 'consideration', 'land-use', 'policy', 'decided'], ['costs', 'associated', 'decisions']]\n"
     ]
    }
   ],
   "source": [
    "sentences = [sentencer(sentence) for sentence in raw]\n",
    "print(len(sentences), sentences[500:503])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(min_count=5, window=10, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45279594, 49749630)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('warming', 0.5628418922424316),\n",
       " ('deforestation', 0.5012110471725464),\n",
       " ('environmental', 0.4848070740699768),\n",
       " ('greenhouse', 0.4760082960128784),\n",
       " ('pollutants', 0.4671592116355896),\n",
       " ('biodiversity', 0.466927707195282),\n",
       " ('impacts', 0.46144992113113403),\n",
       " ('brink', 0.4613882303237915),\n",
       " ('disasters', 0.4569864273071289),\n",
       " ('two-degree', 0.4561028480529785)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"climate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( f\"Model has {len(w2v_model)} terms.\" )\n",
    "\n",
    "# w2v_model.save(\"../output/w2v_model.bin\")\n",
    "\n",
    "# To re-load this model, run\n",
    "# w2v_model = gensim.models.Word2Vec.load(\"w2v-model.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
