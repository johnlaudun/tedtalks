{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec feels like an incantation. In this notebook we are going to try out `gensim`'s word2vec implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, Functions, Stopwords\n",
    "import pandas as pd, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "import gensim\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\", \"\\(video\\)\", \n",
    "                  \"\\(laughs\\)\", \"\\(applause ends\\)\", \"\\(audio\\)\", \"\\(singing\\)\", \n",
    "                  \"\\(music ends\\)\", \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \"\\(marimba sounds\\)\", \n",
    "                  \"\\(drum sounds\\)\" ]\n",
    "\n",
    "def remove_parens(text):\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), flags=re.IGNORECASE)\n",
    "    return new_text\n",
    "\n",
    "# Load the Data\n",
    "df = pd.read_csv('../output/TEDall.csv')\n",
    "\n",
    "# Grab the text of the talks\n",
    "talks = df.text.tolist()\n",
    "\n",
    "# Create some labels we can use later but remove the redundant parts of the URL\n",
    "labels = [re.sub('https://www.ted.com/talks/', '',item) for item in df.public_url.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Word Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In his discussion of the `gensim` implementation, Radim Řehůřek notes that it expects a sequence of sentences as input.[[1](https://rare-technologies.com/word2vec-tutorial/)]. Since it looks like that sequence is simply a list of sentences, we are going to bundle all our sentences into a single string, then break it into sentences using the NLTK sentence tokenizer. We will preprocess the sentences to make them lowercase and to remove stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencer(sentence):\n",
    "    global stopwords\n",
    "    tokens = word_tokenize(sentence)\n",
    "    sentenced = [token for token in tokens if token not in stopwords and  len(token)>2]\n",
    "    return sentenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_talks = ' '.join(talks).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = sent_tokenize(all_talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220118 ['antiquated zoning and land-use regulations are still used to this day to continue putting polluting facilities in my neighborhood.', 'are these factors taken into consideration when land-use policy is decided?', 'what costs are associated with these decisions?']\n"
     ]
    }
   ],
   "source": [
    "print(len(raw), raw[500:503])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = [' '.join(sentencer(sentence)) for sentence in raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences), sentences[500:503])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_raw = gensim.models.Word2Vec(raw, min_count = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 76 terms.\n"
     ]
    }
   ],
   "source": [
    "print( f\"Model has {len(w2v_raw.wv.vocab)} terms.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_raw.save(\"../output/w2v_raw_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To re-load this model, run\n",
    "#w2v_model = gensim.models.Word2Vec.load(\"w2v-model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence( w2v_model, term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
    "            pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "def get_descriptor( all_terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( all_terms[term_index] )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence( w2v_model, term_rankings ) )\n",
    "    print(f\"K={k}: Coherence={coherences[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
