{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequencies for talks by gender\n",
    "\n",
    "In previous notebooks, we have used speakers' genders to apply a gender label to each talk. The gender labels used are: male, female, and not one gender. We note that the list of speakers in the corpus of TED talks identify on the gender binary, at least in the public facing documents about them (which include their TED talk bios and third party websites and publicity). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we will create document-term-frequencies for each set of gendered talks. This notebook follows closely the term-frequency notebook for the full corpus (`01-Terms.ipynb`). We will take each subset separately, annotating the full procedure for just for one of the three subsets. \n",
    "\n",
    "Similarly to `01-Terms.ipynb`, the primary task of this notebook is to count the words in the texts of each gender subset. To do so, we also must address the matter of parentheticals head on. Much of this work occurs in a separate notebook, and the explanations of how that work relates to this process is detailed in `01-Terms.ipynb`.\n",
    "\n",
    "In this notebook, we will take each subset individually (talks labeled male, female, and not one gender) and complete the following procedure: \n",
    "* Remove the parentheticals\n",
    "* Create term matrix from the resulting vocabulary\n",
    "* Adjust the threshold for a word to be included in the term-matrix\n",
    "* Export the resulting as a CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading necessary packages and functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd, re, csv, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# LOCAL FUNCTION --> Explained in 01-Terms.ipynb\n",
    "\n",
    "def remove_parentheticals(text):\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), flags=re.IGNORECASE)\n",
    "    return new_text\n",
    "\n",
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\", \"\\(video\\)\", \n",
    "                  \"\\(laughs\\)\", \"\\(applause ends\\)\", \"\\(audio\\)\", \"\\(singing\\)\", \n",
    "                  \"\\(music ends\\)\", \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \"\\(marimba sounds\\)\", \n",
    "                  \"\\(drum sounds\\)\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talks labeled Female\n",
    "\n",
    "In this section, we will expand on the four step laid out in the summary above for the talks labeled as \"female.\" This labelling was applied to talks where all the speakers identified with female pronouns. \n",
    "\n",
    "We begin by loading the female-labeled talks and taking a peak at the first 5 rows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA\n",
    "talks_female = pd.read_csv('talks_female.csv', index_col='Talk_ID') # Skipping `index_col='Talk_ID'`\n",
    "talks_female.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>public_url</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>event</th>\n",
       "      <th>duration</th>\n",
       "      <th>published</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>speaker_2</th>\n",
       "      <th>speaker_3</th>\n",
       "      <th>speaker_4</th>\n",
       "      <th>talk_gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:18:36</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>MacArthur grant,cities,green,activism,politics...</td>\n",
       "      <td>2000421</td>\n",
       "      <td>If you're here today â€” and I'm very happy th...</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>https://www.ted.com/talks/julia_sweeney_on_let...</td>\n",
       "      <td>Letting go of God</td>\n",
       "      <td>When two young Mormon missionaries knock on Ju...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:16:32</td>\n",
       "      <td>7/10/06</td>\n",
       "      <td>atheism,Christianity,religion,God,comedy,humor...</td>\n",
       "      <td>3903747</td>\n",
       "      <td>On September 10, the morning of my seventh b...</td>\n",
       "      <td>Julia Sweeney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.ted.com/talks/jehane_noujaim_inspi...</td>\n",
       "      <td>My wish: A global day of film</td>\n",
       "      <td>Jehane Noujaim unveils her 2006 TED Prize wish...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:25:38</td>\n",
       "      <td>7/25/06</td>\n",
       "      <td>TED Prize,peace,entertainment,movies,global is...</td>\n",
       "      <td>409921</td>\n",
       "      <td>I can't help but this wish: to think about w...</td>\n",
       "      <td>Jehane Noujaim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.ted.com/talks/jennifer_lin_improvs...</td>\n",
       "      <td>Improvising on piano, aged 14</td>\n",
       "      <td>Pianist and composer Jennifer Lin gives a magi...</td>\n",
       "      <td>TED2004</td>\n",
       "      <td>0:24:05</td>\n",
       "      <td>8/8/06</td>\n",
       "      <td>wunderkind,entertainment,piano,creativity,musi...</td>\n",
       "      <td>1680398</td>\n",
       "      <td>(Music)    (Music ends)    (Applause)    Tha...</td>\n",
       "      <td>Jennifer Lin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.ted.com/talks/sirena_huang_dazzles...</td>\n",
       "      <td>An 11-year-old's magical violin</td>\n",
       "      <td>Violinist Sirena Huang gives a technically bri...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:24:41</td>\n",
       "      <td>8/8/06</td>\n",
       "      <td>violin,wunderkind,entertainment,youth,music,pe...</td>\n",
       "      <td>2811646</td>\n",
       "      <td>(Music)    (Music ends)    (Applause)    (Ap...</td>\n",
       "      <td>Sirena Huang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                public_url  \\\n",
       "Talk_ID                                                      \n",
       "53       https://www.ted.com/talks/majora_carter_s_tale...   \n",
       "86       https://www.ted.com/talks/julia_sweeney_on_let...   \n",
       "55       https://www.ted.com/talks/jehane_noujaim_inspi...   \n",
       "46       https://www.ted.com/talks/jennifer_lin_improvs...   \n",
       "45       https://www.ted.com/talks/sirena_huang_dazzles...   \n",
       "\n",
       "                                headline  \\\n",
       "Talk_ID                                    \n",
       "53                   Greening the ghetto   \n",
       "86                     Letting go of God   \n",
       "55         My wish: A global day of film   \n",
       "46         Improvising on piano, aged 14   \n",
       "45       An 11-year-old's magical violin   \n",
       "\n",
       "                                               description    event duration  \\\n",
       "Talk_ID                                                                        \n",
       "53       In an emotionally charged talk, MacArthur-winn...  TED2006  0:18:36   \n",
       "86       When two young Mormon missionaries knock on Ju...  TED2006  0:16:32   \n",
       "55       Jehane Noujaim unveils her 2006 TED Prize wish...  TED2006  0:25:38   \n",
       "46       Pianist and composer Jennifer Lin gives a magi...  TED2004  0:24:05   \n",
       "45       Violinist Sirena Huang gives a technically bri...  TED2006  0:24:41   \n",
       "\n",
       "        published                                               tags    views  \\\n",
       "Talk_ID                                                                         \n",
       "53        6/27/06  MacArthur grant,cities,green,activism,politics...  2000421   \n",
       "86        7/10/06  atheism,Christianity,religion,God,comedy,humor...  3903747   \n",
       "55        7/25/06  TED Prize,peace,entertainment,movies,global is...   409921   \n",
       "46         8/8/06  wunderkind,entertainment,piano,creativity,musi...  1680398   \n",
       "45         8/8/06  violin,wunderkind,entertainment,youth,music,pe...  2811646   \n",
       "\n",
       "                                                      text       speaker_1  \\\n",
       "Talk_ID                                                                      \n",
       "53         If you're here today â€” and I'm very happy th...   Majora Carter   \n",
       "86         On September 10, the morning of my seventh b...   Julia Sweeney   \n",
       "55         I can't help but this wish: to think about w...  Jehane Noujaim   \n",
       "46         (Music)    (Music ends)    (Applause)    Tha...    Jennifer Lin   \n",
       "45         (Music)    (Music ends)    (Applause)    (Ap...    Sirena Huang   \n",
       "\n",
       "        speaker_2  speaker_3  speaker_4 talk_gender  \n",
       "Talk_ID                                              \n",
       "53            NaN        NaN        NaN      female  \n",
       "86            NaN        NaN        NaN      female  \n",
       "55            NaN        NaN        NaN      female  \n",
       "46            NaN        NaN        NaN      female  \n",
       "45            NaN        NaN        NaN      female  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talks_female.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Frequencies\n",
    "\n",
    "In this notebook, we are only concerned with the texts of each TED talk. Therefore, we begin by creating a new data object that only has the texts of the talks themselves. To uniquely identify each talk, we will also create a second data object that lists the talk IDs in the same order as the talks' texts. The talk ID is unique for each talk and will allow for results from this notebook to be tied to the metadata about each talk (which are stored in another file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_texts = talks_female.text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota bene**: If you are using this notebook to re-create the basic TF matrix you can stop here and jump to the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to get term frequencies, but **SciKit-Learn**'s `CountVectorizer` offers a one-stop shop for generating a DTM from which we can examine words as well as generate BoW-products like topic models. This is made possible by the interoperability between `CountVectorizer` and the other vectorizers and models available in `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In our first experiment, we run `CountVectorizer` unadorned. The default options are: lowercase everything, get rid of all punctuation, make a word out of anything more than two characters long. The only thing that might not be welcome is the splitting of contractions. For now, we will leave things as they are. (Also, please note, no stopwords were used, so we have an unfiltered word list and no pre-processing of the texts is done.)\n",
    "\n",
    "CHECK THIS ASSERTION: Since `CountVectorizer` only counts when *fitting* and does not engage in any kind of normalization or centering, *transforming*, there is no need to break the two functionalities out in the code that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 20990)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to pass options, pass them here:\n",
    "vectorizer = CountVectorizer( lowercase = True )\n",
    "\n",
    "# fit the model to the data \n",
    "# vecs = vectorizer.fit(texts)\n",
    "X = vectorizer.fit_transform(f_texts)\n",
    "\n",
    "# see how many features we have\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<260x20990 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 148171 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20,990 tokens is our raw, unfiltered, no pre-processing baseline. It includes, as we will see, a number of artifacts of the TED transcription process, including a variety of ways to spell out *ah*, the use of numerals for a host of things -- from dates to counts and everything in between, and some things which actually take places outside the talk itself, like audience laughter, which we here term *parentheticals*. (Accounting for all these artifacts requires a number of closer inspections, which take place in a separate notebook: [Term Artifacts](Terms-02).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names())\n",
    "raw_series = raw_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: a10, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So at least our first 340 \"words\" are numbers\n",
    "\n",
    "raw_df.iloc[1:5,345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00             3\n",
       "000          224\n",
       "000000004      1\n",
       "06             1\n",
       "07             1\n",
       "080            1\n",
       "10           173\n",
       "100           93\n",
       "1000th         1\n",
       "100th          1\n",
       "101            3\n",
       "102            1\n",
       "106            1\n",
       "10b            1\n",
       "10th           6\n",
       "11            52\n",
       "111            1\n",
       "114            1\n",
       "115            1\n",
       "118            2\n",
       "11th           6\n",
       "12            74\n",
       "120            7\n",
       "1214b          1\n",
       "124            1\n",
       "125            1\n",
       "128            1\n",
       "129            1\n",
       "12th           1\n",
       "13            26\n",
       "130            7\n",
       "1313           1\n",
       "134            1\n",
       "135            1\n",
       "137            5\n",
       "13th           1\n",
       "14            34\n",
       "140           10\n",
       "14th           1\n",
       "15            84\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_series.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hand inspection of the output above turned up a couple of interesting issues: that there are parentheticals mixed in with the text of the talks (see 01-Terms-02-Parentheticals) and numbers feature in the talks regularly (see 01-Terms-03-Numbers). \n",
    "\n",
    "There are also some interesting tokenization dimensions to a crowd-sourced transcription technology:\n",
    "\n",
    "| Term    | Freq |\n",
    "|:---------|---:|\n",
    "|aa       |12 |\n",
    "|aaa      |7  |\n",
    "|aaaa     |2  |\n",
    "|aaaaa    |1  |\n",
    "|aaaaaaaah|1  |\n",
    "|aaaaaaah |1  |\n",
    "|aaaaaah  |3  |\n",
    "|aaaah    |2  |\n",
    "|aaaahhh  |1  |\n",
    "|aaah     |3  |\n",
    "|aag      |1  |\n",
    "|aah      |10 |\n",
    "\n",
    "There are an equal number of alternate spellings for *shh*. There's also this oddity:\n",
    "\n",
    "| Term    | Freq |\n",
    "|:---------|---:|\n",
    "|FALSE    |115|\n",
    "|TRUE     |909|\n",
    "\n",
    "It appears to be simply the occurrences of *true* and *false* but somehow the vectorizer thinks they are Booleans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the hand-examination turns up no other issues, so the basic vectorization built into `sklearn` appears to be satisfactory, with the only exception being its breaking of contractions at the apostrophe: e.g., *isn't* becomes *isn* and *'t*, with the latter being thrown away as too small -- which means that the indefinite article *a(n)* is also not present in the frequencies, but the definite article *the* remains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised Frequencies without Parentheticals\n",
    "\n",
    "For more on the work leading up to this regex, see the `01-Terms-02-Parentheticals` notebook: it contains a number of experiments, which were vexed by sklearn's vectorizer only accepting strings as inputs. (While gensim appears to accept texts as lists of words, we are hoping, I think, to keep our code base as simple as possible.) The eventual workaround is to clean the texts of the top 20 parentheticals, joining the list back to a string, and then pushing the result to sklearn. This is less than optimum, but the hack works and appears to be a hack performed elsewhere by others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, the first thing we have is the list of the top 20 parentheticals, some of which are two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 20988)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noparens_vec = CountVectorizer(preprocessor = remove_parentheticals)\n",
    "noparens_X = noparens_vec.fit_transform(texts)\n",
    "noparens_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news. Our removal parentheticals has only removed two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "noparens_df = pd.DataFrame(noparens_X.toarray(), columns = noparens_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The baseline count for *laughter* is 1071; with cleaning it is 66'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = 'laughter'\n",
    "f\"The baseline count for *{term}* is {raw_df[term].sum()}; \\\n",
    "with cleaning it is {noparens_df[term].sum()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies of Terms with a 2-Document Minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a `min_df = 2` removes all words that appear in only document. (An exploration of *one document wonders* is elsewhere.) As we can see from the `shape` below, we dropped 50379 to 29340, resulting in a reduction of 42% of the possible features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 10805)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min2_vec = CountVectorizer(preprocessor = remove_parentheticals, min_df = 2)\n",
    "min2_X = min2_vec.fit_transform(texts)\n",
    "min2_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above we can sum our terms and write the results to a CSV file for closer inspection. Here, we highlight just a particular series of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batteries     2\n",
       "battery       9\n",
       "battle       13\n",
       "battles       5\n",
       "bay           9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe\n",
    "min2_df = pd.DataFrame(min2_X.toarray(), columns = min2_vec.get_feature_names())\n",
    "\n",
    "# Create a series of sums\n",
    "min2_series = min2_df.sum()\n",
    "\n",
    "# Write sums to CSV:\n",
    "# min2_series.to_csv('../output/word_freq_clean.csv')\n",
    "\n",
    "# Inspect the sums\n",
    "min2_series[1000:1005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies of Terms Appearing in All Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trimmed terms that occur in only document from our feature set, we also need to determine what words occur across all the documents and thus are not particularly effective discriminators of topic. \n",
    "\n",
    "Note that this is a somewhat more dangerous move to make, since it has been shown that one of the keys to genre and gender in documents are the varying uses of so-called function words. In fact, as our explorations establish, using sklearn's built-in algorithms reveals a focus on removing common words does not really result in much of a reduction in the overall feature set, only 8 words occurring across 99% or more documents and that number only rising to 46 when we change the minimum threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 7)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs_vec = CountVectorizer(preprocessor = remove_parentheticals, min_df = 0.99)\n",
    "alldocs_X = alldocs_vec.fit_transform(texts)\n",
    "alldocs_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and     17437\n",
       "in       8529\n",
       "of      12139\n",
       "the     21585\n",
       "this     5038\n",
       "to      14453\n",
       "you      7777\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs_df = pd.DataFrame(alldocs_X.toarray(), columns = alldocs_vec.get_feature_names())\n",
    "alldocs_series = alldocs_df.sum()\n",
    "alldocs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 41)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostdocs_vec = CountVectorizer(preprocessor = remove_parentheticals, min_df = 0.90)\n",
    "mostdocs_X = mostdocs_vec.fit_transform(texts)\n",
    "mostdocs_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostdocs_df = pd.DataFrame(mostdocs_X.toarray(), columns = mostdocs_vec.get_feature_names())\n",
    "# mostdocs_series = mostdocs_df.sum()\n",
    "# mostdocs_series.head(46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last run simply verifies that the exploration above with `min_df` holds true when we switch to `max_df`. That is, that our exploration of the parameters were not asking something unexpected by the code and possibly generating inexplicable and undesirable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 10766)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tnt = topsntails\n",
    "tnt_vec = CountVectorizer(preprocessor = remove_parentheticals, max_df = 0.9, min_df = 2)\n",
    "tnt_X = tnt_vec.fit_transform(texts)\n",
    "tnt_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers add up. With that done, my recommendation is that we do not, for the time being, throw away common and/or function words: that is, let's not use a stopword list. Those words could be important in other ways, and dropping them has only a limited impact on the actual document-term matrix, one that pales by comparison to the words that occur only in one document."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
