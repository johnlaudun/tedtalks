{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stemming TED Talks\n",
    "\n",
    "29,000 features still seems like a lot. One possible way to reduce the feature set is to stem (or lemmatize) the words involved. (For more on the difference between the two, see the note below from the Stanford NLP group[1].)\n",
    "\n",
    "But such reduction of word forms to a base form throws away information. Is there a way to maintain that information, e.g., if a word form occurred more often as a noun, verb, adjective, adverb? There is, if we run parts-of-speech tagging first, then grab the words from the PoS tuple, but the more we consider this possibility, the more it seems that any interest in a particular word might be just as well served by searching back through a corpus, using something like KWiC, to explore nuances.\n",
    "\n",
    "1. \"Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. â€¦ The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.\" [Stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, the first task in this notebook is to stem/lemmatize the corpus to see what reudction in features can be achieved."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will begin with one block that loads all the usual suspects: modules, data, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd, re, csv, nltk, string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# LOCAL FUNCTION to remove parentheticals (See Terms-O1b)\n",
    "\n",
    "def remove_parentheticals(text):\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), flags=re.IGNORECASE)\n",
    "    return new_text\n",
    "\n",
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\", \"\\(video\\)\", \n",
    "                  \"\\(laughs\\)\", \"\\(applause ends\\)\", \"\\(audio\\)\", \"\\(singing\\)\", \n",
    "                  \"\\(music ends\\)\", \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \"\\(marimba sounds\\)\", \n",
    "                  \"\\(drum sounds\\)\" ]\n",
    "\n",
    "# DATA\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../output/TEDall_speakers.csv')\n",
    "\n",
    "# Grab the texts\n",
    "texts = df.text.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the sake of comparison, here's our baseline DTM again:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# If you want to pass options, pass them here:\n",
    "vectorizer = CountVectorizer(   lowercase = True,\n",
    "                                preprocessor = remove_parentheticals, \n",
    "                                min_df = 2 )\n",
    "\n",
    "tic = time.perf_counter()\n",
    "X_0 = vectorizer.fit_transform(texts)\n",
    "toc = time.perf_counter()\n",
    "# see how many features we have\n",
    "t0 = toc - tic\n",
    "print(X_0.shape, t0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "stem_vectorizer = CountVectorizer(  analyzer=stemmed_words,\n",
    "                                    min_df = 2 )\n",
    "\n",
    "tic = time.perf_counter()\n",
    "X_1 = stem_vectorizer.fit_transform(texts)\n",
    "toc = time.perf_counter()\n",
    "t1 = toc - tic\n",
    "# see how many features we have\n",
    "print(X_1.shape, t1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1747, 18243) 42.66327483299983\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "stem_vectorizer = CountVectorizer(  analyzer=stemmed_words,\n",
    "                                    min_df = 2,\n",
    "                                    lowercase = True,\n",
    "                                    preprocessor = remove_parentheticals )\n",
    "\n",
    "tic = time.perf_counter()\n",
    "X2 = stem_vectorizer.fit_transform(texts)\n",
    "toc = time.perf_counter()\n",
    "t2 = toc - tic"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'x2' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b5afd8649550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x2' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(X2.shape, t2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1747, 18243) 42.99891504199991\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def lemmed_words(doc):\n",
    "    return (wnl.lemmatize(w) for w in analyzer(doc))\n",
    "\n",
    "lem_vectorizer = CountVectorizer(  analyzer = lemmed_words,\n",
    "                                    min_df = 2,\n",
    "                                    lowercase = True,\n",
    "                                    preprocessor = remove_parentheticals )\n",
    "\n",
    "tic = time.perf_counter()\n",
    "X3 = lem_vectorizer.fit_transform(texts)\n",
    "toc = time.perf_counter()\n",
    "t3 = toc - tic\n",
    "\n",
    "# see how many features we have\n",
    "print(X3.shape, t3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1747, 25312) 11.054988249999951\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "print (f\"\"\" \n",
    "Shapes and Times \\n\n",
    "Base:           {X_0.shape} / {t0:.2f} \\n\n",
    "Stemmed:        {X_1.shape} / {t1:.2f} \\n\n",
    "Stemmed + ():   {X2.shape} / {t2:.2f} \\n\n",
    "Lemmatized:     {X3.shape} / {t3:.2f} \\n\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " \n",
      "Shapes and Times \n",
      "\n",
      "Base:           (1747, 29340) / 3.35 \n",
      "\n",
      "Stemmed:        (1747, 18243) / 42.66 \n",
      "\n",
      "Stemmed + ():   (1747, 18243) / 43.00 \n",
      "\n",
      "Lemmatized:     (1747, 25312) / 11.05 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('dev': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}