{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequencies\n",
    "\n",
    "With the TEDtalks-all dataset created, we have 1747 talks with which to work. This is a small corpus, and so the usual reasons for shrinking the feature set for texts do not (necessarily) apply, but as we begin our survey of the contents of the TED talks we want to be mindful of standards that have emerged both so that our results are comparable to the work of others but also so that we could potentially scale up the work here without having to re-think the foundations.\n",
    "\n",
    "At the same time, with a small corpus, we can enjoy taking our time to explore the methodological issues, algorithms and their various (hyper)parameters that studies focused on larger corpora often have to take as given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we load the complete corpus of the TED-curated talks, so both the main TED events as well as TED+ events. We then use Python's `Sci-Kit Learn` library to explore the document-term frequency matrix (DTM). \n",
    "\n",
    "with an initial shape of 1747 x 50379. \n",
    "\n",
    "Summing the words to get a total for each word across all talks in the dataset, we then hand-inspect the totals and discover that there is an assortment of numbers that recur: an exploration of that phenomenon appears in a separate notebook. The primary task of this notebook is to count the words in the text, which means addressing the matter of parentheticals head on. Much of this work occurs in a separate notebook, but everything comes down to the fact that there are two kinds of parenthetical expressions in TED talks:\n",
    "\n",
    "* those which indicate actions and events outside the speaker's discourse, such as the audience laughing or applauding or the speaker sighing or playing music. \n",
    "* those which somehow the TED transcription practices have discerned as being digressive material within the speech of the author. \n",
    "\n",
    "We want to remove the former and keep the latter. In the end, the decision is made to remove, through `regex` matching, the top 20 parentheticals and to leave the others, most of which occur less than a dozen times. The affect on the overall study is thus reduced significantly.\n",
    "\n",
    "After removing the parentheticals, a second term matrix is derived with a total vocabulary of 50377. (This after discovering a slight bug, or at least weirdness, in `sklearn` that is now in their bug-tracking system.) \n",
    "\n",
    "The final step in this notebook is to raise the threshold for a word to be included in the TF-matrix to 2 documents. With that done, we convert the matrix to a pandas dataframe and then save to CSV. (QUESTION: better/more efficient to save as pickle?)\n",
    "\n",
    "**TODO**: Complete commenting throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd, re, csv, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 27)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Data\n",
    "df = pd.read_csv('../output/TEDall_speakers.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set</th>\n",
       "      <th>Talk_ID</th>\n",
       "      <th>public_url</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>event</th>\n",
       "      <th>duration</th>\n",
       "      <th>published</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>...</th>\n",
       "      <th>speaker2_introduction</th>\n",
       "      <th>speaker2_profile</th>\n",
       "      <th>speaker_3</th>\n",
       "      <th>speaker3_occupation</th>\n",
       "      <th>speaker3_introduction</th>\n",
       "      <th>speaker3_profile</th>\n",
       "      <th>speaker_4</th>\n",
       "      <th>speaker4_occupation</th>\n",
       "      <th>speaker4_introduction</th>\n",
       "      <th>speaker4_profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>only</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>alternative energy,cars,global issues,climate ...</td>\n",
       "      <td>3266733</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>only</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:21:26</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>simplicity,entertainment,interface design,soft...</td>\n",
       "      <td>1702201</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>only</td>\n",
       "      <td>53</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:18:36</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>MacArthur grant,cities,green,activism,politics...</td>\n",
       "      <td>2000421</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>only</td>\n",
       "      <td>66</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:19:24</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>children,teaching,creativity,parenting,culture...</td>\n",
       "      <td>51614087</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>only</td>\n",
       "      <td>92</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:19:50</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>demo,Asia,global issues,visualizations,global ...</td>\n",
       "      <td>12662135</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Set  Talk_ID                                         public_url  \\\n",
       "0  only        1  https://www.ted.com/talks/al_gore_on_averting_...   \n",
       "1  only        7  https://www.ted.com/talks/david_pogue_says_sim...   \n",
       "2  only       53  https://www.ted.com/talks/majora_carter_s_tale...   \n",
       "3  only       66  https://www.ted.com/talks/ken_robinson_says_sc...   \n",
       "4  only       92  https://www.ted.com/talks/hans_rosling_shows_t...   \n",
       "\n",
       "                          headline  \\\n",
       "0      Averting the climate crisis   \n",
       "1                 Simplicity sells   \n",
       "2              Greening the ghetto   \n",
       "3      Do schools kill creativity?   \n",
       "4  The best stats you've ever seen   \n",
       "\n",
       "                                         description    event duration  \\\n",
       "0  With the same humor and humanity he exuded in ...  TED2006  0:16:17   \n",
       "1  New York Times columnist David Pogue takes aim...  TED2006  0:21:26   \n",
       "2  In an emotionally charged talk, MacArthur-winn...  TED2006  0:18:36   \n",
       "3  Sir Ken Robinson makes an entertaining and pro...  TED2006  0:19:24   \n",
       "4  You've never seen data presented like this. Wi...  TED2006  0:19:50   \n",
       "\n",
       "  published                                               tags     views  ...  \\\n",
       "0   6/27/06  alternative energy,cars,global issues,climate ...   3266733  ...   \n",
       "1   6/27/06  simplicity,entertainment,interface design,soft...   1702201  ...   \n",
       "2   6/27/06  MacArthur grant,cities,green,activism,politics...   2000421  ...   \n",
       "3   6/27/06  children,teaching,creativity,parenting,culture...  51614087  ...   \n",
       "4   6/27/06  demo,Asia,global issues,visualizations,global ...  12662135  ...   \n",
       "\n",
       "  speaker2_introduction speaker2_profile speaker_3 speaker3_occupation  \\\n",
       "0                   NaN              NaN       NaN                 NaN   \n",
       "1                   NaN              NaN       NaN                 NaN   \n",
       "2                   NaN              NaN       NaN                 NaN   \n",
       "3                   NaN              NaN       NaN                 NaN   \n",
       "4                   NaN              NaN       NaN                 NaN   \n",
       "\n",
       "  speaker3_introduction speaker3_profile speaker_4 speaker4_occupation  \\\n",
       "0                   NaN              NaN       NaN                 NaN   \n",
       "1                   NaN              NaN       NaN                 NaN   \n",
       "2                   NaN              NaN       NaN                 NaN   \n",
       "3                   NaN              NaN       NaN                 NaN   \n",
       "4                   NaN              NaN       NaN                 NaN   \n",
       "\n",
       "  speaker4_introduction speaker4_profile  \n",
       "0                   NaN              NaN  \n",
       "1                   NaN              NaN  \n",
       "2                   NaN              NaN  \n",
       "3                   NaN              NaN  \n",
       "4                   NaN              NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do**: Edit the CSV to remove the vestigial index column at the start of each line. Then use `df.set_index('Talk_ID')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies\n",
    "\n",
    "The sole purpose of this notebook is to establish how we are going to elicit our features, our words, from the collection of talks. Thus, the only column we are interested in is the one with the texts of the talks. While we recognize the utility of having the data in one file, we are looking to minimize individual file sizes, and also make it possible only to load the data we need for any particular task. For that reason, we have chosen to create a number of CSV files, using the ID number that TED assigns each talk as it publishes them to its website.\n",
    "\n",
    "For this first experiment, we will keep it simple, creating two lists, one of the IDs and one of the texts. (Another possibility is to grab the URLs which are also unique as well as human-friendly identifiers for the talks.)\n",
    "\n",
    "**TODO**: We can, perhaps, make them a bit more friendly by modifying them a bit, subtracting `https://www.ted.com/talks/` from each.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs  = df.Talk_ID.tolist()\n",
    "texts = df.text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**>>> Nota bene**: If you are using this notebook to re-create the basic TF matrix you can stop here and jump to the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to get term frequencies, but **SciKit-Learn**'s `CountVectorizer` offers a one-stop shop for generating a DTM from which we can examine words as well as generate BoW-products like topic models. This is made possible by the interoperability between `CountVectorizer` and the other vectorizers and models available in `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Raw Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In our first experiment, we run `CountVectorizer` unadorned. The default options are: lowercase everything, get rid of all punctuation, make a word out of anything more than two characters long. The only thing that might not be welcome is the splitting of contractions. For now, we will leave things as they are. (Also, please note, no stopwords were used, so we have an unfiltered word list and no pre-processing of the texts is done.)\n",
    "\n",
    "CHECK THIS ASSERTION: Since `CountVectorizer` only counts when *fitting* and does not engage in any kind of normalization or centering, *transforming*, there is no need to break the two functionalities out in the code that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 50379)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to pass options, pass them here:\n",
    "vec = CountVectorizer( lowercase = True )\n",
    "\n",
    "# fit the model to the data \n",
    "X = vec.fit_transform(texts)\n",
    "\n",
    "# see how many features we have\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50,379 tokens is our raw, unfiltered, no pre-processing baseline. It includes, as we will see, a number of artifacts of the TED transcription process, including a variety of ways to spell out *ah*, the use of numerals for a host of things -- from dates to counts and everything in between, and some things which actually take places outside the talk itself, like audience laughter, which we here term *parentheticals*. Accounting for all these artifacts requires a number of closer inspections, which take place in a separate notebook: [Term Artifacts](Terms-02)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Totals per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.DataFrame(X.toarray(), columns = vecs.get_feature_names())\n",
    "\n",
    "sums_base = df_base.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hand inspection of the output above turned up a couple of interesting issues: that there are parentheticals mixed in with the text of the talks (see 01-Terms-02-Parentheticals) and numbers feature in the talks regularly (see 01-Terms-03-Numbers). \n",
    "\n",
    "There are also some interesting tokenization dimensions to a crowd-sourced transcription technology:\n",
    "\n",
    "| Term    | Freq |\n",
    "|:---------|---:|\n",
    "|aa       |12 |\n",
    "|aaa      |7  |\n",
    "|aaaa     |2  |\n",
    "|aaaaa    |1  |\n",
    "|aaaaaaaah|1  |\n",
    "|aaaaaaah |1  |\n",
    "|aaaaaah  |3  |\n",
    "|aaaah    |2  |\n",
    "|aaaahhh  |1  |\n",
    "|aaah     |3  |\n",
    "|aag      |1  |\n",
    "|aah      |10 |\n",
    "\n",
    "There are an equal number of alternate spellings for *shh*. There's also this oddity:\n",
    "\n",
    "| Term    | Freq |\n",
    "|:---------|---:|\n",
    "|FALSE    |115|\n",
    "|TRUE     |909|\n",
    "\n",
    "It appears to be simply the occurrences of *true* and *false* but somehow the vectorizer thinks they are Booleans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the hand-examination turns up no other issues, so the basic vectorization built into `sklearn` appears to be satisfactory, with the only exception being its breaking of contractions at the apostrophe: e.g., *isn't* becomes *isn* and *'t*, with the latter being thrown away as too small -- which means that the indefinite article *a* is also not present in the frequencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised Frequencies without Parentheticals\n",
    "\n",
    "For more on the work leading up to this regex, see the `01-Terms-02-Parentheticals` notebook: it contains a number of experiments, which were vexed by sklearn's vectorizer only accepting strings as inputs. (While gensim appears to accept texts as lists of words, we are hoping, I think, to keep our code base as simple as possible.) The eventual workaround is to clean the texts of the top 20 parentheticals, joining the list back to a string, and then pushing the result to sklearn. This is less than optimum, but the hack works and appears to be a hack performed elsewhere by others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, the first thing we have is the list of the top 20 parentheticals, some of which are two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\", \"\\(video\\)\", \n",
    "                  \"\\(laughs\\)\", \"\\(applause ends\\)\", \"\\(audio\\)\", \"\\(singing\\)\", \n",
    "                  \"\\(music ends\\)\", \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \"\\(marimba sounds\\)\", \n",
    "                  \"\\(drum sounds\\)\" ]\n",
    "len(parentheticals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells that follow, we create a test string that has two terms, *laughter* and *applause*, both in the speech of the imaginary speaker as well as in a parenthetical. We want to keep the former and remove the latter.\n",
    "\n",
    "Then we develop a function, `clean_text`, that goes through the list of terms to be removed as it goes through a text. This is a loop within a loop, and I am not sure if this is the most efficient way to do this, but it works. \n",
    "\n",
    "We then test the `clean_text` function on the `test` string and then on one of the talks we know is mostly a performance. The results, at long last, are what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"Laughter is the best medicine. (Laughter) \n",
    "Hold your applause; I'm not done yet. (Applause ends)\"\"\"\n",
    "\n",
    "def clean_text(rgx_list, text):\n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text, flags=re.IGNORECASE)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_text(parentheticals, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(clean_text(parentheticals, texts[235]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a working function, but the two arguments of `clean_text` cannot be used in `CountVectorizer`: we are going to use this as a `preprocessor` argument, which takes only one argument itself, the text. For the sake of simplicity, we are simply going to embed the regex list, `parentheticals`, into the function, renaming it `clean_parens`.\n",
    "\n",
    "With that done, we are going to use our new custom preprocessor in a new instantiation of the `CountVectorizer`, which runs up our term count from 50379 to 58404. \n",
    "\n",
    "Locally, the comparisons look good: a term like *laughter* drops from 7374 occurrenaces to 87. Other terms show similar drops in usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parens(text):\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), flags=re.IGNORECASE)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A survey of the global increase in terms, from 50379 to 58404, turned up some interesting restuls: first, the difference between the two sets of names was 19,891 and it looks like the `CountVectorizer` didn't lowercase a lot of things, even when the `lowercase=True` argument was passed to it. This necessitated a slight workaround, until we understand otherwise, of lowering everything in the regex function, `clean_parens`, as had occurred above.\n",
    "\n",
    "With that change in place, we vectorize things again, and we get the slightest of drops in features, from 50379 to 50377, which means we are pretty stable. Nice! (A hand inspection reveals that this still includes a lot of named entities, so we are going to raise the threshold for `min_df` from the default 1 to 2 in a moment. First let's explore what our cleaning has wrought:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 50377)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_clean = CountVectorizer(preprocessor = clean_parens)\n",
    "X_clean = vec_clean.fit_transform(texts)\n",
    "X_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame(X_clean.toarray(), columns = vec_clean.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The baseline count for *laughter* is 7374; with cleaning it is 98'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = 'laughter'\n",
    "f\"The baseline count for *{term}* is {df_base[term].sum()}; \\\n",
    "with cleaning it is {df_clean[term].sum()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Matrix with a 2-Document Minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of parentheticals and the `clean_parens` function are repeated here for ease of readability and portability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 29340)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\", \"\\(video\\)\", \n",
    "                  \"\\(laughs\\)\", \"\\(applause ends\\)\", \"\\(audio\\)\", \"\\(singing\\)\", \n",
    "                  \"\\(music ends\\)\", \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \"\\(marimba sounds\\)\", \n",
    "                  \"\\(drum sounds\\)\" ]\n",
    "\n",
    "def clean_parens(text):\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), flags=re.IGNORECASE)\n",
    "    return new_text\n",
    "\n",
    "vec_new = CountVectorizer(preprocessor = clean_parens, min_df = 2)\n",
    "X_new = vec_new.fit_transform(texts)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above where we sum our terms and write the results to a CSV file using `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "affiliations    3\n",
       "affinity        3\n",
       "affirm          7\n",
       "affirmation     4\n",
       "affirmations    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe\n",
    "df_new = pd.DataFrame(X_new.toarray(), columns = vec_new.get_feature_names())\n",
    "\n",
    "# Create a series of sums\n",
    "sums_new = df_new.sum()\n",
    "\n",
    "# Inspect the sums\n",
    "sums_new[1000:1005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write sums to CSV:\n",
    "# sums.to_csv('../output/word_freq_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Revised Dataset with Term Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe whose contents are those of the term frequency matrix for all our texts. First we give it an index using the `urls` list drawn from the base dataframe. Then we save it to a CSV, and finally we merge the base dataframe with the term matrix to produce a dataframe that has everything we need for our exploration of trends. \n",
    "\n",
    "As above, where cells write files, or possibly overwrite files, the code is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_labeled = df_new.set_axis(urls, axis=0)\n",
    "\n",
    "# df_new_labeled.to_csv('../output/talks_all_tf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public_url'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the index for the new dataframe as 'public_url'\n",
    "df_new_labeled.index.name = 'public_url'\n",
    "\n",
    "# Use this to check the name\n",
    "df_new_labeled.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can merge the dataframes on the columns we know match:\n",
    "df_merge = pd.merge(df, df_new_labeled, on = 'public_url')\n",
    "\n",
    "# Write the merged dataframe to file:\n",
    "# df_merge.to_csv('../output/talks_all_tf_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving a Custom Stopword List Based on Term Frequencies\n",
    "\n",
    "In the section that follows, we take the array holding all the talks and their terms and transpose it so that the terms are on the rows. We then sum the rows and turn the sum column into a series which is then sorted and saved to CSV for hand inspection. What the hand inspection revealed was, beyond the usual stopwords, there were enough terms in the top 300 terms that had semantic possibilities that they should be kept. The best bet moving forward is to use an available, and widely-used, stopword list. We chose the one provided by the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50379, 1747)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix = pd.DataFrame(X.todense(), columns=vec.get_feature_names())\n",
    "tfdf = term_matrix.transpose()\n",
    "tfdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1738</th>\n",
       "      <th>1739</th>\n",
       "      <th>1740</th>\n",
       "      <th>1741</th>\n",
       "      <th>1742</th>\n",
       "      <th>1743</th>\n",
       "      <th>1744</th>\n",
       "      <th>1745</th>\n",
       "      <th>1746</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000004</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000042</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00046</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000th</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 1748 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1  2  3  4  5  6  7  8  9  ...  1738  1739  1740  1741  1742  \\\n",
       "00         0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
       "000        0  1  4  0  1  4  0  0  0  2  ...     0     0     0     1     3   \n",
       "000000004  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
       "0000001    0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
       "000001     0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
       "00001      0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
       "000042     0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
       "0001       0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
       "00046      0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
       "000th      0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
       "\n",
       "           1743  1744  1745  1746   sum  \n",
       "00            0     0     0     0    15  \n",
       "000           1     0     0     1  2098  \n",
       "000000004     0     0     0     0     1  \n",
       "0000001       0     0     0     0     1  \n",
       "000001        0     0     0     0     1  \n",
       "00001         0     0     0     0     1  \n",
       "000042        0     0     0     0     1  \n",
       "0001          0     0     0     0     2  \n",
       "00046         0     0     0     0     1  \n",
       "000th         0     0     0     0     6  \n",
       "\n",
       "[10 rows x 1748 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the counts for each term into a column titled \"sum\":\n",
    "tfdf['sum'] = tfdf.sum(axis=1)\n",
    "\n",
    "# Isolate the sums in a series\n",
    "sums = pd.Series(tfdf['sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums.to_csv('../output/tf_main.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dev': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}