{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject-Verb-Objects\n",
    "\n",
    "In this notebook, we conduct a series of experiments in order: \n",
    "\n",
    "- **First**, we isolate the subject-verb-object (SVO) triples in the texts of speakers we have gendered male or female. (Using a pandas dataframe, we save the results to s CSV files for later re-use.)\n",
    "- **Second**, we compare the SVO count against the overall sentence count to determine how much of the texts have been included for analysis. See [Counts of Sentences vs SVOs](#sentences).\n",
    "- **Third**, we explore usage of male and female pronouns and nouns as subjects in both corpus: first by raw count, and then by actions (verbs) associated with those nouns and pronouns. See: [Gendered Subjects](#genderedsubjects)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">It might be useful to find a way to combine verbs via WordNet.</div>\n",
    "\n",
    "- **Fourth**, we map the objects associated with those actions. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">The same wish, to compress a variety of words under some form of hypernym.</div>\n",
    "\n",
    "then to explore the *character spaces* they establish for gendered entities within their speech as well as the nature of the *character space* they create for themselves as speakers. \n",
    "\n",
    "**Note**: We are not excluding parentheticals in this notebook.\n",
    "\n",
    "**Next Steps**: Work on code to compile / visualize this as a network graph (?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re, spacy, textacy\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From our 992x14 CSV, we have a list of 992 talks: 260 by women and 720 by men.\n"
     ]
    }
   ],
   "source": [
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "talks_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "talks_all = pd.concat([talks_m, talks_f, talks_nog])\n",
    "\n",
    "# And then grabbing on the texts of the talks:\n",
    "texts_all = talks_all.text.tolist()\n",
    "texts_women = talks_f.text.tolist()\n",
    "texts_men = talks_m.text.tolist()\n",
    "\n",
    "print(f\"From our {talks_all.shape[0]}x{talks_all.shape[1]} CSV, \\\n",
    "we have a list of {len(texts_all)} talks: {len(texts_women)} by women and \\\n",
    "{len(texts_men)} by men.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing everything upfront because we don't care whether it is *She* or *she*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase everything before we create spaCy doc and Textacy SVO triple\n",
    "texts_w = [text.lower() for text in texts_women]\n",
    "texts_m = [text.lower() for text in texts_men]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Texts to SVOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we create the SVOs that will become the basis for much of the analysis done in this notebook and the next. We first create the SVOs using spaCy's built-in functionality to do so, and then convert those SVOs to text strings that are saved in a three-column dataframe.\n",
    "\n",
    "spaCy has three different English language models: small, medium, and large. Before proceeding, we wanted to measure the timing and production of the models available to us in spaCy. Using the the two subcorpora we have, we timed the process and counted the outcomes:\n",
    "\n",
    "| Corpus | spaCy Model |  Time  | SVO Count |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| ♀︎      | small       | 1'13\"  | 26,610    |\n",
    "| ♀︎      | medium      | 1'19\"  | 26,520    |\n",
    "| ♀︎      | large       | 1'21\"  | 26,527    |\n",
    "| ♂︎      | small       | 3'34\"  | 80,460    |\n",
    "| ♂︎      | medium      | 3'56\"  | 80,433    |\n",
    "| ♂︎      | large       | 4'06\"  | 80,331    |\n",
    "\n",
    "According to [spaCy's documentation](https://spacy.io/models), the precision of the three models when it comes to parsing sentences is roughly equivalent, so we decide to use the smaller model for slightly faster results and a, again slightly, greater number of SVOs. (For greater readability of the comparison between the models, see this [post](https://johnlaudun.net/spacy-model-features/).)\n",
    "\n",
    "We used the following code in a Jupyter cell to time how long it took to load the model and then generate the list of spaCy docs:\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "docs_m = list(nlp.pipe(texts_m))\n",
    "```\n",
    "\n",
    "This uses Jupyter's now built-in timing functionality which produces the following output:\n",
    "\n",
    "```\n",
    "CPU times: user 3min 27s, sys: 33.7 s, total: 4min 1s\n",
    "Wall time: 4min 6s\n",
    "```\n",
    "\n",
    "We then simply used `len(svos_m)` to get the count, but we ran it in a separate cell so that it would not affect the time of the operation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Create the SVOs\n",
    "\n",
    "After determining which model to use, we follow spaCy's conventions for feeding it a set of texts as a list of strings. \n",
    "\n",
    "The preview simply checks that everything went as planned: it gives us a word count and the first 50 characters -- which is weird because in theory it has converted the string to a series of spacy objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(2690 tokens: \"  thank you so much, chris. and it\\'s truly a gr...\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Use the pipe method to feed documents \n",
    "docs_w = list(nlp.pipe(texts_w))\n",
    "docs_m = list(nlp.pipe(texts_m))\n",
    "\n",
    "# A quick check of our work:\n",
    "docs_m[0]._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. SVOs to Dataframe\n",
    "\n",
    "Since we create SVOs for every sentence in the two subcorpora, why not save both to two dataframes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions(doc, svo_list):\n",
    "    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))\n",
    "    for item in svotriples:\n",
    "        svo_list.append(\n",
    "            {\n",
    "                'subject': str(item[0][-1]), \n",
    "                'verb': str(item[1][-1]), \n",
    "                'object': str(item[2])\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80460 26610\n"
     ]
    }
   ],
   "source": [
    "# Create the two lists\n",
    "all_svos_m = []\n",
    "all_svos_w = []\n",
    "\n",
    "# Populate the lists with SVO triples\n",
    "for doc in docs_m:\n",
    "    actions(doc, all_svos_m)\n",
    "\n",
    "for doc in docs_w:\n",
    "    actions(doc, all_svos_w)\n",
    "\n",
    "# Convert the lists to dataframes\n",
    "svos_w = pd.DataFrame(all_svos_w)\n",
    "svos_m = pd.DataFrame(all_svos_m)\n",
    "\n",
    "print(svos_m.shape[0], svos_w.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files \n",
    "# >>> Commented out once run\n",
    "# svos_w.to_csv(\"../output/svos_w.csv\")\n",
    "# svos_m.to_csv(\"../output/svos_m.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Counts of Sentences vs SVOs <a id='sentences'></a>\n",
    "\n",
    "The code above suggests that 70% of the SVOs in TED talks have `'i', 'we', 'she', 'he', 'they', 'it', 'you'` as their subject. It's not clear, however, how much the SVO pattern represents all sentences in the talks. In this section we explore counting sentences, both through NLTK and spaCy, but also a hand count of a few sample texts to see how well our code is reflecting underlying realities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "sents_w = [ sent_tokenize(text) for text in texts_w ]    \n",
    "sents_m = [ sent_tokenize(text) for text in texts_m ]\n",
    "\n",
    "print(len(sents_w[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Female corp sent count: 30799\n",
      " Male corp sent count: 96342\n"
     ]
    }
   ],
   "source": [
    "sent_count_m = 0\n",
    "for text in texts_m:\n",
    "    sent_count_m += len(sent_tokenize(text))\n",
    "\n",
    "sent_count_w = 0\n",
    "for text in texts_w:\n",
    "    sent_count_w += len(sent_tokenize(text))\n",
    "\n",
    "print(f\" Female corp sent count: {sent_count_w}\\n Male corp sent count: {sent_count_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That results in the following percentages of SVOs out of the total number of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female subcorpora: 0.8639890905548882\n",
      "Male subcorpora: 0.8351497789126238\n"
     ]
    }
   ],
   "source": [
    "print(f\"Female subcorpora: {svos_w.shape[0] / sent_count_w}\")\n",
    "print(f\"Male subcorpora: {svos_m.shape[0] / sent_count_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "Our spaCy documents already exist, so we just need to use the `.sents` method to call the sentences and count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: 31939, M: 99992.\n"
     ]
    }
   ],
   "source": [
    "snt_cnt_w = 0\n",
    "for doc in docs_w:\n",
    "    snt_cnt_w += len(list(doc.sents))\n",
    "\n",
    "snt_cnt_m = 0\n",
    "for doc in docs_m:\n",
    "    snt_cnt_m += len(list(doc.sents))\n",
    "\n",
    "print(f\"F: {snt_cnt_w}, M: {snt_cnt_m}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: 0.8331506935095024\n",
      "M: 0.804664373149852\n"
     ]
    }
   ],
   "source": [
    "print(f\"F: {svos_w.shape[0] / snt_cnt_w}\")\n",
    "print(f\"M: {svos_m.shape[0] / snt_cnt_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total sentence counts are:\n",
    "```\n",
    "Women - NLTK : 30,799 with SVO ratio of 86%\n",
    "        spaCy: 31,673 with SVO ratio of 84%\n",
    "Men -   NLTK : 96,342 with SVO ratio of 83%\n",
    "        spaCy: 99,039 with SVO ratio of 80%\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
