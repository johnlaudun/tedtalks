{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook instantiates a small toy corpus and then explores the nature of SciKit-Learn's `CountVectorizer`. *Print calls were used to check work as I went, but are commented out once functionality is established. I have left them in case you want to check the various outputs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidated import block upfront:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Toy Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a toy corpus made up of 27 texts from a Louisiana legends collection along with the file titles in case we need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where files are located\n",
    "path = '../corpora/legends/louisiana'\n",
    "\n",
    "# Turn contents of directory into list of files \n",
    "# over which we can iterate\n",
    "files = [filename for filename in glob.glob(path + \"/*.txt\")]\n",
    "\n",
    "# Iterate over files to get their contents into a second list\n",
    "texts = []\n",
    "for item in glob.glob(path + \"/*.txt\"):\n",
    "    with open(item) as the_file:\n",
    "        text = the_file.read()\n",
    "        texts.append(text)\n",
    "# print(len(texts), texts[0][0:50])\n",
    "\n",
    "titles = [item.replace('../corpora/legends/louisiana/','').replace('.txt', '') \\\n",
    "    for item in files]\n",
    "# print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a Pandas dataframe, not because we need one right now but because if we need a toy corpus again, this reduces all of the above to one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uls-009</td>\n",
       "      <td>The thing is. Like he said, like Gator said. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uls-008</td>\n",
       "      <td>The legend goes the person -- it was like a, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lau-013</td>\n",
       "      <td>One day ... my family was kind of weird. Becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anc-090</td>\n",
       "      <td>Mom said that they used to dig a lot for money...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anc-091</td>\n",
       "      <td>I know them well. There was Jesse Venable. Tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      File                                               Text\n",
       "0  uls-009  The thing is. Like he said, like Gator said. T...\n",
       "1  uls-008  The legend goes the person -- it was like a, u...\n",
       "2  lau-013  One day ... my family was kind of weird. Becau...\n",
       "3  anc-090  Mom said that they used to dig a lot for money...\n",
       "4  anc-091  I know them well. There was Jesse Venable. Tha..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(titles, texts)), columns = ['File', 'Text'])\n",
    "df.to_csv('../corpora/legends/treasures.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 1155)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the vectorizer\n",
    "# (here is where you would normally pass in things like stopwords)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1812', '1912', 'able', 'about', 'according']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section explores `CountVectorizer`'s functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To report on hyper-parameters used (in this case the default)\n",
    "vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first read the documentation for `get_stop_words()` -- \"Build or fetch the effective stop words list\" -- as perhaps offering a suggested list. Alas, I think it simply recalls the stopword list used. *Wah wah wahhh*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "stopwords = vectorizer.get_stop_words()\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inverse_transform()` returns the texts back but only as words in the BoW. Interesting functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['the', 'thing', 'is', 'like', 'he', 'said', 'gator', 'back',\n",
      "       'then', 'man', 'asked', 'worker', 'must', 've', 'been', 'if',\n",
      "       'would', 'watch', 'his', 'money', 'and', 'one', 'of', 'workers',\n",
      "       'no', 'this', 'other', 'wanted', 'to', 'yea', 'so', 'what', 'they',\n",
      "       'did', 'kill', 'him', 'bury', 'underground', 'now', 'you',\n",
      "       'watching', 'it', 'that', 'say', 'was', 'buried', 'gonna', 'yep'],\n",
      "      dtype='<U14')]\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.inverse_transform(X)\n",
    "print(Y[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Conclusion**: There is no hyper-parameter that allows an analyst to tell the algorithm to ignore/discard words that occur below a certain threshold within a text, only across texts. (This feels like a \"that I've yet found\" so I will continue to explore this.) This seems like foundational functionality, but perhaps it can be covered by functions we create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section I want to explore the possibility of working with the sparse matrix as either a numpy array but probably as a pandas dataframe in order to determine if we can identify words that occur across multiple texts but only singly. Texts as small as this are going to have a fair amount of singles and removing the singles will, in fact, affect the semantic dimensions of these texts, but this is simply proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dev': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}