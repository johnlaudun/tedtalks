{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Into the Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're just here for the term-year matrix, skip past the build section below and go straight to the [Analysis](#Analysis) section, where you can load the results of the build section from a CSV of ther results. \n",
    "\n",
    "See: [Hedonometer](https://hedonometer.org/timeseries/en_all/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first section we (1) filter out all but the TED main talks, (2) group those talks by year, and then (3) count terms for each year. The process is in the following subsections:\n",
    "\n",
    "1. In [Imports and Data](#Imports-and-data) we load the data, filter for the main TED talks, and then by way of inspection, count the number of talks available for each year. As it turns out, the first few years do not have many talks -- the first three years have only one talk each -- and so we drop those years subsequently.\n",
    "2. In [Create \"Texts\" for Each Year](#Create-texts-for-each-year), we deploy **pandas**' `groupby` method to create a series with each year as the index and all the texts for that year as the value.\n",
    "3. In [Clean the Texts](#Clean-the-texts), we attempt to remove a number of elements that do not belong in the texts proper but this currently is not complete. \n",
    "4. We then do [A Quick Word Count for Each Year](#A-quick-word-count-for-each-year) just to check what our numbers are looking like.\n",
    "5. Finally, we [Vectorize the Texts](#Vectorize-the-texts), creating a dataframe which has the years for rows and the terms for columns which is then transposed so that the words are the rows and the years columns, making it easier, we hope, to \"see\" trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd, re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, we use `.shape` and `list()` to make sure the dataset loaded as expected and then to remind us of our column headers -- we are looking for the column that distinguishes between the main TED events and the various extra events. We are going to focus on the main events for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'only', 'plus'}\n"
     ]
    }
   ],
   "source": [
    "# Load the Data\n",
    "dfAll = pd.read_csv('../output/TEDall.csv')\n",
    "\n",
    "# Remind ourselves what the terms are to distinguish\n",
    "# between TED main talks and all the other talks\n",
    "print(set(dfAll.Set.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1984     1\n",
       "1990     1\n",
       "1994     1\n",
       "1998     6\n",
       "2001     3\n",
       "2002    28\n",
       "2003    34\n",
       "2004    31\n",
       "2005    36\n",
       "2006    43\n",
       "2007    68\n",
       "2008    56\n",
       "2009    81\n",
       "2010    68\n",
       "2011    70\n",
       "2012    65\n",
       "2013    76\n",
       "2014    84\n",
       "2015    75\n",
       "2016    75\n",
       "2017    90\n",
       "Name: presented, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the dataframe to just the TED main talks:\n",
    "main = dfAll[dfAll['Set']=='only']\n",
    "# main.shape\n",
    "main['presented'].value_counts().sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create \"Texts\" for Each Year\n",
    "\n",
    "It looks like there's not much point in including the first five years on record: they total to 11, which is only half as many as the total of 28 for 2002. \n",
    "\n",
    "The easiest way to proceed is:\n",
    "\n",
    "1. Concatenate all the texts of the talks into one big pseudo-document for each year\n",
    "2. Drop the first five years\n",
    "\n",
    "We start there with concatenating all the texts of the talks into a pandas series with the years as index. In pandas you can [concatenate strings][] based on some other criteria: here we are *grouping by* the year a talk was given, which is `presented` in our dataset. (We use the `all_years` variable initially so that we can call the edited series simply `years`.)\n",
    "\n",
    "[concatenate strings]: https://stackoverflow.com/questions/27298178/concatenate-strings-from-several-rows-using-pandas-groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the texts of the talks into one big pseudo-document for each year\n",
    "all_years = main.groupby(['presented'])['text'].apply(lambda x: ','.join(x))\n",
    "\n",
    "# Drop the first five years\n",
    "years = all_years.drop([1984, 1990, 1994, 1998, 2001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002:   What I want to talk about is, as background, is the idea t\n",
      "2003:   You know, one of the intense pleasures of travel and one o\n",
      "2004:   (Music)    (Music ends)    (Applause)    Thank you!    (Ap\n",
      "2005:   My name is Lovegrove. I only know nine Lovegroves, two of \n",
      "2006:   Thank you so much, Chris. And it's truly a great honor to \n",
      "2007:   I have all my life wondered what \"mind-boggling\" meant. Af\n",
      "2008:   Roy Gould: Less than a year from now, the world is going t\n",
      "2009:   I wrote a letter last week talking about the work of the f\n",
      "2010:   Sadly, in the next 18 minutes when I do our chat, four Ame\n",
      "2011:   Ten years ago exactly, I was in Afghanistan. I was coverin\n",
      "2012:   Let me begin with four words that will provide the context\n",
      "2013:   What is going to be the future of learning?    I do have a\n",
      "2014:   Chris Anderson: The rights of citizens, the future of the \n",
      "2015:   We are built out of very small stuff, and we are embedded \n",
      "2016:   So a while ago, I tried an experiment. For one year, I wou\n",
      "2017:   Gayle King: Have a seat, Serena Williams, or should we say\n"
     ]
    }
   ],
   "source": [
    "for index, value in years.iteritems():\n",
    "    print(f'{index}: {value[0:60]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test case for later work, and without being terribly important for the current experiment, we are going to clean our texts using two functions: one to remove speakers and one to remove parentheticals.\n",
    "\n",
    "In the cell below we create our two lists, speakers and parentheticals, and then create two separate functions and then a function to combine them. \n",
    "\n",
    "Our first step is to create the two lists of strings we want removed:\n",
    "\n",
    "* `parentheticals` is from previous experiments\n",
    "* `speakers` is probably unpythonic in its expression but it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\", \"\\(video\\)\", \n",
    "                  \"\\(laughs\\)\", \"\\(applause ends\\)\", \"\\(audio\\)\", \"\\(singing\\)\", \n",
    "                  \"\\(music ends\\)\", \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \"\\(marimba sounds\\)\", \n",
    "                  \"\\(drum sounds\\)\" ]\n",
    "len(parentheticals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = dfAll.speaker_1.tolist() + dfAll.speaker_2.tolist() + dfAll.speaker_3.tolist() + dfAll.speaker_4.tolist()\n",
    "print(speakers[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_parens(text):\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), flags=re.IGNORECASE)\n",
    "    return new_text\n",
    "\n",
    "def remove_speaker_names(text):\n",
    "    temp_text = text\n",
    "    for rgx_match in speakers:\n",
    "        temp_text = re.sub(rgx_match, ' ', temp_text)\n",
    "    return temp_text\n",
    "\n",
    "def clean_text(text):\n",
    "    the_text = text\n",
    "    cleaned = remove_parens(remove_speaker_names(the_text))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`remove_speaker_names` keeps throwing a `TypeError`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Word Count for Each Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, let's just get a quick word count for each of our years. In this section of cells, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our series to a dataframe to make it easier to work in place:\n",
    "dfYears = years.to_frame()\n",
    "\n",
    "# Lowercase our texts\n",
    "dfYears = dfYears.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "# Remove everything that isn't a word, or space\n",
    "dfYears = dfYears.replace('[^\\w\\s\\+]', '', regex = True)\n",
    "\n",
    "# Split on spaces and then count the length of the resulting list\n",
    "dfYears['word_count'] = dfYears.text.apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "# See the results\n",
    "dfYears.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the Pseudo-Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we instantiate our term frequency vectorizer and turn it loose on our pseudo-documents:\n",
    "\n",
    "1. In creating a list from the `years` series we are returning to the texts with punctuation, which we need for our `remove_parens` function to do its job. The difference almost a thousand words in the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countvectorizer expects a list, so we create a list\n",
    "texts = [ value for index, value in years.iteritems() ]\n",
    "\n",
    "# We are going to bring our years back to the resulting term matrix below, \n",
    "# so while we are creating lists from our series, lets grab those years\n",
    "# (And yes you can create two lists from one list comprehension, but don't.)\n",
    "year_labels = [ index for index, value in years.iteritems() ]\n",
    "\n",
    "# This just checks our results\n",
    "print(len(texts), texts[0][0:50], year_labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual incantation (minus the desired speaker removal for now):\n",
    "vec = CountVectorizer(preprocessor = remove_parens, min_df = 1, max_df = 1.0)\n",
    "word_count_vector=vec.fit_transform(texts)\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on how changes to the vectorizer parameters affect the overall word count:\n",
    "```\n",
    "min_df = 0, max_df = n/a   ==> 39118\n",
    "min_df = 1, max_df = 1.0   ==> 39118\n",
    "min_df = 2, max_df = n/a   ==> 21723\n",
    "min_df = 2, max_df = 1.00  ==> 21723 (This makes sense, but I was just checking.)\n",
    "min_df = 2, max_df = 0.99  ==> 19844 (\"global\" and \"climate\" seem to disappear?!)\n",
    "min_df = 2, max_df = 0.95  ==> 19844\n",
    "min_df = 2, max_df = 0.90  ==> 19158\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the resulting array\n",
    "X = vec.fit_transform(texts)\n",
    "term_matrix = pd.DataFrame(X.todense(), columns=vec.get_feature_names())\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix['year'] = year_labels\n",
    "term_matrix.set_index('year', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "term_df = term_matrix.transpose()\n",
    "term_df.reset_index(inplace=True)\n",
    "term_df = word_df.rename(columns={'index': 'term'})\n",
    "term_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save this dataframe \n",
    "# ==> Commented out so re-running notebook doesn't result in new file\n",
    "# word_df.to_csv('../output/YTM_min1-max100.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
