{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-Data\" data-toc-modified-id=\"Loading-the-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading the Data</a></span></li><li><span><a href=\"#Term-Frequencies\" data-toc-modified-id=\"Term-Frequencies-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Term Frequencies</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TF-IDF</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words\n",
    "\n",
    "Like cats, there is more than one way to approach getting bags of words out of a collection of texts. And that's all we are doing here. We are not worrying about capturing any context more fine-grained than the text itself. So, no syntax, no sentences, no paragraphs. We may look at parts of speech, but only as a dimension of a word itself. \n",
    "\n",
    "That noted, there are two questions any effort at tokenization must answer: (1) **tokens** or **lemmas**? and (2) **stopwords**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Working with only the data from the release, we have two files:\n",
    "\n",
    "    TEDonly_speakers_final.csv\n",
    "    TEDplus_speakers_final.csv\n",
    "\n",
    "The first thing we will do is to load both as `pandas` dataframes, but I think we also want to consider creating a merged file -- something like `TEDall.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_only =  (992, 27) \n",
      "df_plus =  (755, 27)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_only = pd.read_csv('../data/Release_v0/TEDonly_speakers_final.csv')\n",
    "df_plus = pd.read_csv('../data/Release_v0/TEDplus_speakers_final.csv')\n",
    "\n",
    "print(\"df_only = \", df_only.shape, \"\\n\" + \"df_plus = \", df_plus.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of columns, so we want to double check the column names to get the texts of the talks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Talk_ID', 'public_url', 'headline', 'description', 'event', 'duration', 'published', 'tags', 'views', 'text', 'speaker_1', 'speaker1_occupation', 'speaker1_introduction', 'speaker1_profile', 'speaker_2', 'speaker2_occupation', 'speaker2_introduction', 'speaker2_profile', 'speaker_3', 'speaker3_occupation', 'speaker3_introduction', 'speaker3_profile', 'speaker_4', 'speaker4_occupation', 'speaker4_introduction', 'speaker4_profile']\n"
     ]
    }
   ],
   "source": [
    "with open('../data/Release_v0/TEDonly_speakers_final.csv') as f:\n",
    "    colnames = f.readline().strip().split(\",\")\n",
    "    \n",
    "print(colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this particular experiment, I am going to convert the column into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992 755\n"
     ]
    }
   ],
   "source": [
    "only = df_only.text.tolist()\n",
    "plus = df_plus.text.tolist()\n",
    "print(len(only), len(plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_talks = only + plus\n",
    "print(len(all_talks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, as we work through our basic methods, let's work with the `only` texts. The first thing to check is how many talks are empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in only:\n",
    "    if len(text) < 10:\n",
    "        print(only.index(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how many short ones there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "235\n",
      "382\n",
      "496\n",
      "573\n",
      "799\n",
      "899\n"
     ]
    }
   ],
   "source": [
    "for text in only:\n",
    "    if len(text) < 300:\n",
    "        print(only.index(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, just to make sure that the list does not include the column header, we will print a bit of the first item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Thank you so much, Chris. And it's truly a great\n"
     ]
    }
   ],
   "source": [
    "print(only[0][0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so no empty texts, but some short ones, and our first text is not the column name, but our first talk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is to reduce our texts to bags of words. There are a number of ways to do this, but **SciKit-Learn**'s `CountVectorizer` is, I think, the way to go, since it will work well with the Kit's TFIDF vectorizer, and both of those are foundations for either LDA or NMF topic modeling as well as being useful in and of themselves. We can also store the counts as either their own CSV or as an ammendment to our current CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run it unadorned, we get a word count of almost 40,000 from our not quite a thousand texts. For now, let's let that be: the default options are to lowercase everything, get rid of all punctuation, etc. The only thing that might not be welcome is that splitting of contractions. \n",
    "\n",
    "For this current work, we are running `fit()` and `transform()` separately, but since `fit()` just calculates the parameters and saves them as an internal objects state `transform()`  applies the transformation to a particular set of examples (the ones we just fitted), the two operations are usually simply done at the same time as `fit_transform()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to pass options, pass them here:\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the model to the data \n",
    "vecs = vectorizer.fit(only)\n",
    "\n",
    "# transform the data according to the fitted model\n",
    "bow = vecs.transform(only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 39515)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a complete matrix in place, we can total up our columns for each feature (word). We can then take that vector, with each position describing one word and convert it into a tuple, which we can then sort by frequency. (I'm doing it this way because it appears to be the way to do it, but it also strikes me that there should be a way to do this within the array itself, or, perhaps, to do it through **pandas**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 93853), ('and', 67710), ('to', 57089), ('of', 52313), ('that', 44087), ('it', 35339), ('in', 34728), ('you', 34162), ('we', 30407), ('is', 28569), ('this', 22640), ('so', 16733), ('was', 14124), ('they', 13963), ('for', 13606), ('but', 12527), ('have', 12298), ('are', 12245), ('on', 12118), ('what', 12019)]\n"
     ]
    }
   ],
   "source": [
    "# summing up the counts for each word\n",
    "sum_words = bow.sum(axis=0)\n",
    "\n",
    "# create a tuple\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vecs.vocabulary_.items()]\n",
    "\n",
    "# sort the tuple\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# check the results of our work by printing the top 20 more frequent words\n",
    "print(words_freq[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save these results to a CSV file and check to see that the words we see are the kinds of words we want. We don't want any weirdness affecting our overall results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('word_freq.csv','w') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['name','num'])\n",
    "    for row in words_freq:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hand inspection looks good. I didn't see anything in words 4 or above in frequency that looked off. (So, the simplest solution works!) \n",
    "\n",
    "What I did note was the frequency of certain **numbers**: **100**, **12**, etc. This might be worth taking a closer look: are there *power numbers*? (I am thinking here of Alan Dundes' essay on the \"power\" of three in American culture.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can do some one-off inspections: I would like also to develop code where we could grab all words of a certain frequency, or range of frequencies. (In part, this is a note to myself to understand better the nature of the `vec` and the `bow` so as to be able to interact with them more effectively.)\n",
    "\n",
    "**Follow-up**: whenever I attempt some version of\n",
    "```python\n",
    "for item in vecs:\n",
    "    if vecs.vocabulary_.get(item) == 1691:\n",
    "        print(item)\n",
    "```\n",
    "I get **`TypeError: 'CountVectorizer' object is not iterable`**. My best guess, for now, is that we need to use the tuple above to get this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1691"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.vocabulary_.get('algorithm') # Wait! This doesn't get the count but the index?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algeria', 'algorithm', 'algorithmic', 'algorithmically', 'algorithms']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.get_feature_names()[1690:1695]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of features begins with numbers, and there are a lot of them. (We don't, as yet, know how often or where they occur.) The range below isolates where the list transitions from numbers to words, in case anyone was wondering. The different spellings of \"aaa\" is interesting, but I am most curious about \"aakash.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['94', '95', '950', '952', '956', '96', '962', '97', '974', '98', '982', '984', '987', '99', '999', '999999', '9mm', '9th', '______', '________', 'a10', 'a12', 'a380', 'a4', 'a92', 'aa', 'aaa', 'aaaa', 'aaaaa', 'aaaah', 'aag', 'aah', 'aakash', 'aargh', 'aaron', 'aaronson', 'aarp', 'aat', 'aatcagggaccc', 'ab']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vecs.get_feature_names()\n",
    "print(feature_names[760:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the solution to this problem appears to be to return to the tuple and, perhaps, convert it to a dictionary. Or at least that's one solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_dict = dict(words_freq)\n",
    "wf_dict['algorithm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length, admit, automatically, imagined, amounts, presentation, miss, axis, shit, recording, grows, require, depends, co, suit, witness, rid, parent, actions, relate, furniture, wanting, teeth, practical, terrorism, classes, battle, chicken, algorithm, "
     ]
    }
   ],
   "source": [
    "for key in wf_dict:\n",
    "    if wf_dict[key] == 63:\n",
    "        print(key, end = \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, interesting weirdness: \"co\" has apparently been broken off from the nouns and verbs it might modify. Oops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_bow_only.pkl']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output a pickle file for the model\n",
    "joblib.dump(bow, 'saved_bow_only.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_vecs_only.pkl']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vecs, 'saved_vecs_only.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use a TF-IDF matrix to glean how words achieve a certain level of significance? (Is this what NMF does?)\n",
    "\n",
    "\n",
    "**Useful Links**\n",
    "\n",
    "* [Extracting Important Keywords from Text with TF-IDF](https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb) explains how to train and then test a document to discover its keywords. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf = tfidf_transformer.fit(bow)\n",
    "# tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
