{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-Data\" data-toc-modified-id=\"Loading-the-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading the Data</a></span></li><li><span><a href=\"#Term-Frequencies\" data-toc-modified-id=\"Term-Frequencies-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Term Frequencies</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TF-IDF</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words\n",
    "\n",
    "Like cats, there is more than one way to approach getting bags of words out of a collection of texts. And that's all we are doing here. We are not worrying about capturing any context more fine-grained than the text itself. So, no syntax, no sentences, no paragraphs. We may look at parts of speech, but only as a dimension of a word itself. \n",
    "\n",
    "That noted, there are two questions any effort at tokenization must answer: (1) **tokens** or **lemmas**? and (2) **stopwords**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sections in this Notebook**\n",
    "1. [Loading the Data](#Loading-the-Data)\n",
    "2. [Term Frequencies](#Term-Frequencies)\n",
    "3. [TF-IDF](#TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Working with only the data from the release, we have two data sets, `TEDonly` and `TEDplus` that we have previously merged into `TEDall_speakers` with an additional column indicating from which data set a given talk is taken. The first thing we will do is to load `TEDall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 27)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../output/TEDall_speakers.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set</th>\n",
       "      <th>Talk_ID</th>\n",
       "      <th>public_url</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>event</th>\n",
       "      <th>duration</th>\n",
       "      <th>published</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>...</th>\n",
       "      <th>speaker2_introduction</th>\n",
       "      <th>speaker2_profile</th>\n",
       "      <th>speaker_3</th>\n",
       "      <th>speaker3_occupation</th>\n",
       "      <th>speaker3_introduction</th>\n",
       "      <th>speaker3_profile</th>\n",
       "      <th>speaker_4</th>\n",
       "      <th>speaker4_occupation</th>\n",
       "      <th>speaker4_introduction</th>\n",
       "      <th>speaker4_profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>only</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>alternative energy,cars,global issues,climate ...</td>\n",
       "      <td>3266733</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>only</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:21:26</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>simplicity,entertainment,interface design,soft...</td>\n",
       "      <td>1702201</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>only</td>\n",
       "      <td>53</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>0:18:36</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>MacArthur grant,cities,green,activism,politics...</td>\n",
       "      <td>2000421</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Set  Talk_ID                                         public_url  \\\n",
       "0  only        1  https://www.ted.com/talks/al_gore_on_averting_...   \n",
       "1  only        7  https://www.ted.com/talks/david_pogue_says_sim...   \n",
       "2  only       53  https://www.ted.com/talks/majora_carter_s_tale...   \n",
       "\n",
       "                      headline  \\\n",
       "0  Averting the climate crisis   \n",
       "1             Simplicity sells   \n",
       "2          Greening the ghetto   \n",
       "\n",
       "                                         description    event duration  \\\n",
       "0  With the same humor and humanity he exuded in ...  TED2006  0:16:17   \n",
       "1  New York Times columnist David Pogue takes aim...  TED2006  0:21:26   \n",
       "2  In an emotionally charged talk, MacArthur-winn...  TED2006  0:18:36   \n",
       "\n",
       "  published                                               tags    views  ...  \\\n",
       "0   6/27/06  alternative energy,cars,global issues,climate ...  3266733  ...   \n",
       "1   6/27/06  simplicity,entertainment,interface design,soft...  1702201  ...   \n",
       "2   6/27/06  MacArthur grant,cities,green,activism,politics...  2000421  ...   \n",
       "\n",
       "  speaker2_introduction speaker2_profile speaker_3 speaker3_occupation  \\\n",
       "0                   NaN              NaN       NaN                 NaN   \n",
       "1                   NaN              NaN       NaN                 NaN   \n",
       "2                   NaN              NaN       NaN                 NaN   \n",
       "\n",
       "  speaker3_introduction speaker3_profile speaker_4 speaker4_occupation  \\\n",
       "0                   NaN              NaN       NaN                 NaN   \n",
       "1                   NaN              NaN       NaN                 NaN   \n",
       "2                   NaN              NaN       NaN                 NaN   \n",
       "\n",
       "  speaker4_introduction speaker4_profile  \n",
       "0                   NaN              NaN  \n",
       "1                   NaN              NaN  \n",
       "2                   NaN              NaN  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with just the talks for the time being, we will double-check the label of the talk column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Set', 'Talk_ID', 'public_url', 'headline', 'description', 'event', 'duration', 'published', 'tags', 'views', 'text', 'speaker_1', 'speaker1_occupation', 'speaker1_introduction', 'speaker1_profile', 'speaker_2', 'speaker2_occupation', 'speaker2_introduction', 'speaker2_profile', 'speaker_3', 'speaker3_occupation', 'speaker3_introduction', 'speaker3_profile', 'speaker_4', 'speaker4_occupation', 'speaker4_introduction', 'speaker4_profile']\n"
     ]
    }
   ],
   "source": [
    "with open('../output/TEDall_speakers.csv') as f:\n",
    "    cols_only = f.readline().strip().split(\",\")\n",
    "\n",
    "print(cols_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this particular experiment, I am going to convert the column into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1747"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talks = df.text.tolist()\n",
    "len(talks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to check is how many talks are, essentially, empty. We are going to set an arbitrary string length of 10 or less to see what gets returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for talk in talks:\n",
    "    if len(talk) < 10:\n",
    "        print(talks.index(talk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, no empty talks. Now let's see how many short ones there are. For reference, the length of the first 10 talks in our dataset is in the five figures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12074\n",
      "18785\n",
      "18477\n",
      "17801\n",
      "17561\n",
      "23666\n",
      "18794\n",
      "16063\n",
      "18097\n",
      "20267\n"
     ]
    }
   ],
   "source": [
    "for talk in talks[0:10]:\n",
    "    print(len(talk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "235\n",
      "382\n",
      "496\n",
      "573\n",
      "799\n",
      "899\n",
      "1484\n",
      "1564\n"
     ]
    }
   ],
   "source": [
    "for talk in talks:\n",
    "    if len(talk) < 500:\n",
    "        print(talks.index(talk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect one of the short talks: it looks like it's a musical performance and all we have in the transcript are three parantheticals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Applause)    (Music)    (Applause)  \n"
     ]
    }
   ],
   "source": [
    "print(talks[113])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, just to make sure that the list does not include the column header, we will print a bit of the first item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Thank you so much, Chris. And it's truly a great\n"
     ]
    }
   ],
   "source": [
    "print(talks[0][0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so no empty texts, but some short ones, and our first text is not the column name, but our first talk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is to reduce our texts to bags of words. There are a number of ways to do this, but **SciKit-Learn**'s `CountVectorizer` is, I think, the way to go, since it will work well with the other vectorizers and models also available in `sklearn`. We can store the counts as either their own CSV or as an ammendment to our current CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run it unadorned, we get a word count of a little over 50,000. In the `TEDonly` talks, there were not quite 40,000 words. So `TEDplus` has added 10,000 words. I'm not sure of the significance, or the lack thereof, of that number. We have used the default options of the `CountVectorizer`: lowercase everything, get rid of all punctuation, make a word our of anything more than two characters long. The only thing that might not be welcome is the splitting of contractions. For now, we will leave things as they are. (Also, please note, no stopwords were used, so we have an unfiltered word list.)\n",
    "\n",
    "For this current work, we are running `fit()` and `transform()` separately, but since `fit()` just calculates the parameters and saves them as an internal objects state `transform()`  applies the transformation to a particular set of examples (the ones we just fitted), the two operations are usually simply done at the same time as `fit_transform()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 50379)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to pass options, pass them here:\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the model to the data \n",
    "vecs = vectorizer.fit(talks)\n",
    "\n",
    "# transform the data according to the fitted model\n",
    "bow = vecs.transform(talks)\n",
    "\n",
    "# see how many features we have\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a complete matrix in place, we can total up our columns for each feature (word). We can then take that vector, with each position describing one word and convert it into a tuple, which we can then sort by frequency. (I'm doing it this way because it appears to be the way to do it, but it also strikes me that there should be a way to do this within the array itself, or, perhaps, to do it through **pandas**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 166093), ('and', 118989), ('to', 102276), ('of', 92416), ('that', 76268), ('in', 62673), ('it', 59191), ('you', 56296), ('we', 54458), ('is', 50072), ('this', 38510), ('so', 29001), ('they', 25157), ('was', 24582), ('for', 24445), ('are', 22592), ('have', 21965), ('but', 21804), ('on', 20978), ('what', 20907)]\n"
     ]
    }
   ],
   "source": [
    "# summing up the counts for each word\n",
    "sum_words = bow.sum(axis=0)\n",
    "\n",
    "# create a tuple\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vecs.vocabulary_.items()]\n",
    "\n",
    "# sort the tuple\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# check the results of our work by printing the top 20 more frequent words\n",
    "print(words_freq[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save these results to a CSV file and check to see that the words we see are the kinds of words we want. We don't want any weirdness affecting our overall results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../output/word_freq.csv','w') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['word','count'])\n",
    "    for row in words_freq:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hand inspection looks good. I didn't see anything in words 4 or above in frequency that looked off. (So, the simplest solution works!) \n",
    "\n",
    "What I did note was the frequency of certain **numbers**: **100**, **12**, etc. This might be worth taking a closer look: are there *power numbers*? (I am thinking here of Alan Dundes' essay on the \"power\" of three in American culture.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can do some one-off inspections: I would like also to develop code where we could grab all words of a certain frequency, or range of frequencies. (In part, this is a note to myself to understand better the nature of the `vec` and the `bow` so as to be able to interact with them more effectively.)\n",
    "\n",
    "**Follow-up**: whenever I attempt some version of\n",
    "```python\n",
    "for item in vecs:\n",
    "    if vecs.vocabulary_.get(item) == 1691:\n",
    "        print(item)\n",
    "```\n",
    "I get **`TypeError: 'CountVectorizer' object is not iterable`**. My best guess, for now, is that we need to use the tuple above to get this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2124"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.vocabulary_.get('algorithm') # Wait! This doesn't get the count but the index?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algeria', 'algerian', 'algiers', 'algo', 'algorithm']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.get_feature_names()[2120:2125]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of features begins with numbers, and there are a lot of them. (We don't, as yet, know how often or where they occur.) The range below isolates where the list transitions from numbers to words, in case anyone was wondering. The different spellings of \"aaa\" is interesting, but I am most curious about \"aatcagggaccc.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['838', '84', '841', '844', '8462852', '849', '85', '850', '855', '8567', '85871', '85th', '86', '860', '862', '863', '8649', '86th', '87', '88', '880', '883', '885', '886', '89', '890', '8pm', '8th', '8x10', '90', '900', '9000', '90s', '91', '911', '912', '92', '920', '92121', '928', '93', '930', '931', '94', '95', '950', '952', '956', '96', '961', '962', '967', '97', '974', '979', '98', '982', '984', '987', '99', '996', '997', '999', '999999', '9mm', '9th', '______', '________', 'a10', 'a12', 'a380', 'a4', 'a92', 'aa', 'aaa', 'aaaa', 'aaaaa', 'aaaaaaaah', 'aaaaaaah', 'aaaaaah', 'aaaah', 'aaaahhh', 'aaah', 'aag', 'aah', 'aakash', 'aaleh', 'aargh', 'aarhus', 'aaron', 'aaronson', 'aarp', 'aat', 'aatcagggaccc', 'ab', 'ab32', 'aba', 'ababa', 'abacazanian', 'abacha']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vecs.get_feature_names()\n",
    "print(feature_names[900:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the solution to this problem appears to be to return to the tuple and, perhaps, convert it to a dictionary. Or at least that's one solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_dict = dict(words_freq)\n",
    "wf_dict['algorithm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fresh, personally, appreciate, features, pulled, increased, budget, ride, 45, rooms, lay, embrace, stronger, affected, younger, fears, battery, milk, horse, inequality, oceans, algorithm, "
     ]
    }
   ],
   "source": [
    "for key in wf_dict:\n",
    "    if wf_dict[key] == 123:\n",
    "        print(key, end = \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting weirdness I discovered while doing this: \"co\" has apparently been broken off from the nouns and verbs it might modify. Oops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, if we save the term frequency matrix, we need not run all the code above again. Here we are saving the `bow` and the `vec` as pickles. The bow, at 12.5MB, is the larger of the two files. While the `vec` weighs in at a mere 2.5MB by contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/bow.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output a pickle file for the model\n",
    "joblib.dump(bow, '../output/bow.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/vec.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vecs, '../output/vec.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use a TF-IDF matrix to glean how words achieve a certain level of significance? (Is this what NMF does?)\n",
    "\n",
    "\n",
    "**Useful Links**\n",
    "\n",
    "* [Extracting Important Keywords from Text with TF-IDF](https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb) explains how to train and then test a document to discover its keywords. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf = tfidf_transformer.fit(bow)\n",
    "# tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
