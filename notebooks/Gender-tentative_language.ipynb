{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re \n",
    "import nltk\n",
    "import numpy as np, pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 260 talks by women, and 720 talks by men.\n"
     ]
    }
   ],
   "source": [
    "# Remove quotes at top and bottom to uncomment this block\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "\n",
    "# Load binary gendered talks \n",
    "df_f = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "df_m = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "\n",
    "# No one gender ==> NOG\n",
    "# talks_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "\n",
    "# all_talks = pd.concat([talks_male, talks_female, talks_nog])\n",
    "# texts = all_talks.text.tolist()\n",
    "\n",
    "texts_f = df_f.text.tolist()\n",
    "texts_m = df_m.text.tolist()\n",
    "\n",
    "print(f\"There are {len(texts_f)} talks by women, and {len(texts_m)} talks by men.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "\n",
    "corpus = gutenberg.words('melville-moby_dick.txt')\n",
    "text = Text(corpus)\n",
    "con_list = text.concordance_list(\"monstrous\")\n",
    "con_list[2].line\n",
    "'ll over with a heathenish array of monstrous clubs and spears . Some were thick'\n",
    "len(con_list)\n",
    "11\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert the texts above into an NLTK text, then:\n",
    "text.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "> spaCy supports two methods to find word similarity: using context-sensitive tensors and useing word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f7ee4a36050>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f7ee4a360c0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f7ef08ead50>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f7ef08f26e0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f7ef08fbaf0>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f7ef08ea950>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = 'dog cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(words)\n",
    "\n",
    "for token in tokens: \n",
    "    token1, token2 = tokens[0], tokens[1]\n",
    "\n",
    "print (f\"Similarity: {token1.similarity(token2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Libraries\n",
    "\n",
    "Gensim: https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#from-strings-to-vectors"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
