{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts\n",
    "\n",
    "This notebook allows us to examine the contexts in which words occur in sentences throughout the two subcorpora. Contexts vary according to the method used:\n",
    "\n",
    "- Using spaCy we can explore contexts by specifying both a word and its part-of-speech.\n",
    "- Using NLTK's `concordance` functionality we can explore all a word's contexts in the conventional KWiC format -- there is also code here for those interested in lemmatizing words before generating an NLTK `text`.\n",
    "- Using the already-generated SVOs, we can quickly glimpse the related subjects, verbs, and objects for a particular word, though it has to be appear somewhere as an S, V, or O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "The code below uses spaCy's `child` functionality to determine what are the subjects of a sentence and then to return the sentences in which a particular subject appears. It could be adapted to a wide variety of uses. \n",
    "\n",
    "Development of this code was based on insights from this Stackoverflow thread: [How to get the dependency tree with spaCy?](https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy) thread on Stack Overflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd, spacy\n",
    "# from spacy.lang.en import English\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('../output/talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('../output/talks_female.csv', index_col='Talk_ID')\n",
    "\n",
    "# And then grabbing on the texts of the talks:\n",
    "texts_women = talks_f.text.tolist()\n",
    "texts_men = talks_m.text.tolist()\n",
    "\n",
    "# Lowercase everything before we create spaCy docs\n",
    "texts_w = ' '.join([text.lower() for text in texts_women])\n",
    "texts_m = ' '.join([text.lower() for text in texts_men])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# spaCy is fussy about memory allocation\n",
    "# Use the pipe method to feed documents \n",
    "docs_w = nlp.pipe(texts_w)\n",
    "# docs_m = list(nlp.pipe(texts_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows us to specify the subject of the sentence\n",
    "# and to see all the sentences in which it appears as the subject.\n",
    "def find_subject(subject, doc):\n",
    "    subject_sents = []\n",
    "    sentences = doc.sents\n",
    "    for sentence in sentences:\n",
    "        root_token = sentence.root\n",
    "        for child in root_token.children:\n",
    "            if child.dep_ == 'nsubj':\n",
    "                subj = child\n",
    "                if subj.text == subject:\n",
    "                    subject_sents.append(sentence)\n",
    "    return subject_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for doc in docs_w:\n",
    "    find_subject(\"father\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure why **below** works and **above** does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_one = nlp(texts_women[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_subject('father', doc_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts_women[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(subject, doc):\n",
    "    subject_sents = []\n",
    "    sentences = doc.sents\n",
    "    for sentence in sentences:\n",
    "        root_token = sentence.root\n",
    "        for child in root_token.children:\n",
    "            if child.dep_ == 'nsubj':\n",
    "                subj = child\n",
    "                if subj.text == subject:\n",
    "                    subject_sents.append(sentence)\n",
    "    return subject_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_subject(\"i\", docs_w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "To change display results, the contents of the concordance method are: `(\"word\", window width, lines=#[25, all])`. The `window width` is an integer specifying the number of characters to the left and right of a word to display. The default for `lines` is 25, but it can be set to any integer or to `all` (no quotation marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Create NLTK texts for concordances\n",
    "words_w = word_tokenize(\" \".join(talks_f.text.tolist()))\n",
    "women = nltk.Text(words_w)\n",
    "\n",
    "words_m = word_tokenize(\" \".join(talks_m.text.tolist()))\n",
    "men = nltk.Text(words_m)\n",
    "\n",
    "# Test\n",
    "women.concordance('kill', lines=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')  # Download the necessary tokenizer data\n",
    "\n",
    "def find_sentences(text, noun, verb):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    matching_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if (noun in sentence.split()) and (verb in sentence.split()):\n",
    "            matching_sentences.append(sentence)\n",
    "    return matching_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_sentences(texts_w, \"he\", \"kill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')  # Download the necessary tokenizer data\n",
    "# nltk.download('averaged_perceptron_tagger')  # Download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(text, noun, verb):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    matching_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        verb_in_sentence = False\n",
    "        for word, tag in pos_tags:\n",
    "            if word == verb and tag.startswith('VB'):\n",
    "                verb_in_sentence = True\n",
    "                break\n",
    "        if noun in sentence.split() and verb_in_sentence:\n",
    "            matching_sentences.append(sentence)\n",
    "    return matching_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(laughter)    and if he knew i was showing this right now — i put this in today — he would kill me.',\n",
       " \"this took place in egypt in january 2011, and as president hosni mubarak attempted a desperate move to quash the rising revolution on the streets of cairo, he sent his personal troops down to egypt's internet service providers and had them physically kill the switch on the country's connection to the world overnight.\",\n",
       " 'now, my dad is my biggest fan, so in that crushing moment where he wanted to kill my new little life form, i realized that actually i had failed him, both as a daughter and a scientist.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sents(texts_w, \"he\", \"kill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVO\n",
    "\n",
    "While this was at first appealing because of the simplicity and accuracy, since we would be loading the SVOs themselves, the resulting contexts were too impoverished to be much use for hand inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "\n",
    "# LOAD DATAFRAMES\n",
    "# the `lem` suffix indicates the verbs have been lemmatized\n",
    "svos_m = pd.read_csv(\"../output/svos_m_lem.csv\", index_col=0)\n",
    "svos_w = pd.read_csv(\"../output/svos_w_lem.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svos_w.query(' subject==\"he\" & verb==\"kill\" ')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
