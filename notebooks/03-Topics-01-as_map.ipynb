{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics as Map or Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, we are going to run the SciKit-Learn's NMF algorithm which is both fast for smaller corpora and deterministic in its outcomes varying the number of components until we seem to have achieved a stable number.\n",
    "\n",
    "We are including both main and other TED-curated events here so that we can see how much the separate events are part of the larger map or if they are distinct.\n",
    "\n",
    "Or maybe this is a good time to try out **k-means clustering**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, Functions, Stopwords\n",
    "import pandas as pd, re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import NMF\n",
    "import gensim, numpy as np\n",
    "# from itertools import combinations\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\", \"\\(video\\)\", \n",
    "#                   \"\\(laughs\\)\", \"\\(applause ends\\)\", \"\\(audio\\)\", \"\\(singing\\)\", \n",
    "#                   \"\\(music ends\\)\", \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "#                   \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "#                   \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \"\\(marimba sounds\\)\", \n",
    "#                   \"\\(drum sounds\\)\" ]\n",
    "\n",
    "# def remove_parens(text):\n",
    "#     new_text = text\n",
    "#     for rgx_match in parentheticals:\n",
    "#         new_text = re.sub(rgx_match, ' ', new_text.lower(), flags=re.IGNORECASE)\n",
    "#     return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "df = pd.read_csv('../output/TEDall.csv')\n",
    "\n",
    "# Grab the text of the talks\n",
    "talks = df.text.tolist()\n",
    "\n",
    "# Create some labels we can use later but remove the redundant parts of the URL\n",
    "labels = [re.sub('https://www.ted.com/talks/', '',item) for item in df.public_url.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametize the vectorizer:\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = stopwords,\n",
    "                                   min_df = 2, \n",
    "                                   max_df = 0.9)\n",
    "\n",
    "# Vectorize our texts\n",
    "tfidf = tfidf_vectorizer.fit_transform(talks)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(f\"Vocabulary has {len(terms)} distinct terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def rank_terms( A, terms ):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = rank_terms( tfidf, terms )\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print(f\"{i+1:2d} {pair[0]} ({pair[1]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=25,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(tfidf)\n",
    "print(kmeans.inertia_, kmeans.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try it for a range of possible clusters:\n",
    "kmeans_kwargs = {\n",
    "    \"init\": \"random\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 300,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# A list holds the SSE values for each k\n",
    "sse = []\n",
    "for k in range(25, 101, 10):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(tfidf)\n",
    "    sse.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(25, 101, 10), sse)\n",
    "plt.xticks(range(25, 101, 10))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kmeans** doesn't turn up: the slight elbow at 45 doesn't show up when you run `Kneelocator`:\n",
    "\n",
    "```python\n",
    "rom kneed import KneeLocator\n",
    "\n",
    "kl = KneeLocator(range(25, 101, 10), sse, curve=\"convex\", direction=\"decreasing\")\n",
    "print(kl.elbow)\n",
    "\n",
    "UserWarning: No knee/elbow found\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is another way of approaching k through topic coherence as demonstrated by Derek Greene. The current implementation is from his notebook with the following steps:\n",
    "\n",
    "1. Create the topic models\n",
    "2. Build word embedding models\n",
    "3. Select the number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create the Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin, kmax = 20, 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_models = []\n",
    "# try each value of k\n",
    "for k in range(kmin, kmax + 1, 10):\n",
    "    print(\"Applying NMF for k=%d ...\" % k )\n",
    "    # run NMF\n",
    "    model = NMF( init=\"nndsvd\", n_components=k ) \n",
    "    W = model.fit_transform( tfidf )\n",
    "    H = model.components_    \n",
    "    # store for later\n",
    "    topic_models.append( (k,W,H) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Word Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenGenerator:\n",
    "    def __init__( self, documents, stopwords ):\n",
    "        self.documents = documents\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = re.compile( r\"(?u)\\b\\w\\w+\\b\" )\n",
    "\n",
    "    def __iter__( self ):\n",
    "        print(\"Building Word2Vec model ...\")\n",
    "        for doc in self.documents:\n",
    "            tokens = []\n",
    "            for tok in self.tokenizer.findall( doc ):\n",
    "                if tok in self.stopwords:\n",
    "                    tokens.append( \"<stopword>\" )\n",
    "                elif len(tok) >= 2:\n",
    "                    tokens.append( tok )\n",
    "            yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docgen = TokenGenerator( talks, stopwords )\n",
    "# the model has 500 dimensions, the minimum document-term frequency is 20\n",
    "w2v_model = gensim.models.Word2Vec(docgen, size=500, min_count=20, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f\"Model has {len(w2v_model.wv.vocab)} terms.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"../output/w2v_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To re-load this model, run\n",
    "#w2v_model = gensim.models.Word2Vec.load(\"w2v-model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Select the Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence( w2v_model, term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
    "            pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "def get_descriptor( all_terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( all_terms[term_index] )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence( w2v_model, term_rankings ) )\n",
    "    print(f\"K={k}: Coherence={coherences[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 45 as the most slightly suggested starting point, we run the NMF decomposition bracketing it with 35 and 55 and then hand inspecting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First a function to make printing the most associated words with a topic:\n",
    "def print_keywords(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"{topic_idx},\"+\" \".join([feature_names[i] \n",
    "                                            for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently not working:\n",
    "# NameError: name 'topic_idx' is not defined\n",
    "def save_to_csv(model, feature_names, keywords):\n",
    "    with open(f'../output/topics-NMF-{n_components}.csv', mode='w') as the_file:\n",
    "        writer = csv.writer(the_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(f\"{topic_idx},\"+\" \".join([feature_names[i] \n",
    "                                              for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 15\n",
    "n_components = 55\n",
    "\n",
    "# Here's our parameters\n",
    "nmf = NMF(n_components = n_components, \n",
    "          init=\"nndsvd\"\n",
    "         )\n",
    "model = nmf.fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used this to output the topics and then copy and paste them into a CSV.\n",
    "# print_keywords(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the resulting array\n",
    "X = vec.fit_transform(texts)\n",
    "term_matrix = pd.DataFrame(X.todense(), columns=vec.get_feature_names())\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
