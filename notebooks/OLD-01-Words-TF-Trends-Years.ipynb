{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends\n",
    "\n",
    "We are going to do some re-importing of texts here by year. The first time around we are going to do the combined dataset and look for overall trends, and then we will follow that up by loading both the `only` and `plus` datasets separately to see if there are any differences worth noting. Our goal here is to see what words trend not only to learn about TED talks as a developing collection of events but it might also be possible to compare the trends glimpsed here against either trends from the BYU corpus or Google Trends itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1><span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-Data-Load\" data-toc-modified-id=\"Imports-and-Data-Load-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and Data Load</a></span></li><li><span><a href=\"#Working-with-the-Years\" data-toc-modified-id=\"Working-with-the-Years-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Working with the Years</a></span><ul class=\"toc-item\"><li><span><a href=\"#KK-notes:\" data-toc-modified-id=\"KK-notes:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>KK notes:</a></span></li></ul></li><li><span><a href=\"#Finding-Words-with-Limited-Spans-of-Usage\" data-toc-modified-id=\"Finding-Words-with-Limited-Spans-of-Usage-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Finding Words with Limited Spans of Usage</a></span></li><li><span><a href=\"#Trends\" data-toc-modified-id=\"Trends-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Trends</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pandas as pd, matplotlib.pyplot as plt, numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whatever reason, the changing of figure size only ever works for me after I have created the first graph. For that reason, it is on a separate line so that I can run it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../output/TEDall_years.csv', index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>public_url</th>\n",
       "      <th>event</th>\n",
       "      <th>published</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a g...</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garf...</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>If you're here today — and I'm very happy th...</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>Good morning. How are you?    (Laughter)    ...</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>6/27/06</td>\n",
       "      <td>About 10 years ago, I took on the task to te...</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          public_url    event published  \\\n",
       "0  https://www.ted.com/talks/al_gore_on_averting_...  TED2006   6/27/06   \n",
       "1  https://www.ted.com/talks/david_pogue_says_sim...  TED2006   6/27/06   \n",
       "2  https://www.ted.com/talks/majora_carter_s_tale...  TED2006   6/27/06   \n",
       "3  https://www.ted.com/talks/ken_robinson_says_sc...  TED2006   6/27/06   \n",
       "4  https://www.ted.com/talks/hans_rosling_shows_t...  TED2006   6/27/06   \n",
       "\n",
       "                                                text  year  \n",
       "0    Thank you so much, Chris. And it's truly a g...  2006  \n",
       "1    (Music: \"The Sound of Silence,\" Simon & Garf...  2006  \n",
       "2    If you're here today — and I'm very happy th...  2006  \n",
       "3    Good morning. How are you?    (Laughter)    ...  2006  \n",
       "4    About 10 years ago, I took on the task to te...  2006  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "public_url    object\n",
       "event         object\n",
       "published     object\n",
       "text          object\n",
       "year           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the Years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now the data analysis begins with us sorting out the talks into year bins where we can count terms and then determine the best way to find out which words, if any, show notable dynamism. \n",
    "\n",
    "And we are going to have to decide how to define dynamism: Google Trends has a formula for word frequency\n",
    "\n",
    "```\n",
    "                   count of word(1)\n",
    "                  ------------------\n",
    "             count of all words in a year\n",
    "```\n",
    "\n",
    "But one searches on that and it graphs. I think we want to find some way to arrive at some algorithmic \"flagging\" of terms with particular kinds of dynamics.\n",
    "\n",
    "Our hypothesis here is that TED events will likely have some topicality, and so we will see one event dynamism, but we also probably want to try to find words that rise and fall over two years or more.\n",
    "\n",
    "We can make a quick check to see how many talks we have for each year. As `df.groupby('year').size()` reveals, the first year for which we have a substantial number of talks is 2002. We can probably safely start our analysis there.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.groupby('year').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also, somewhat gratuitously, visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('year').size().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to filter by year, so maybe choosing a year like 1998 with 6 talks might be a good place to begin building our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_2001 = df.loc[df['year'] == 2001]\n",
    "year_2001.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do is count all the words in a given year and then be able to compare across years. So, what we want is something like this:\n",
    "\n",
    "1. count all the words in a year\n",
    "2. divide every word by the total count to assign it its frequency for the year[^1]\n",
    "3. create a pandas dataframe (?) that looks like this:\n",
    "\n",
    "```\n",
    "| word  | 2002  | 2003  | 2004  |  ...  |\n",
    "|-------|-------|-------|-------|-------|\n",
    "| other | 0.007 | 0.007 |0.0008 | 0.001 |\n",
    "| stuff | 0.001 | 0.002 | 0.002 | 0.003 |\n",
    "```\n",
    "\n",
    "So, do we create 16 lists, each with all the words for a year, and then write date back into a dataframe or is there a way to do this within **pandas**? (I'm trying to grok \"the pandas way\" as much as possible.)\n",
    "\n",
    "[^1]: This is how Google Trends does it. See: Younes, N., & Reips, U. D. (2019). Guideline for improving the reliability of Google Ngram studies: Evidence from religious terms. PloS one, 14(3), e0213554. doi:10.1371/journal.pone.0213554."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to do this, I think, is to feed a function or a for loop a list of years, filter the dataframe by year, and create a giant list of words for each year. We need both the year and a name for that year, so we are going to need to get the years we want and then create an object that pairs the year with the label for the bag of words we are going to create. \n",
    "\n",
    "The code below is a little fussy, but it filters out the years we don't want and then creates a dictionary with the year as the key and the BoW name as the value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the heart of this is a lambda function, which allows us to apply a filter \n",
    "# the list and sorted functions add a lot of parentheses.\n",
    "years = sorted(list(filter(lambda year: year > 2001, df.year.unique())))\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think what we need to do is create empty dictionaries for each year and then fill those. \n",
    "\n",
    "Two levels: \n",
    "1. First years to dictionaries.   \n",
    "   keys = years   \n",
    "   items = each year's dictionary   \n",
    "2. Level 2:   \n",
    "   keys = word   \n",
    "   items = count   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KK notes:\n",
    "    \n",
    "https://stackoverflow.com/questions/12453580/concatenate-item-in-list-to-strings\n",
    "\n",
    "https://codeburst.io/python-basics-11-word-count-filter-out-punctuation-dictionary-manipulation-and-sorting-lists-3f6c55420855\n",
    "\n",
    "```python\n",
    "year_words = {}\n",
    "\n",
    "for year in years[0]:\n",
    "    yeartalks = df.loc[df['year'] == year].text.tolist()\n",
    "    onetalk = \" \".join(yeartalks)\n",
    "    \n",
    "    # Make dictionary called word_counts. Choose from codeburst\n",
    "    \n",
    "    #year_words[year] = word_counts\n",
    "    year_words[year] = yeartalks[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onetext (label):\n",
    "    \"\"\"Simple function to grab texts out of pandas dataframe\"\"\"\n",
    "    texts = df.loc[df['year'] == label].text.tolist()\n",
    "    onetext = \" \".join(texts)\n",
    "    return onetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeartexts = dict(zip([year for year in years], [onetext(year) for year in years]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of our texts now in a dictionary with the year as the key and the talks for a given year in one long string as a value, we now need to create a term frequency matrix with the years as rows and the words as columns. I know we can feed a list of texts into **Sci-Kit Learn** and it will build us an array, but we also want to associate those rows with our years..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a step back, but until I can discover how to send a dictionary to the vectorizer and get named rows, we are going to create two lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, t = zip(*yeartexts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to pass options, pass them here:\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the model to the data \n",
    "vecs = vectorizer.fit_transform(t)\n",
    "\n",
    "# check our work\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the vectors established, we need to send them to a **numpy** array and then we can send that array to a **pandas** dataframe. In order to have human-understandable indices and column headers, we will use the `years` list for the index and create a list of terms using `get_feature_names()` for the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn vector thingamabob to numpy array\n",
    "vec_array = vecs.toarray()\n",
    "\n",
    "# terms for column headers\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# from array to dataframe\n",
    "trends = pd.DataFrame(data = vec_array, index = years, columns = feature_names)\n",
    "\n",
    "# check our work\n",
    "trends.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, we will save this to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends.to_csv('../output/tf_trends.csv', sep=',') # This is a 2.1 MB file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Words with Limited Spans of Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, **words that occur only once, in a small cluster, or sporadically**.\n",
    "\n",
    "Before we get to the trends across the years, for which we will sum each year and then divide each word's count by that sum, let's just take a look to see if there are words that occur only in one year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000000004</th>\n",
       "      <th>0000001</th>\n",
       "      <th>000001</th>\n",
       "      <th>00001</th>\n",
       "      <th>000042</th>\n",
       "      <th>0001</th>\n",
       "      <th>00046</th>\n",
       "      <th>000th</th>\n",
       "      <th>...</th>\n",
       "      <th>ālep</th>\n",
       "      <th>čapek</th>\n",
       "      <th>ōfunato</th>\n",
       "      <th>ʾan</th>\n",
       "      <th>ʾilla</th>\n",
       "      <th>ʾilāha</th>\n",
       "      <th>อย</th>\n",
       "      <th>อยman</th>\n",
       "      <th>อร</th>\n",
       "      <th>送你葱</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  000000004  0000001  000001  00001  000042  0001  00046  000th  \\\n",
       "2002   0   43          0        0       0      0       0     0      0      0   \n",
       "2003   0   54          0        0       0      0       0     0      0      0   \n",
       "2004   0   61          0        0       0      0       0     0      0      0   \n",
       "2005   0  138          0        0       0      0       0     0      0      1   \n",
       "2006   0   62          0        0       0      0       0     0      0      0   \n",
       "\n",
       "      ...  ālep  čapek  ōfunato  ʾan  ʾilla  ʾilāha  อย  อยman  อร  送你葱  \n",
       "2002  ...     0      0        0    0      0       0   0      0   0    0  \n",
       "2003  ...     0      0        0    0      0       0   0      0   0    0  \n",
       "2004  ...     0      0        0    0      0       0   0      0   0    0  \n",
       "2005  ...     0      0        0    0      0       0   0      0   0    0  \n",
       "2006  ...     0      0        0    0      0       0   0      0   0    0  \n",
       "\n",
       "[5 rows x 50096 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-load the trends dataframe\n",
    "trends = pd.read_csv('../output/tf_trends.csv', index_col = 0)\n",
    "trends.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = trends.columns.tolist()\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['consented', 'consenting', 'consequence', 'consequences', 'consequential', 'consequently', 'conservancies', 'conservancy', 'conservation', 'conservationist']\n"
     ]
    }
   ],
   "source": [
    "print(cols[10000:10010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a [SO question][] for the solution below.\n",
    "\n",
    "[SO question]: https://stackoverflow.com/questions/57630072/find-columns-with-only-one-non-zero-value-in-pandas/57630162#57630162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.melt('year').loc[lambda x : x['value']!=0].groupby('variable')['value'].apply(tupl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = trends.sum(axis=1)\n",
    "print(type(sums), \"\\n\", sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mimicking Google Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to sum the words for a given year and then divide each words count by the sum in order to determine its normalized frequency for a given year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by recreating our one text per year upno which we will base our word counts. We are not simply using the `trends` dataframe (above) for the moment because we want to see how we can tweak the counts by perhaps cutting off words that only occur once and thus won't really be a compelling addition to an examination of trends. There are also a lot of numbers in the raw text, and I don't know how useful they are -- though it does suggest a separate examination is called for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../output/TEDall_years.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the version of onetext below has been changed to remove all non-letter characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onetext (label):\n",
    "    \"\"\"Simple function to grab texts out of pandas dataframe\"\"\"\n",
    "    texts = df.loc[df['year'] == label].text.tolist()\n",
    "    joined = \" \".join(texts)\n",
    "    onetext = re.sub(\"[^a-zA-Z']\",\" \", joined)\n",
    "    return onetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(list(filter(lambda year: year > 2001, df.year.unique())))\n",
    "\n",
    "yeartexts = dict(zip([year for year in years], \n",
    "                     [onetext(year) for year in years]))\n",
    "\n",
    "y, t = zip(*yeartexts.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a list of tuples which has the year and its total word count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2002, 429343), (2003, 499867), (2004, 469364), (2005, 1042616), (2006, 624273), (2007, 1113749), (2008, 671601), (2009, 1656804), (2010, 1830130), (2011, 1596795), (2012, 1492180), (2013, 1671871), (2014, 1551879), (2015, 1594325), (2016, 1573638), (2017, 1718390)]\n"
     ]
    }
   ],
   "source": [
    "wordcounts = [len(text) for text in t]\n",
    "yearcounts = list(zip(years, wordcounts))\n",
    "print(yearcounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will vectorize the words for all the years -- I went back and forth a number of times on the value to provide `min_df`. Because our \"documents\" are analytical fictions of all talks for a given year and not individual talks, we do need to see if there are words that only occur in a given year. So we'll set `min_df = 1`, but that nearly doubles are word count to 48959 -- `min_df = 2` yields 28013 words.\n",
    "\n",
    "Perhaps what we can do later is throw away words with values below a certain level -- so a word that only occurs once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to pass options, pass them here:\n",
    "vectorizer = CountVectorizer(max_df = 1.0, \n",
    "                             min_df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 48959)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model to the data \n",
    "vecs = vectorizer.fit_transform(t)\n",
    "\n",
    "# check our work\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaaaaah', 'aaaah', 'aah', 'aaron', 'ab', 'ababa', 'abacha', 'aback', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abate', 'abaya', 'abbey', 'abbreviated', 'abbreviation', 'abby', 'abc', 'abdomen', 'abdomens', 'abdominal', 'abducted', 'abduction', 'abdul', 'abdullah', 'abe', 'abhors', 'abide', 'abiding', 'abidjan', 'abilities', 'ability', 'abject', 'ablaze', 'able', 'abnormal', 'abnormalities', 'abnormality', 'aboard', 'abode', 'abolish', 'abolished', 'abolishing', 'abolition', 'abomination', 'aboriginal', 'abort', 'aborted']\n"
     ]
    }
   ],
   "source": [
    "print(features[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the original \"bookworm\" code look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
