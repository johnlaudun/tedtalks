{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequencies partitioned by gender\n",
    "\n",
    "In previous notebooks, we have used speakers' genders to apply a gender label to each talk. The gender labels used are: male, female, and not one gender. We note that the list of speakers in the corpus of TED talks identify on the gender binary, at least in the public facing documents about them (which include their TED talk bios and third party websites and publicity). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This is the second attempt to look a freqencies by gender. In the first attempt in `03-Terms-by-gender`, we applied the method from `01-Terms.ipynb` on just the talks gendered as \"female.\" This approach has some limits, chiefly that if we also do the same to \"male\" talks, we have no clear method for comparing how different words appear in talks by either binary gender. \n",
    "\n",
    "In this second attempt, we will instead load the talks such that the male talks are all together, followed by the female talks, and finally followed by those talks that are neither binary gender. \n",
    "\n",
    "With the data loaded in this partitioned manner, we will then count the words in each of the texts and like with the previous attempt, addres the matter of parentheticals head on. Much of this work occurs in a separate notebook, and the explanations of how that work relates to this process is detailed in `01-Terms.ipynb`.\n",
    "\n",
    "In this notebook, we will **not** take each subset individually but rather as one partitioned set (first talks labeled male, female, and not one gender) and complete the following procedure: \n",
    "* Remove the parentheticals\n",
    "* Create term matrix from the resulting vocabulary\n",
    "* Adjust the threshold for a word to be included in the term-matrix\n",
    "* Export the resulting as a CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd, re, csv, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL FUNCTION --> Explained in 01-Terms.ipynb\n",
    "\n",
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\", \"\\(video\\)\", \n",
    "                  \"\\(laughs\\)\", \"\\(applause ends\\)\", \"\\(audio\\)\", \"\\(singing\\)\", \n",
    "                  \"\\(music ends\\)\", \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \"\\(marimba sounds\\)\", \n",
    "                  \"\\(drum sounds\\)\" ]\n",
    "\n",
    "def remove_parentheticals(text):\n",
    "    global parentheticals\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), flags=re.IGNORECASE)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Load binary gendered talks \n",
    "talks_male = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_female = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "\n",
    "# No one gender ==> NOG\n",
    "talks_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714 260 8\n"
     ]
    }
   ],
   "source": [
    "print(talks_male.shape[0],talks_female.shape[0],talks_nog.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 14)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the data\n",
    "\n",
    "all_talks = pd.concat([talks_male,talks_female,talks_nog])\n",
    "all_talks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check that the shape is what we expect\n",
    "\n",
    "print(all_talks.shape[0] == (talks_male.shape[0]+talks_female.shape[0]+talks_nog.shape[0]))\n",
    "print(all_talks.shape[1] == talks_nog.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Frequencies\n",
    "\n",
    "The goal of this notebook is to establish how we are going to elicit our features, our words, from the collection of talks. Thus, the only column we are interested in is the one with the texts of the talks. While we recognize the utility of having the data in one file, we are looking to minimize individual file sizes, and also make it possible only to load the data we need for any particular task. For that reason, we have chosen to create a number of CSV files, using the ID number that TED assigns each talk as it publishes them to its website.\n",
    "\n",
    "For this first experiment, we keep it simple by creating two lists, one of the talk IDs and one of the texts themselves: we will only be using the IDs later when we write the DTM to a CSV. This ID will allow us later to correlate the DTM with metadata associated with each talk: the speaker, her biography, the talk's popularity, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_texts = all_talks.text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota bene**: If you are using this notebook to re-create the basic TF matrix you can stop here and jump to the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to get term frequencies, but **SciKit-Learn**'s `CountVectorizer` offers a one-stop shop for generating a DTM from which we can examine words as well as generate BoW-products like topic models. This is made possible by the interoperability between `CountVectorizer` and the other vectorizers and models available in `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In our first experiment, we run `CountVectorizer` unadorned. The default options are: lowercase everything, get rid of all punctuation, make a word out of anything more than two characters long. The only thing that might not be welcome is the splitting of contractions. For now, we will leave things as they are. (Also, please note, no stopwords were used, so we have an unfiltered word list and no pre-processing of the texts is done.)\n",
    "\n",
    "CHECK THIS ASSERTION: Since `CountVectorizer` only counts when *fitting* and does not engage in any kind of normalization or centering, *transforming*, there is no need to break the two functionalities out in the code that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 39418)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to pass options, pass them here:\n",
    "vectorizer = CountVectorizer( lowercase = True )\n",
    "\n",
    "# fit the model to the data \n",
    "# vecs = vectorizer.fit(partitioned_texts)\n",
    "X = vectorizer.fit_transform(partitioned_texts)\n",
    "\n",
    "# see how many features we have\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "39418 tokens is our raw, unfiltered, no pre-processing baseline. It includes, as we will see, a number of artifacts of the TED transcription process, including a variety of ways to spell out *ah*, the use of numerals for a host of things -- from dates to counts and everything in between, and some things which actually take places outside the talk itself, like audience laughter, which we here term *parentheticals*. (Accounting for all these artifacts requires a number of closer inspections, which take place in a separate notebook: [Term Artifacts](Terms-02).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 39418)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names())\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39418"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list = list(raw_df.columns)\n",
    "\n",
    "len(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list.index(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaa',\n",
       " 'aaaa',\n",
       " 'aaaaa',\n",
       " 'aaaah',\n",
       " 'aag',\n",
       " 'aah',\n",
       " 'aakash',\n",
       " 'aargh',\n",
       " 'aaron']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list[782:792]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list.index(\"aakash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa       7\n",
      "aaa      6\n",
      "aaaa     2\n",
      "aaaaa    0\n",
      "aaaah    2\n",
      "aag      1\n",
      "dtype: int64\n",
      "aa       4\n",
      "aaa      0\n",
      "aaaa     0\n",
      "aaaaa    1\n",
      "aaaah    0\n",
      "aag      0\n",
      "dtype: int64\n",
      "aa       0\n",
      "aaa      0\n",
      "aaaa     0\n",
      "aaaaa    0\n",
      "aaaah    0\n",
      "aag      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Crowd noises are at 782 through 788\n",
    "\n",
    "male_crowd = raw_df.iloc[:714,782:788].sum()\n",
    "print(male_crowd)\n",
    "\n",
    "female_crowd = raw_df.iloc[714:(714+260),782:788].sum()\n",
    "print(female_crowd)\n",
    "\n",
    "nog_crowd = raw_df.iloc[-8:,782:788].sum()\n",
    "print(nog_crowd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_df.iloc[-8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First words start at index 789\n",
    "\n",
    "789/39418\n",
    "\n",
    "## About 2% of the way into the word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many of the non-words each talk has (regardless of how many times each non-word appears.)\n",
    "m_test = (raw_df.iloc[:714,:789]>0).sum(axis = 1)\n",
    "f_test = (raw_df.iloc[714:(714+260),:789]>0).sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(m_test, 30, density=True, facecolor='g', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD6CAYAAACvZ4z8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQgElEQVR4nO3df6zdd13H8efLuy3AgGy6i9S20mIasVmQLTdlEUOYgGmHsWiiWQ0M5h91ySqbkejkH9DEhJhJYMmypm41LE4awqY2pnHwR4mSsNm7MTdKHd7USS8t9BJkYy5h1L3943wnJ5dze7+3997dez57PpKTe76fH9/z+eSbvvrt937Op6kqJEnt+om1HoAkaXUZ9JLUOINekhpn0EtS4wx6SWqcQS9JjesV9El2JnkyyUyS20bUvynJl5P8IMmHh8o3Jzma5ESS40luWcnBS5IWl8XW0SeZAL4OvBuYBY4Be6rqa0NtXge8AXgv8N9VdXtXvgHYUFWPJnkN8Ajw3uG+o1xxxRW1ZcuWC52TJL3sPPLII9+pqslRdRf16L8DmKmqkwBJDgG7gf8P66o6C5xN8p7hjlV1BjjTvf9+khPAxuG+o2zZsoXp6ekeQ5MkAST5r4Xq+jy62QicGjqe7cqWOogtwFXAwwvU700ynWR6bm5uqaeXJC2gT9BnRNmS9k1I8mrgfuDWqnpmVJuqOlBVU1U1NTk58l8fkqQL0CfoZ4HNQ8ebgNN9PyDJxQxC/r6qemBpw5MkLVefoD8GbEuyNcklwPXA4T4nTxLgHuBEVX3iwocpSbpQi/4ytqrOJdkHPAhMAAer6niSm7r6/UleD0wDrwVeSHIrsB14M/B+4Ikkj3Wn/EhVHVnxmUiSRuqz6oYumI/MK9s/9P5bDB7pzPclRj/jlyS9RPxmrCQ1zqCXpMYZ9JLUuF7P6LW+XXttv3ZHj67uOCStT97RS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS4XkGfZGeSJ5PMJLltRP2bknw5yQ+SfHgpfSVJq2vRoE8yAdwJ7AK2A3uSbJ/X7LvAh4DbL6CvJGkV9bmj3wHMVNXJqnoeOATsHm5QVWer6hjww6X2lSStrj5BvxE4NXQ825X10btvkr1JppNMz83N9Ty9JGkxfYI+I8qq5/l7962qA1U1VVVTk5OTPU8vSVpMn6CfBTYPHW8CTvc8/3L6SpJWQJ+gPwZsS7I1ySXA9cDhnudfTl9J0gq4aLEGVXUuyT7gQWACOFhVx5Pc1NXvT/J6YBp4LfBCkluB7VX1zKi+qzQXSdIIiwY9QFUdAY7MK9s/9P5bDB7L9OorSXrp+M1YSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGtdrP/pxcu21/dodPbq645Ck9cI7eklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS45pZXavlcoiq1xTt6SWqcQS9JjTPoJalxBr0kNa5X0CfZmeTJJDNJbhtRnyR3dPWPJ7l6qO4PkhxP8tUkn0nyipWcgCTp/BYN+iQTwJ3ALmA7sCfJ9nnNdgHbutde4K6u70bgQ8BUVV0JTADXr9joJUmL6nNHvwOYqaqTVfU8cAjYPa/NbuDeGngIuCzJhq7uIuCVSS4CXgWcXqGxS5J66BP0G4FTQ8ezXdmibarqm8DtwDeAM8DTVfX5UR+SZG+S6STTc3NzfccvSVpEn6DPiLLq0ybJ5Qzu9rcCPwNcmuR9oz6kqg5U1VRVTU1OTvYYliSpjz5BPwtsHjrexI8/flmozbuA/6yquar6IfAA8EsXPlxJ0lL1CfpjwLYkW5NcwuCXqYfntTkM3NCtvrmGwSOaMwwe2VyT5FVJArwTOLGC45ckLWLRvW6q6lySfcCDDFbNHKyq40lu6ur3A0eA64AZ4Dngxq7u4SSfAx4FzgFfAQ6sxkQkSaP12tSsqo4wCPPhsv1D7wu4eYG+HwU+uowxSpKWwd0rX0b67kopqS1ugSBJjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4d6/Uquu7a+bRo6s7Dunlyjt6SWqcQS9JjTPoJalxBr0kNc6gl6TGuepmEa4YkTTuvKOXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9Jjeu1vDLJTuBTwARwd1V9fF59uvrrgOeAD1bVo13dZcDdwJVAAb9bVV9eqQmMm77LNcElm5JWxqJ39EkmgDuBXcB2YE+S7fOa7QK2da+9wF1DdZ8C/qmq3gT8InBiBcYtSeqpz6ObHcBMVZ2squeBQ8DueW12A/fWwEPAZUk2JHkt8HbgHoCqer6qvrdyw5ckLaZP0G8ETg0dz3Zlfdq8EZgD/jrJV5LcneTSUR+SZG+S6STTc3NzvScgSTq/PkGfEWXVs81FwNXAXVV1FfA/wG2jPqSqDlTVVFVNTU5O9hiWJKmPPkE/C2weOt4EnO7ZZhaYraqHu/LPMQh+SdJLpE/QHwO2Jdma5BLgeuDwvDaHgRsycA3wdFWdqapvAaeS/HzX7p3A11Zq8JKkxS26vLKqziXZBzzIYHnlwao6nuSmrn4/cITB0soZBssrbxw6xe8D93V/SZycV9eMpSybXMtzSnr56bWOvqqOMAjz4bL9Q+8LuHmBvo8BUxc+REnScvjNWElqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1rtemZi1yZ0hJLxfe0UtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxvUK+iQ7kzyZZCbJbSPqk+SOrv7xJFfPq59I8pUk/7hSA5ck9bNo0CeZAO4EdgHbgT1Jts9rtgvY1r32AnfNq78FOLHs0UqSlqzPHf0OYKaqTlbV88AhYPe8NruBe2vgIeCyJBsAkmwC3gPcvYLjliT11CfoNwKnho5nu7K+bT4J/BHwwvk+JMneJNNJpufm5noMS5LUR5+gz4iy6tMmya8BZ6vqkcU+pKoOVNVUVU1NTk72GJYkqY8+QT8LbB463gSc7tnmbcCvJ3mKwSOfX0nyNxc8WknSkvX5z8GPAduSbAW+CVwP/M68NoeBfUkOAW8Fnq6qM8CfdC+SvAP4cFW9b2WGrrW20v/B+lLOd/Toyn621LJFg76qziXZBzwITAAHq+p4kpu6+v3AEeA6YAZ4Drhx9YYsSVqKPnf0VNURBmE+XLZ/6H0BNy9yji8CX1zyCCVJy+I3YyWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMb12utGWm/67nTpLpeSd/SS1DyDXpIaZ9BLUuMMeklqnEEvSY1z1Y2a5uocyTt6SWqeQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiXV0pL4HJNjSPv6CWpcQa9JDWuV9An2ZnkySQzSW4bUZ8kd3T1jye5uivfnORokhNJjie5ZaUnIEk6v0WDPskEcCewC9gO7EmyfV6zXcC27rUXuKsrPwf8YVX9AnANcPOIvpKkVdTnjn4HMFNVJ6vqeeAQsHtem93AvTXwEHBZkg1VdaaqHgWoqu8DJ4CNKzh+SdIi+gT9RuDU0PEsPx7Wi7ZJsgW4Cnh41Ick2ZtkOsn03Nxcj2FJkvros7wyI8pqKW2SvBq4H7i1qp4Z9SFVdQA4ADA1NTX//NKq6rtscq0+1+WaWo4+d/SzwOah403A6b5tklzMIOTvq6oHLnyokqQL0SfojwHbkmxNcglwPXB4XpvDwA3d6ptrgKer6kySAPcAJ6rqEys6cklSL4s+uqmqc0n2AQ8CE8DBqjqe5Kaufj9wBLgOmAGeA27sur8NeD/wRJLHurKPVNWRFZ2FJGlBvbZA6IL5yLyy/UPvC7h5RL8vMfr5vSTpJeI3YyWpcQa9JDXO3SslvaRcUvrS845ekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc7lldIqWKvdMKVRvKOXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjXN5pTQG1nK5prtILt9a79jpHb0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnMsrJZ3XWi3tXI3P7bt8ca2XQ6407+glqXEGvSQ1zqCXpMb1CvokO5M8mWQmyW0j6pPkjq7+8SRX9+0rSVpdiwZ9kgngTmAXsB3Yk2T7vGa7gG3day9w1xL6SpJWUZ87+h3ATFWdrKrngUPA7nltdgP31sBDwGVJNvTsK0laRX2WV24ETg0dzwJv7dFmY8++ACTZy+BfAwDPJnmyx9hGuQL4zgX2XW9amUsr8wDnsh71nkeysh+8CudbzjV5w0IVfYJ+1FSqZ5s+fQeFVQeAAz3Gc15JpqtqarnnWQ9amUsr8wDnsh61Mg9Yvbn0CfpZYPPQ8SbgdM82l/ToK0laRX2e0R8DtiXZmuQS4Hrg8Lw2h4EbutU31wBPV9WZnn0lSato0Tv6qjqXZB/wIDABHKyq40lu6ur3A0eA64AZ4DngxvP1XZWZ/MiyH/+sI63MpZV5gHNZj1qZB6zSXFI18pG5JKkRfjNWkhpn0EtS45oJ+pa2WkjyVJInkjyWZHqtx7MUSQ4mOZvkq0NlP5nkC0n+o/t5+VqOsa8F5vKxJN/srs1jSa5byzH2kWRzkqNJTiQ5nuSWrnzsrst55jKO1+UVSf41yb91c/nTrnzFr0sTz+i7rRa+DrybwVLPY8Ceqvramg7sAiV5CpiqqrH7MkuStwPPMvim9JVd2V8A362qj3d/CV9eVX+8luPsY4G5fAx4tqpuX8uxLUX3LfUNVfVoktcAjwDvBT7ImF2X88zltxm/6xLg0qp6NsnFwJeAW4DfZIWvSyt39G61sE5U1T8D351XvBv4dPf+0wz+YK57C8xl7FTVmap6tHv/feAEg2+tj911Oc9cxk63Zcyz3eHF3atYhevSStAvtAXDuCrg80ke6baGGHc/3X2vgu7n69Z4PMu1r9ul9eA4PO4YlmQLcBXwMGN+XebNBcbwuiSZSPIYcBb4QlWtynVpJeh7b7UwJt5WVVcz2PXz5u4RgtaHu4CfA94CnAH+ck1HswRJXg3cD9xaVc+s9XiWY8RcxvK6VNX/VtVbGOwasCPJlavxOa0EfZ9tGsZGVZ3ufp4F/o7Bo6lx9u3u2eqLz1jPrvF4LlhVfbv7w/kC8FeMybXpngHfD9xXVQ90xWN5XUbNZVyvy4uq6nvAF4GdrMJ1aSXom9lqIcml3S+ZSHIp8KvAV8/fa907DHyge/8B4B/WcCzL8uIfwM5vMAbXpvul3z3Aiar6xFDV2F2XheYyptdlMsll3ftXAu8C/p1VuC5NrLoB6JZTfZIfbbXw52s7oguT5I0M7uJhsEXF347TXJJ8BngHg61jvw18FPh74LPAzwLfAH6rqtb9LzkXmMs7GDweKOAp4PdefJ66XiX5ZeBfgCeAF7rijzB4tj1W1+U8c9nD+F2XNzP4ZesEg5vuz1bVnyX5KVb4ujQT9JKk0Vp5dCNJWoBBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhr3f4jHPeO/G+xCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(f_test, 30, density=True, facecolor='b', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_series = df_raw.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00             15\n",
       "000          2098\n",
       "000000004       1\n",
       "0000001         1\n",
       "000001          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hand inspection of the output above turned up a couple of interesting issues: that there are parentheticals mixed in with the text of the talks (see 01-Terms-02-Parentheticals) and numbers feature in the talks regularly (see 01-Terms-03-Numbers). \n",
    "\n",
    "There are also some interesting tokenization dimensions to a crowd-sourced transcription technology:\n",
    "\n",
    "| Term    | Freq |\n",
    "|:---------|---:|\n",
    "|aa       |12 |\n",
    "|aaa      |7  |\n",
    "|aaaa     |2  |\n",
    "|aaaaa    |1  |\n",
    "|aaaaaaaah|1  |\n",
    "|aaaaaaah |1  |\n",
    "|aaaaaah  |3  |\n",
    "|aaaah    |2  |\n",
    "|aaaahhh  |1  |\n",
    "|aaah     |3  |\n",
    "|aag      |1  |\n",
    "|aah      |10 |\n",
    "\n",
    "There are an equal number of alternate spellings for *shh*. There's also this oddity:\n",
    "\n",
    "| Term    | Freq |\n",
    "|:---------|---:|\n",
    "|FALSE    |115|\n",
    "|TRUE     |909|\n",
    "\n",
    "It appears to be simply the occurrences of *true* and *false* but somehow the vectorizer thinks they are Booleans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the hand-examination turns up no other issues, so the basic vectorization built into `sklearn` appears to be satisfactory, with the only exception being its breaking of contractions at the apostrophe: e.g., *isn't* becomes *isn* and *'t*, with the latter being thrown away as too small -- which means that the indefinite article *a(n)* is also not present in the frequencies, but the definite article *the* remains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised Frequencies without Parentheticals\n",
    "\n",
    "For more on the work leading up to this regex, see the `01-Terms-02-Parentheticals` notebook: it contains a number of experiments, which were vexed by sklearn's vectorizer only accepting strings as inputs. (While gensim appears to accept texts as lists of words, we are hoping, I think, to keep our code base as simple as possible.) The eventual workaround is to clean the texts of the top 20 parentheticals, joining the list back to a string, and then pushing the result to sklearn. This is less than optimum, but the hack works and appears to be a hack performed elsewhere by others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, the first thing we have is the list of the top 20 parentheticals, some of which are two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 50377)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noparens_vec = CountVectorizer(preprocessor = remove_parentheticals)\n",
    "noparens_X = noparens_vec.fit_transform(texts)\n",
    "noparens_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news. Our removal parentheticals has only removed two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "noparens_df = pd.DataFrame(noparens_X.toarray(), columns = noparens_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The baseline count for *laughter* is 7374; with cleaning it is 98'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = 'laughter'\n",
    "f\"The baseline count for *{term}* is {raw_df[term].sum()}; \\\n",
    "with cleaning it is {noparens_df[term].sum()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies of Terms with a 2-Document Minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a `min_df = 2` removes all words that appear in only document. (An exploration of *one document wonders* is elsewhere.) As we can see from the `shape` below, we dropped 50379 to 29340, resulting in a reduction of 42% of the possible features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 29340)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min2_vec = CountVectorizer(preprocessor = remove_parentheticals, min_df = 2)\n",
    "min2_X = min2_vec.fit_transform(texts)\n",
    "min2_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above we can sum our terms and write the results to a CSV file for closer inspection. Here, we highlight just a particular series of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "affiliations    3\n",
       "affinity        3\n",
       "affirm          7\n",
       "affirmation     4\n",
       "affirmations    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe\n",
    "min2_df = pd.DataFrame(min2_X.toarray(), columns = min2_vec.get_feature_names())\n",
    "\n",
    "# Create a series of sums\n",
    "min2_series = min2_df.sum()\n",
    "\n",
    "# Write sums to CSV:\n",
    "# min2_series.to_csv('../output/word_freq_clean.csv')\n",
    "\n",
    "# Inspect the sums\n",
    "min2_series[1000:1005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies of Terms Appearing in All Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trimmed terms that occur in only document from our feature set, we also need to determine what words occur across all the documents and thus are not particularly effective discriminators of topic. \n",
    "\n",
    "Note that this is a somewhat more dangerous move to make, since it has been shown that one of the keys to genre and gender in documents are the varying uses of so-called function words. In fact, as our explorations establish, using sklearn's built-in algorithms reveals a focus on removing common words does not really result in much of a reduction in the overall feature set, only 8 words occurring across 99% or more documents and that number only rising to 46 when we change the minimum threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs_vec = CountVectorizer(preprocessor = remove_parentheticals, min_df = 0.99)\n",
    "alldocs_X = alldocs_vec.fit_transform(texts)\n",
    "alldocs_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and    118989\n",
       "in      62673\n",
       "is      50072\n",
       "it      59191\n",
       "of      92416\n",
       "the    166093\n",
       "to     102276\n",
       "you     56296\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs_df = pd.DataFrame(alldocs_X.toarray(), columns = alldocs_vec.get_feature_names())\n",
    "alldocs_series = alldocs_df.sum()\n",
    "alldocs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 46)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostdocs_vec = CountVectorizer(preprocessor = remove_parentheticals, min_df = 0.90)\n",
    "mostdocs_X = mostdocs_vec.fit_transform(texts)\n",
    "mostdocs_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostdocs_df = pd.DataFrame(mostdocs_X.toarray(), columns = mostdocs_vec.get_feature_names())\n",
    "# mostdocs_series = mostdocs_df.sum()\n",
    "# mostdocs_series.head(46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last run simply verifies that the exploration above with `min_df` holds true when we switch to `max_df`. That is, that our exploration of the parameters were not asking something unexpected by the code and possibly generating inexplicable and undesirable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 29294)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tnt = topsntails\n",
    "tnt_vec = CountVectorizer(preprocessor = remove_parentheticals, max_df = 0.9, min_df = 2)\n",
    "tnt_X = tnt_vec.fit_transform(texts)\n",
    "tnt_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers add up. With that done, my recommendation is that we do not, for the time being, throw away common and/or function words: that is, let's not use a stopword list. Those words could be important in other ways, and dropping them has only a limited impact on the actual document-term matrix, one that pales by comparison to the words that occur only in one document."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
