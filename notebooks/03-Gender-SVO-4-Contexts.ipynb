{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts\n",
    "\n",
    "This notebook allows us to examine the contexts in which words occur in sentences throughout the two subcorpora. Contexts vary according to the method used:\n",
    "\n",
    "- Using spaCy we can explore contexts by specifying both a word and its part-of-speech.\n",
    "- Using NLTK's `concordance` functionality we can explore all a word's contexts in the conventional KWiC format -- there is also code here for those interested in lemmatizing words before generating an NLTK `text`.\n",
    "- Using the already-generated SVOs, we can quickly glimpse the related subjects, verbs, and objects for a particular word, though it has to be appear somewhere as an S, V, or O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "The code below uses spaCy's `child` functionality to determine what are the subjects of a sentence and then to return the sentences in which a particular subject appears. It could be adapted to a wide variety of uses. \n",
    "\n",
    "Development of this code was based on insights from this Stackoverflow thread: [How to get the dependency tree with spaCy?](https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy) thread on Stack Overflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd, spacy\n",
    "# from spacy.lang.en import English\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('../output/talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('../output/talks_female.csv', index_col='Talk_ID')\n",
    "\n",
    "# And then grabbing on the texts of the talks:\n",
    "texts_women = talks_f.text.tolist()\n",
    "texts_men = talks_m.text.tolist()\n",
    "\n",
    "# Lowercase everything before we create spaCy docs\n",
    "texts_w = ' '.join([text.lower() for text in texts_women])\n",
    "texts_m = ' '.join([text.lower() for text in texts_men])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy is fussy about memory allocation\n",
    "# Use the pipe method to feed documents \n",
    "docs_w = list(nlp.pipe(texts_w))\n",
    "docs_m = list(nlp.pipe(texts_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_svpairs(text, noun, verb):\n",
    "#     doc = nlp(text)\n",
    "    matching_sentences = []\n",
    "    for sent in doc.sents:\n",
    "        verb_in_sentence = False\n",
    "        for token in sent:\n",
    "            if token.lemma_ == verb and token.pos_ == \"VERB\":\n",
    "                verb_in_sentence = True\n",
    "                break\n",
    "        if noun in sent.text.split() and verb_in_sentence:\n",
    "            matching_sentences.append(sent.text)\n",
    "    return matching_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noun = \"he\"\n",
    "verb = \"turn\"\n",
    "\n",
    "matches = []\n",
    "for doc in docs_w:\n",
    "    matching_sentences = find_svpairs(doc, noun, verb)\n",
    "    matches.append(matching_sentences)\n",
    "    \n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "To change display results, the contents of the concordance method are: `(\"word\", window width, lines=#[25, all])`. The `window width` is an integer specifying the number of characters to the left and right of a word to display. The default for `lines` is 25, but it can be set to any integer or to `all` (no quotation marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just uses the NLTK's `concordance` method to find a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Create NLTK texts for concordances\n",
    "words_w = word_tokenize(\" \".join(talks_f.text.tolist()))\n",
    "women = nltk.Text(words_w)\n",
    "\n",
    "words_m = word_tokenize(\" \".join(talks_m.text.tolist()))\n",
    "men = nltk.Text(words_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 50 of 105 matches:\n",
      " degrade under the sun , which we in turn breathe . Green roofs also retain up\n",
      "at the age of reason starts when you turn seven , and then you 're capable of \n",
      "p ? ] [ Woman : Perhaps . We have to turn it into a moral war . ] [ Man : How \n",
      "y to harness this waste resource and turn it into a fuel that would be somethi\n",
      " who were angry , and anger that can turn to violence , and we 're all familia\n",
      "ithecine would have walked . As they turn toward us you 'll see that the pelvi\n",
      " or not . And so therefore , this in turn meant that there was an extremely in\n",
      "ts of tweaking and torquing , and we turn our puffer into the Mola . You know \n",
      " words make that difficult left-hand turn into the dictionary , and keep the b\n",
      "e 's another male on top waiting his turn . Often the queens mate more than on\n",
      "t with a bit of sand , put it down , turn around , and go back in . And finall\n",
      "oing to be the tool that 's going to turn our usage overnight . And what kind \n",
      "hat follows you around if you do n't turn the thing off . ( Laughter ) Maybe w\n",
      "at if I could control that sound and turn it into an instrument , because ther\n",
      "ll around us now , is that the temes turn us into teme machines , with these i\n",
      " glad all of the while . ♫ ♫ I could turn the gray skies to blue , if I had yo\n",
      "hat later , OK . So for now , let 's turn to the evidence for dark matter . In\n",
      "n dark matter . OK . So now , let 's turn to dark energy . So to understand th\n",
      "rawling along , some guy 's going to turn on the brake lights just as you pull\n",
      "rchy , and it works . ( Laughter ) I turn around , but it 's not a child . It \n",
      "sper , `` God , I miss her . '' They turn then , shoulder to shoulder , and wa\n",
      "s the luck of their DNA draw . And I turn to Esther , who 's rocking on those \n",
      "his children , and their children in turn . But it was too late . Despite all \n",
      "ic . And over the years , around the turn of the 20th century , it started to \n",
      "n a Tuesday as a no-brainer and then turn up on Wednesday and look for troop c\n",
      "u have to make the decision . Do you turn off the international spigot of life\n",
      " person , the person we most need to turn to now , this shepherd , at a time w\n",
      "dow , you see the horizon . I always turn my head on the side , and that simpl\n",
      "e 's still time , but not a lot , to turn things around . But business as usua\n",
      "hat molecule and all of the bacteria turn on light in synchrony . That 's how \n",
      "to the cells that tells the cells to turn on this collective behavior of makin\n",
      "hey recognize those words , and they turn on group behaviors that are only suc\n",
      "any given population . Then again we turn to chemistry , and we figured out wh\n",
      " of approaching agency and authority turn inside out to reflect the reality th\n",
      "t of the public good . Students , in turn , continuously move outside the clas\n",
      "at will happen inside . We intend to turn the intellectual and imaginative pow\n",
      "ed in such abundance here , needs to turn its attention to that collaborative \n",
      "into alternate selves that tune in , turn on , drop out thoughts , perceptions\n",
      "ed , I decided it was a good time to turn my back on this work . And about 20 \n",
      ", suggesting his friends and fans to turn off their electricity every Sunday f\n",
      "ntinuum of traits . When does a nerd turn into Asperger , which is just mild a\n",
      "orm , '' and I can hold it there and turn them into videos . Now , visual thin\n",
      "ke you ? Really ? The goal is not to turn kids into your kind of adult , but r\n",
      "riend or someone that you hope might turn into one of those things , we have t\n",
      "own a little . ( Laughter ) It 's my turn . I still have two minutes . ( Laugh\n",
      "in the position of hot even when you turn it on , and then you start to use mo\n",
      "n , dependence , etc. ? Or maybe you turn to the past . After all , we have sp\n",
      "ically , frogs lay eggs and the eggs turn into tadpoles , and tadpoles turn in\n",
      "gs turn into tadpoles , and tadpoles turn into frogs . '' And I said , `` Yeah\n",
      "oing to take something from that and turn it into something else , because ins\n"
     ]
    }
   ],
   "source": [
    "women.concordance('turn', lines=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 50 of 388 matches:\n",
      "ita instead of family income , and I turn these individual data into regional \n",
      " sort of `` shotgun flexibility '' — turn your head this way ; shoot ; and you\n",
      "rpose Driven Life . '' And I want to turn now , briefly , to talk about that b\n",
      "XML is going to do is it 's going to turn those pages into Lego blocks . XML a\n",
      "ng to aspire to . It 's to make them turn their back on what they think they l\n",
      " in Pennsylvania , and I took a left turn trying to get back to the highway . \n",
      "an appropriate part of the brain can turn off a brain disorder . And consider \n",
      "n actually change the world . We can turn the inevitable outcomes , and transf\n",
      "f years ago . We 'd like to actually turn that program off . They tried that i\n",
      "ersonalization . So , with that , in turn , 20 million dollars today does this\n",
      "hin client computers . And then , in turn , businesses started to grow , like \n",
      "had to use it , channel its energy , turn it into something that would clarify\n",
      "y moment , `` OK , they 're going to turn the page , and I 'm going to see the\n",
      "tion storage mechanisms , it may not turn out to be chips . It may turn out to\n",
      "ay not turn out to be chips . It may turn out to be something that looks a lit\n",
      "trative offices . And then that , in turn , propelled us to look at larger-sca\n",
      "? And at what point did that uniform turn from white to black ? Water has been\n",
      ". That 's what you did . You did n't turn it off . And as I said before , they\n",
      "he word `` God , '' many people will turn off immediately . And most people , \n",
      "ng back . It 's coming back . Let me turn the trails on , so you can see that \n",
      "guages . Now you can also see , if I turn this around here — hopefully I wo n'\n",
      " , some of them will really work and turn out , such as News . But then we had\n",
      "o wait a long time for projectors to turn on and off , and they 're noisy , so\n",
      " display in the mirror , and you can turn around — but there 's a three second\n",
      "f a minute probably are n't going to turn . ( Laughter ) We can also do intell\n",
      " limit . I do n't know how this will turn out . I know a lot of people who inv\n",
      " let me just grab these guys — I can turn things into piles instead of just th\n",
      "The freshness , the capacity . I can turn this image . I can look at it from m\n",
      "stics is Europe . It was only by the turn of the century that more than 90 per\n",
      "ground ? How do you take a robot and turn it into a field microbiologist ? The\n",
      " constellations of today . And I can turn them all on , and you can see them m\n",
      "blessing , we decided on the spot to turn it into a film , because we felt tha\n",
      "er , where it can feel obstacles and turn around . You see , it 's going the o\n",
      "whole animal is fixed . The wind may turn , but the animal will always turn it\n",
      "ay turn , but the animal will always turn its nose into the wind . Now , anoth\n",
      " like it said in the video , you can turn it left or right just by putting the\n",
      " oops , that was going to be a right turn . ( Laughter ) Okay , this one will \n",
      "ter ) Okay , this one will be a left turn . Here , but — ( Applause ) — anyway\n",
      " know , the status quo and trying to turn it upside down . So I 've seen life \n",
      "s too much really . CA : You did n't turn out too bad , I have to say , I 'm .\n",
      "out most , care most about , want to turn your resources toward ? RB : Well , \n",
      "'re walking out of a space , and you turn around , and quite often you tap you\n",
      "eir bags tap their pockets . And you turn around , and you look back into the \n",
      "ough we seek happiness , it seems we turn our back to it . Although we want to\n",
      "all of space . Does n't matter if we turn the whole of space around by some an\n",
      "ll being saved as the vehicle stocks turn over , at an average cost of only 12\n",
      "ng DARPAnet into the Internet . Just turn it over to the private sector , and \n",
      " imports going forever up , they can turn down with the 12 dollars a barrel ef\n",
      "irst of all ? Audience : 1966 . 66 — turn to the calendar with 1966 . And what\n",
      "ok in front of you , do me a favor , turn to a year outside of the 1900s , eit\n"
     ]
    }
   ],
   "source": [
    "men.concordance('turn', lines=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attempts to match on the noun and verb but it just treats them as two words in a sentence. \n",
    "\n",
    "WARNING: Tense and conjugation matter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')  # Download the necessary tokenizer data\n",
    "\n",
    "def find_sentences(text, noun, verb):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    matching_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if (noun in sentence.split()) and (verb in sentence.split()):\n",
    "            matching_sentences.append(sentence)\n",
    "    return matching_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_he_turn = find_sentences(texts_w, \"he\", \"turn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['and there\\'d be that sound in the signal — it\\'s like (screeching) — and he thought, \"oh, what if i could control that sound and turn it into an instrument, because there are pitches in it.\"']\n"
     ]
    }
   ],
   "source": [
    "print(len(w_he_turn))\n",
    "print(w_he_turn[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_i_think = find_sentences(texts_m, \"i\", \"think\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1914\n",
      "[\"i don't think you understand.\", 'i think your phone lines are unmanned.', 'and the truth is, for years i was a little depressed, because americans obviously did not value it, because the mac had three percent market share, windows had 95 percent market share — people did not think it was worth putting a price on it.', 'so i have a big interest in education, and i think we all do.', \"and she's exceptional, but i think she's not, so to speak, exceptional in the whole of childhood.\", 'we were sitting there and i think they just went out of sequence, because we talked to the little boy afterward and we said, \"you ok with that?\"', 'i think this is rather important.', 'i think math is very important, but so is dance.', \"i think you'd have to conclude, if you look at the output, who really succeeds by this, who does everything that they should, who gets all the brownie points, who are the winners — i think you'd have to conclude the whole purpose of public education throughout the world is to produce university professors.\", \"and i think we can't afford to go on that way.\", \"i think now they'd say she had adhd.\", '(applause)    what i think it comes to is this: al gore spoke the other night about ecology and the revolution that was triggered by rachel carson.', 'because my students, what they said when they looked upon the world, and i asked them, \"what do you really think about the world?\"', \"if we don't look in the data, i think we all underestimate the tremendous change in asia, which was in social change before we saw the economical change.\", \"(laughter)    but i think it's very important to have all this information.\", \"often that's what people think i do, and it's the furthest thing from it.\", \"and i think what's important is, when we first unveiled the building, the public saw it as being totally about our whim and ego.\", \"but let's see if i can very quickly get into the book spiral, because i think it's, as i said, the most — this is the main reading room — the most unique part of the building.\", \"not in my wildest dreams did i think — i don't even consider myself to be an author.\", \"and i think it's because spiritual emptiness is a universal disease.\"]\n"
     ]
    }
   ],
   "source": [
    "print(len(m_i_think))\n",
    "print(m_i_think[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually addresses the noun and verb as parts of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')  # Download the necessary tokenizer data\n",
    "# nltk.download('averaged_perceptron_tagger')  # Download \n",
    "\n",
    "def find_sents(text, noun, verb):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    matching_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        verb_in_sentence = False\n",
    "        for word, tag in pos_tags:\n",
    "            if word == verb and tag.startswith('VB'):\n",
    "                verb_in_sentence = True\n",
    "                break\n",
    "        if noun in sentence.split() and verb_in_sentence:\n",
    "            matching_sentences.append(sentence)\n",
    "    return matching_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"because she can't believe i can't eat this penguin.\",\n",
       " 'so she asked, \"eat what?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sents(texts_m, \"she\", \"eat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVO\n",
    "\n",
    "While this was at first appealing because of the simplicity and accuracy, since we would be loading the SVOs themselves, the resulting contexts were too impoverished to be much use for hand inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "\n",
    "# LOAD DATAFRAMES\n",
    "# the `lem` suffix indicates the verbs have been lemmatized\n",
    "svos_m = pd.read_csv(\"../output/svos_m_lem.csv\", index_col=0)\n",
    "svos_w = pd.read_csv(\"../output/svos_w_lem.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12790</th>\n",
       "      <td>834</td>\n",
       "      <td>he</td>\n",
       "      <td>turn</td>\n",
       "      <td>[party, invitations]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14539</th>\n",
       "      <td>852</td>\n",
       "      <td>he</td>\n",
       "      <td>turn</td>\n",
       "      <td>[flashlight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16020</th>\n",
       "      <td>868</td>\n",
       "      <td>he</td>\n",
       "      <td>turn</td>\n",
       "      <td>[sustainability, project]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18894</th>\n",
       "      <td>898</td>\n",
       "      <td>he</td>\n",
       "      <td>turn</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18904</th>\n",
       "      <td>898</td>\n",
       "      <td>he</td>\n",
       "      <td>turn</td>\n",
       "      <td>[11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26071</th>\n",
       "      <td>973</td>\n",
       "      <td>he</td>\n",
       "      <td>turn</td>\n",
       "      <td>[you]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc subject  verb                     object\n",
       "12790  834      he  turn       [party, invitations]\n",
       "14539  852      he  turn               [flashlight]\n",
       "16020  868      he  turn  [sustainability, project]\n",
       "18894  898      he  turn                       [18]\n",
       "18904  898      he  turn                       [11]\n",
       "26071  973      he  turn                      [you]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svos_w.query(' subject==\"he\" & verb==\"turn\" ')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
