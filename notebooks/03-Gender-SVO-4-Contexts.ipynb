{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVO Contexts\n",
    "\n",
    "This notebook is intended primarily to \"hand\" inspect the contexts for results in other explorations. We are going to try two approaches:\n",
    "\n",
    "- using the SVOs (This turns out to be rather underwhelming.)\n",
    "- using the NLTK's concordance method\n",
    "\n",
    "In the *SVO as Context* below, we end up loading only the mens subcorpus to explore how useful the complete SVOs would be. We don't take this exploration any further.\n",
    "\n",
    "Afterwards, we turn to the texts themselves as context, loading the two gendered dataframes and then collecting the texts from each. We process the two lists of texts with spacy's NLP pipeline to produce featureful documents which make it easy to get the lemmas out. Using the spacy lemmatization parallels our usage in building the SVOs, so we are going to get the same results and thus enable us to explore the two subcorpora much more effectively. \n",
    "\n",
    "Finally, we take the texts as strings of lemmas and create a single NLTK Text, which allows us to use the NLTK's concordance functionality to see words in context. To do this, we create a function that takes spacy doc, lemmatizes the words within it, build a list of lemmas, compiles those lists into a list for the subcorpus, flattens that subcorpus list into a single, very long, string of tokens, from which an NLTK Text is created. We create two NLTK Texts: one for female speakers, `women`, and one for male speakers, `men`. \n",
    "\n",
    "**TO DO**: Find a way to save either the list of spacy docs or the NLTK Text. The spacy NLP pipeline takes time to run, and we shouldn't need to create space docs or NLTK Texts every time we want to use this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVOs as Context\n",
    "\n",
    "While this was at first appealing because of the simplicity and accuracy, since we would be loading the SVOs themselves, the resulting contexts were too impoverished to be much use for hand inspection. Only the men's subcorpus is loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re\n",
    "with open('../output/svos_m_lem.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    # Drop the first row\n",
    "    next(reader)\n",
    "    # Skip the first column\n",
    "    contexts_m = [row[1:4] for row in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in **contexts** is a list of three strings. The list comprehension below first joins all the items in each line into a single string, it then replaces any square brackets, that sometimes occur in the third item on each line, with nothing. It does this for all the lines in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i blow conference', 'i want \"to', 'i need that', 'laughter put yourselves', 'i fly two', 'i have \"to', 'i tell story', 'i leave \"the', 'i look me', 'it hit me']\n"
     ]
    }
   ],
   "source": [
    "sentences = [re.sub(\"[\\([{})\\]]\", \"\", ' '.join(item)) for item in contexts_m]\n",
    "print(sentences[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuller Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import nltk, pandas as pd, re, spacy\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# wnl = WordNetLemmatizer()\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "\n",
    "# And then grabbing the texts of the talks:\n",
    "texts_w = [text.lower() for text in talks_f.text.tolist()]\n",
    "texts_m = [text.lower() for text in talks_m.text.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below takes the most time to run. Finding a way to save either the spacy docs or the resulting NLTK text object so that we could retrieve them for exploration would be really useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Use the pipe method to feed documents \n",
    "docs_w = list(nlp.pipe(texts_w))\n",
    "docs_m = list(nlp.pipe(texts_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualize(spacy_docs):\n",
    "    '''contextualize takes a list of spaCy docs\n",
    "       and converts them to a single NLTK text'''\n",
    "    all_lemmas = []\n",
    "    # Grab the lemmas from each of the documents\n",
    "    # and append to a list of all the lemmas\n",
    "    for doc in spacy_docs:\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        all_lemmas.append(lemmas)\n",
    "    # all_lemmas is a list of lists that needs to be flattened\n",
    "    flattened = [item for sublist in all_lemmas for item in sublist]\n",
    "    # all our texts are now one long list of words to be fed into NLTK Text\n",
    "    contextualized = nltk.Text(flattened)\n",
    "    return contextualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "women = contextualize(docs_w)\n",
    "men = contextualize(docs_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Load NLTK Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump(women, file = open(\"contexts_w.pickle\", \"wb\"))\n",
    "dill.dump(men, file = open(\"contexts_m.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pickup where we left off without having to re-run everything:\n",
    "import dill, nltk\n",
    "women = dill.load(open(\"contexts_w.pickle\", \"rb\"))\n",
    "men = dill.load(open(\"contexts_m.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words in Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 579 matches:\n",
      "nd luckily for I , that home and the love inside of it , along with help from \n",
      "hat ruth build by destroy two well - love community park . now , we 'll have e\n",
      "they say , \" do you believe that god love you with all his heart ? \" and I thi\n",
      "k in my head : do I believe that god love I with all his heart ? because I be \n",
      " have ask I , \" do you feel that god love you with all his heart ? \" well , th\n",
      "feel it all the time . I feel god 's love when I be hurt and confuse , and I f\n",
      " care for . I take shelter in god 's love when I do not understand why tragedy\n",
      " why tragedy hit , and I feel god 's love when I look with gratitude at all th\n",
      " it bring out the complexity of this love - hate relationship that the arab wo\n",
      "er )     ( applause )     design — I love its design . I remember when I be li\n",
      "e same model .     entertainment — I love the entertainment . but actually , t\n",
      "go , you could not stop they . woman love to talk about their vagina , they do\n",
      ", and great sex life , and how woman love their vagina . \" but in fact , that \n",
      "ave meet a new specie . and I really love hear about all these specie at the b\n",
      "nt to start with my work on romantic love , because that be my most recent wor\n",
      "o be put 32 people , who be madly in love , into a functional mri brain scanne\n",
      "i brain scanner . 17 who be madly in love and their love be accept ; and 15 wh\n",
      " . 17 who be madly in love and their love be accept ; and 15 who be madly in l\n",
      "e be accept ; and 15 who be madly in love and they have just be dump . and so \n",
      " , and then go on into where I think love be go .     ( laughter )     \" what \n",
      "    ( laughter )     \" what ' tis to love ? \" shakespeare say . I think our an\n",
      "t by try to figure out what romantic love be by look at the last 45 year of th\n",
      "f thing that happen when you fall in love . the first thing that happen be , a\n",
      " bernard shaw say it differently . \" love consist of overestimate the differen\n",
      " on what you do . as chaucer say , \" love be blind . \"     in try to understan\n"
     ]
    }
   ],
   "source": [
    "women.concordance('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 798 matches:\n",
      "ou who be good at branding , I would love to get your advice and help on how t\n",
      " a technology nut , and I absolutely love it . the job , though , come with on\n",
      "   I hear a great story recently — I love tell it — of a little girl who be in\n",
      "not want to come to los angeles . he love it , but he have a girlfriend in eng\n",
      " girlfriend in england . this be the love of his life , sarah . he would know \n",
      "ut the good computer . you give they love , joy , be there to comfort they . t\n",
      "the rest of their life with all this love , education , money and background g\n",
      "ere to go to work , and you meet the love of your life there , a career decisi\n",
      "ople , to build , create something , love someone . there be the fuel you pick\n",
      "need surprise . how many of you here love surprise ? say , \" aye . \"     audie\n",
      "what we really need : connection and love , fourth need . we all want it ; mos\n",
      "nt it ; most settle for connection , love 's too scary . who here have be hurt\n",
      " money or friend you have , how many love you , you feel like hell . and I bel\n",
      "ve . be you drive by significance or love ? we all need all six , but what you\n",
      "map — I can not use google because I love mac , and they have not make it good\n",
      "ade center where he work , say , \" I love you , I want you to know I want to m\n",
      " now on , every day , give you all , love you all . do not let anything ever s\n",
      "m pakistan , I be a muslim . I would love to hold your hand and say I be sorry\n",
      "er )     ok ? I just , this be why I love what michael do , because it be like\n",
      "      it be wonderful to be back . I love this wonderful gathering . and you m\n",
      " come in .     this one I personally love , because the idea be that architect\n",
      "s be a serious solution . this one I love . the idea be it be not just a clini\n",
      "pyright violation ?     can : no . I love it .     ( laughter )     lb : well \n",
      "do not have electricity . the parent love it , because when they open up the l\n",
      "ple of the kind of research I really love : all this compute power to make thi\n"
     ]
    }
   ],
   "source": [
    "men.concordance(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change display results, the contents of the concordance method are: `(\"word\", window width, lines=#[25, all])`. The `window width` is an integer specifying the number of characters to the left and right of a word to display. The default for `lines` is 25, but it can be set to any integer or to `all` (no quotation marks)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
