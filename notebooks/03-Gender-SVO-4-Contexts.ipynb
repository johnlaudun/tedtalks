{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVO Contexts\n",
    "\n",
    "This notebook is intended primarily to \"hand\" inspect the contexts for results in other explorations. We are going to try two approaches:\n",
    "\n",
    "- using the SVOs (This turns out to be rather underwhelming.)\n",
    "- using the NLTK's concordance method\n",
    "\n",
    "In the *SVO as Context* below, we end up loading only the mens subcorpus to explore how useful the complete SVOs would be. We don't take this exploration any further.\n",
    "\n",
    "Afterwards, we turn to the texts themselves as context, loading the two gendered dataframes and then collecting the texts from each. We process the two lists of texts with spacy's NLP pipeline to produce featureful documents which make it easy to get the lemmas out. Using the spacy lemmatization parallels our usage in building the SVOs, so we are going to get the same results and thus enable us to explore the two subcorpora much more effectively. \n",
    "\n",
    "Finally, we take the texts as strings of lemmas and create a single NLTK Text, which allows us to use the NLTK's concordance functionality to see words in context. To do this, we create a function that takes spacy doc, lemmatizes the words within it, build a list of lemmas, compiles those lists into a list for the subcorpus, flattens that subcorpus list into a single, very long, string of tokens, from which an NLTK Text is created. We create two NLTK Texts: one for female speakers, `women`, and one for male speakers, `men`. \n",
    "\n",
    "**TO DO**: Find a way to save either the list of spacy docs or the NLTK Text. The spacy NLP pipeline takes time to run, and we shouldn't need to create space docs or NLTK Texts every time we want to use this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVOs as Context\n",
    "\n",
    "While this was at first appealing because of the simplicity and accuracy, since we would be loading the SVOs themselves, the resulting contexts were too impoverished to be much use for hand inspection. Only the men's subcorpus is loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re\n",
    "with open('../output/svos_m_lem.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    # Drop the first row\n",
    "    next(reader)\n",
    "    # Skip the first column\n",
    "    contexts_m = [row[1:4] for row in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in **contexts** is a list of three strings. The list comprehension below first joins all the items in each line into a single string, it then replaces any square brackets, that sometimes occur in the third item on each line, with nothing. It does this for all the lines in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i blow conference', 'i want \"to', 'i need that', 'laughter put yourselves', 'i fly two', 'i have \"to', 'i tell story', 'i leave \"the', 'i look me', 'it hit me']\n"
     ]
    }
   ],
   "source": [
    "sentences = [re.sub(\"[\\([{})\\]]\", \"\", ' '.join(item)) for item in contexts_m]\n",
    "print(sentences[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuller Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import nltk, pandas as pd, re, spacy\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# wnl = WordNetLemmatizer()\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "\n",
    "# And then grabbing the texts of the talks:\n",
    "texts_w = [text.lower() for text in talks_f.text.tolist()]\n",
    "texts_m = [text.lower() for text in talks_m.text.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below takes the most time to run. Finding a way to save either the spacy docs or the resulting NLTK text object so that we could retrieve them for exploration would be really useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Use the pipe method to feed documents \n",
    "docs_w = list(nlp.pipe(texts_w))\n",
    "docs_m = list(nlp.pipe(texts_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualize(spacy_docs):\n",
    "    '''contextualize takes a list of spaCy docs\n",
    "       and converts them to a single NLTK text'''\n",
    "    all_lemmas = []\n",
    "    # Grab the lemmas from each of the documents\n",
    "    # and append to a list of all the lemmas\n",
    "    for doc in spacy_docs:\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        all_lemmas.append(lemmas)\n",
    "    # all_lemmas is a list of lists that needs to be flattened\n",
    "    flattened = [item for sublist in all_lemmas for item in sublist]\n",
    "    # all our texts are now one long list of words to be fed into NLTK Text\n",
    "    contextualized = nltk.Text(flattened)\n",
    "    return contextualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "women = contextualize(docs_w)\n",
    "men = contextualize(docs_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Load NLTK Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump(women, file = open(\"contexts_w.pickle\", \"wb\"))\n",
    "dill.dump(men, file = open(\"contexts_m.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pickup where we left off without having to re-run everything:\n",
    "import dill, nltk\n",
    "women = dill.load(open(\"contexts_w.pickle\", \"rb\"))\n",
    "men = dill.load(open(\"contexts_m.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words in Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 49 of 49 matches:\n",
      "l lamanite . but apparently somebody blow it , because the lamanite be able to\n",
      "st .     in the united states , I be blow away by the motivation , the positiv\n",
      "be amazing . I 've watch it and I be blow away by it and its potential to brin\n",
      " by use both hand and aim his glance blow , he can make much large , sharp fla\n",
      "wind ♫     ♫ from the island ♫     ♫ blow a storm cloud ♫     ♫ across the new\n",
      "7 , where I meet a man who be try to blow up the math building of nyu . and I \n",
      "eep write bad poetry , and he do not blow up the math building , but he go to \n",
      " happy to pay it , because for every blow that I receive , I be able to delive\n",
      "ou .     sw : that be good . can you blow they a kiss ?     e : [ kiss noise ]\n",
      "ira de mello who , as chris say , be blow up in iraq in 2003 . he be the victi\n",
      " and down at atlanta , and I be just blow away by the fact that just a year ag\n",
      "te from where I live , and I be just blow away , like , it be just amazing , a\n",
      " be chair of the sculpture and glass blow department at rhode island school of\n",
      "ve this before the symptom be full - blow ?     lg : right . so I have for a l\n",
      "eport that we would make this mind - blow discovery — this once - in - a - lif\n",
      " limitation of how hard it can be to blow glass into certain shape , they just\n",
      "or child , so that we can grow up to blow you away .     ( laughter )     adul\n",
      "thing ♫     ♫ which race     sideway blow ♫     ♫ blow ♫     ♫ blow ♫     ♫ ma\n",
      " which race     sideway blow ♫     ♫ blow ♫     ♫ blow ♫     ♫ may come home  \n",
      "   sideway blow ♫     ♫ blow ♫     ♫ blow ♫     ♫ may come home     with a smo\n",
      "anist , the artist ... everyone that blow my mind this week . thank you . ( ap\n",
      "m .     this boy be call zenola . he blow himself up , kill six . this boy be \n",
      "ry to hold onto other while thing be blow up around you , know that while you \n",
      " and it anchor itself in the sand by blow up a balloon on the end of its stock\n",
      "of sound . six of eight atlas rocket blow up on the pad . after 11 complete mi\n",
      "can sit atop a dandelion puff and be blow away with a wisp of air — so light t\n",
      "ce when you perspire . it completely blow apart the way that perfume be , and \n",
      "ey , no picture ! \" and it be such a blow that it wake I up , in the way that \n",
      " crystallization , probably from the blow , because it be an insult in a way .\n",
      "l sweet spot , and we be sit there , blow it . that be when I realize that thi\n",
      "ience . there be a warm , moist wind blow at your back and the smell of the ea\n",
      " do not know . the wind be massively blow , and the only way I could get to th\n",
      " to try different mixture , and I be blow away by how tiny change in dosage dr\n",
      " of a man in his early 20 , and I be blow away by how my thought change . ( la\n",
      "trong wind from the east now . it be blow about . it be blow ! after 12 day of\n",
      " east now . it be blow about . it be blow ! after 12 day of storm I get to row\n",
      "r - old kid , but she say thing that blow people away , well express by perhap\n",
      "he top of her 90 - foot mast . we be blow on our side in the southern ocean . \n",
      "nd the spray from the break crest be blow horizontally like snow in a blizzard\n",
      "ng show tune . we play with ball . I blow bubble and they pop they . and I fee\n",
      "nd still might not pan out , or just blow stuff up ? you 'll probably choose t\n",
      "have destroy temple at palmyra . who blow up a temple ? they 've destroy the t\n",
      "be ablaze , it do not do any good to blow out the match . you have to blow out\n",
      " to blow out the match . you have to blow out the match before the forest catc\n",
      "mer 's , even if you have the full - blow disease pathology ablaze in your bra\n",
      "woman but I recognize she . and so I blow she a kiss because it be sybrina ful\n",
      "p and greet your maker , for gabriel blow his horn .     weary totin ' such a \n",
      "p and greet your maker , for gabriel blow his horn .     weary totin ' such a \n",
      " selfie .     it will never cease to blow my mind that we can see our planet f\n"
     ]
    }
   ],
   "source": [
    "women.concordance('blow', lines=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 798 matches:\n",
      "ou who be good at branding , I would love to get your advice and help on how t\n",
      " a technology nut , and I absolutely love it . the job , though , come with on\n",
      "   I hear a great story recently — I love tell it — of a little girl who be in\n",
      "not want to come to los angeles . he love it , but he have a girlfriend in eng\n",
      " girlfriend in england . this be the love of his life , sarah . he would know \n",
      "ut the good computer . you give they love , joy , be there to comfort they . t\n",
      "the rest of their life with all this love , education , money and background g\n",
      "ere to go to work , and you meet the love of your life there , a career decisi\n",
      "ople , to build , create something , love someone . there be the fuel you pick\n",
      "need surprise . how many of you here love surprise ? say , \" aye . \"     audie\n",
      "what we really need : connection and love , fourth need . we all want it ; mos\n",
      "nt it ; most settle for connection , love 's too scary . who here have be hurt\n",
      " money or friend you have , how many love you , you feel like hell . and I bel\n",
      "ve . be you drive by significance or love ? we all need all six , but what you\n",
      "map — I can not use google because I love mac , and they have not make it good\n",
      "ade center where he work , say , \" I love you , I want you to know I want to m\n",
      " now on , every day , give you all , love you all . do not let anything ever s\n",
      "m pakistan , I be a muslim . I would love to hold your hand and say I be sorry\n",
      "er )     ok ? I just , this be why I love what michael do , because it be like\n",
      "      it be wonderful to be back . I love this wonderful gathering . and you m\n",
      " come in .     this one I personally love , because the idea be that architect\n",
      "s be a serious solution . this one I love . the idea be it be not just a clini\n",
      "pyright violation ?     can : no . I love it .     ( laughter )     lb : well \n",
      "do not have electricity . the parent love it , because when they open up the l\n",
      "ple of the kind of research I really love : all this compute power to make thi\n"
     ]
    }
   ],
   "source": [
    "men.concordance(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change display results, the contents of the concordance method are: `(\"word\", window width, lines=#[25, all])`. The `window width` is an integer specifying the number of characters to the left and right of a word to display. The default for `lines` is 25, but it can be set to any integer or to `all` (no quotation marks)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
