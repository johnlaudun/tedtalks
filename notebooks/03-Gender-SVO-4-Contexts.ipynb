{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVO Contexts\n",
    "\n",
    "This notebook is intended primarily to \"hand\" inspect the contexts for results in other explorations. We are going to try two approaches:\n",
    "\n",
    "- using the SVOs (This turns out to be rather underwhelming.)\n",
    "- using the NLTK's concordance method\n",
    "\n",
    "In the *SVO as Context* below, we end up loading only the mens subcorpus to explore how useful the complete SVOs would be. We don't take this exploration any further.\n",
    "\n",
    "Afterwards, we turn to the texts themselves as context, loading the two gendered dataframes and then collecting the texts from each. We process the two lists of texts with spacy's NLP pipeline to produce featureful documents which make it easy to get the lemmas out. Using the spacy lemmatization parallels our usage in building the SVOs, so we are going to get the same results and thus enable us to explore the two subcorpora much more effectively. \n",
    "\n",
    "Finally, we take the texts as strings of lemmas and create a single NLTK Text, which allows us to use the NLTK's concordance functionality to see words in context. To do this, we create a function that takes spacy doc, lemmatizes the words within it, build a list of lemmas, compiles those lists into a list for the subcorpus, flattens that subcorpus list into a single, very long, string of tokens, from which an NLTK Text is created. We create two NLTK Texts: one for female speakers, `women`, and one for male speakers, `men`. \n",
    "\n",
    "**TO DO**: Find a way to save either the list of spacy docs or the NLTK Text. The spacy NLP pipeline takes time to run, and we shouldn't need to create space docs or NLTK Texts every time we want to use this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVOs as Context\n",
    "\n",
    "While this was at first appealing because of the simplicity and accuracy, since we would be loading the SVOs themselves, the resulting contexts were too impoverished to be much use for hand inspection. Only the men's subcorpus is loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re\n",
    "with open('../output/svos_m_lem.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    # Drop the first row\n",
    "    next(reader)\n",
    "    # Skip the first column\n",
    "    contexts_m = [row[1:4] for row in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in **contexts** is a list of three strings. The list comprehension below first joins all the items in each line into a single string, it then replaces any square brackets, that sometimes occur in the third item on each line, with nothing. It does this for all the lines in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i blow conference', 'i want \"to', 'i need that', 'laughter put yourselves', 'i fly two', 'i have \"to', 'i tell story', 'i leave \"the', 'i look me', 'it hit me']\n"
     ]
    }
   ],
   "source": [
    "sentences = [re.sub(\"[\\([{})\\]]\", \"\", ' '.join(item)) for item in contexts_m]\n",
    "print(sentences[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuller Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import nltk, pandas as pd, re, spacy\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# wnl = WordNetLemmatizer()\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "\n",
    "# And then grabbing the texts of the talks:\n",
    "texts_w = [text.lower() for text in talks_f.text.tolist()]\n",
    "texts_m = [text.lower() for text in talks_m.text.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below takes the most time to run. Finding a way to save either the spacy docs or the resulting NLTK text object so that we could retrieve them for exploration would be really useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Use the pipe method to feed documents \n",
    "docs_w = list(nlp.pipe(texts_w))\n",
    "docs_m = list(nlp.pipe(texts_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualize(spacy_docs):\n",
    "    '''contextualize takes a list of spaCy docs\n",
    "       and converts them to a single NLTK text'''\n",
    "    all_lemmas = []\n",
    "    # Grab the lemmas from each of the documents\n",
    "    # and append to a list of all the lemmas\n",
    "    for doc in spacy_docs:\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        all_lemmas.append(lemmas)\n",
    "    # all_lemmas is a list of lists that needs to be flattened\n",
    "    flattened = [item for sublist in all_lemmas for item in sublist]\n",
    "    # all our texts are now one long list of words to be fed into NLTK Text\n",
    "    contextualized = nltk.Text(flattened)\n",
    "    return contextualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "women = contextualize(docs_w)\n",
    "men = contextualize(docs_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Load NLTK Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump(women, file = open(\"contexts_w.pickle\", \"wb\"))\n",
    "dill.dump(men, file = open(\"contexts_m.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pickup where we left off without having to re-run everything:\n",
    "import dill, nltk\n",
    "women = dill.load(open(\"contexts_w.pickle\", \"rb\"))\n",
    "men = dill.load(open(\"contexts_m.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words in Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 1737 matches:\n",
      "y much .     ( applause )     that be he press my button back there , which he\n",
      "he press my button back there , which he do all the time .     ( laughter )   \n",
      " south bronx , and a few year later , he marry my mom . at the time , the comm\n",
      " dad be not alone . and as other like he pursue their own version of the ameri\n",
      "uential mayor name enrique peñalosa . he look at the demographic . few bogotan\n",
      "re world . for his brilliant effort , he be nearly impeach . but as people beg\n",
      "the other day after breakfast , I ask he how environmental justice activist be\n",
      "e be a grant program . I do not think he understand that I be not ask for fund\n",
      " I be not ask for funding . I be make he an offer .     ( applause )     what \n",
      "ver the night of christmas eve , like he do for every other family who get to \n",
      " have to loop back to our house after he have go to everybody else 's ? there \n",
      ", who be jolly — but let us face it , he be also very judgmental . so to find \n",
      "e answer be clear : my brother bill . he be six . well , I finally find bill a\n",
      "l playground . it be a saturday , and he be all by himself , just kick a ball \n",
      "inst the side of a wall . I run up to he and say , \" bill ! I just realize tha\n",
      "to and god will not notice it . \" and he say , \" so ? \" and I say , \" so ? so \n",
      "nd I turn to run . I be so angry with he . but when I get to the top of the st\n",
      "tus . and god come to lehi and say to he , \" put your family on a boat and I w\n",
      "ut of here . \" and god do lead they . he lead they to america . I say , \" amer\n",
      "r our sin , on his way up to heaven , he stop by america and visit the nephite\n",
      "he nephite .     ( laughter )     and he tell they that if you all remain tota\n",
      " to survive by hide in the wood . and he make sure this whole story be write d\n",
      "lyphic chisel onto gold plate , which he then bury near palmyra , new york .  \n",
      "old plate right in his backyard , and he also find this magic stone back there\n",
      "find this magic stone back there that he put into his hat and then bury his fa\n"
     ]
    }
   ],
   "source": [
    "women.concordance('he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change display results, the contents of the concordance method are: `(\"word\", window width, lines=#[25, all])`. The `window width` is an integer specifying the number of characters to the left and right of a word to display. The default for `lines` is 25, but it can be set to any integer or to `all` (no quotation marks)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
