{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we load our modules and data, creating a list from the text column so that we can work with subsets of texts, we explore part-of-speech tagging and how to focus on the verbs and their tenses and then count those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../output/TEDall_speakers.csv')\n",
    "texts = df.text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working through POS-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is just so we can pull some sentences from various TED talks in order to test our POS-tagging script as we develop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentience (the_string):\n",
    "    sentences = [\n",
    "            [word.lower() for word in nltk.word_tokenize(sentence)]\n",
    "            for sentence in nltk.sent_tokenize(the_string)\n",
    "        ]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we are going to do is grab two sentences from a talk and use them as our `test`. There has to be a more elegant, or even pythonic, way to do this, but nothing I tried with nested list comprehensions of itertool's `chain` worked. This did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(len(text2[0:2])): #Traversing through the main list\n",
    "    for j in range (len(text2[i])): #Traversing through each sublist\n",
    "        test.append(text2[i][j]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', \"'re\", 'here', 'today', '—', 'and', 'i', \"'m\", 'very', 'happy', 'that', 'you', 'are', '—', 'you', \"'ve\", 'all', 'heard', 'about', 'how', 'sustainable', 'development', 'will', 'save', 'us', 'from', 'ourselves', '.', 'however', ',', 'when', 'we', \"'re\", 'not', 'at', 'ted', ',', 'we', 'are', 'often', 'told', 'that', 'a', 'real', 'sustainability', 'policy', 'agenda', 'is', 'just', 'not', 'feasible', ',', 'especially', 'in', 'large', 'urban', 'areas', 'like', 'new', 'york', 'city', '.']\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('if', 'IN'), ('you', 'PRP'), (\"'re\", 'VBP'), ('here', 'RB'), ('today', 'NN'), ('—', 'NNP'), ('and', 'CC'), ('i', 'JJ'), (\"'m\", 'VBP'), ('very', 'RB'), ('happy', 'JJ'), ('that', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('—', 'JJ'), ('you', 'PRP'), (\"'ve\", 'VBP'), ('all', 'DT'), ('heard', 'NN'), ('about', 'IN'), ('how', 'WRB'), ('sustainable', 'JJ'), ('development', 'NN'), ('will', 'MD'), ('save', 'VB'), ('us', 'PRP'), ('from', 'IN'), ('ourselves', 'PRP'), ('.', '.'), ('however', 'RB'), (',', ','), ('when', 'WRB'), ('we', 'PRP'), (\"'re\", 'VBP'), ('not', 'RB'), ('at', 'IN'), ('ted', 'VBN'), (',', ','), ('we', 'PRP'), ('are', 'VBP'), ('often', 'RB'), ('told', 'VBN'), ('that', 'IN'), ('a', 'DT'), ('real', 'JJ'), ('sustainability', 'NN'), ('policy', 'NN'), ('agenda', 'NN'), ('is', 'VBZ'), ('just', 'RB'), ('not', 'RB'), ('feasible', 'JJ'), (',', ','), ('especially', 'RB'), ('in', 'IN'), ('large', 'JJ'), ('urban', 'JJ'), ('areas', 'NNS'), ('like', 'IN'), ('new', 'JJ'), ('york', 'NN'), ('city', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('molly', 'RB'), ('wanted', 'VBD'), ('to', 'TO'), ('leave', 'VB'), ('.', '.'), ('molly', 'RB'), ('wants', 'VBZ'), ('to', 'TO'), ('leave', 'VB'), ('.', '.'), ('molly', 'RB'), ('will', 'MD'), ('want', 'VB'), ('to', 'TO'), ('leave', 'VB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tenses = '''\n",
    "Molly wanted to leave.\n",
    "Molly wants to leave.\n",
    "Molly will want to leave.\n",
    "'''\n",
    "print(nltk.pos_tag([word.lower() for word in nltk.word_tokenize(tenses)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing in which we are interested in the current moment is the tense of the main verb. NLTK has the following possibilities:\n",
    "\n",
    "\n",
    "| Tag | Object                          | Example |\n",
    "|--:--|:--------------------------------|:--------|\n",
    "| VB  | verb, base form                 | take    |\n",
    "| VBD | verb, past tense                | took    |\n",
    "| VBG | verb, gerund/present participle | taking  |\n",
    "| VBN | verb, past participle           | taken   |\n",
    "| VBP | verb, sing. present, non-3d     | take    |\n",
    "| VBZ | verb, 3rd person sing. present  | takes   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for: present tense verbs (VB, VBP, VBZ) and past tense verbs (VBD). We will ignore the complexities introduced via participles. And we will have to explore how to track future (will) and, possibly, subjunctive tenses (would, could, should). \n",
    "\n",
    "So, what the NLTK POS-tagger returns is a list of tuples with punctuation still in place, so we need to find the tuples with the form `('word', 'tag')` where `tag == ` one of our four possibilities (VB, VBP, VBZ for present tense and VBD for past tense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenser(sentence_words):\n",
    "    fps_length=len(first_singular_words.intersection(sentence_words))\n",
    "    fpp_length=len(first_plural_words.intersection(sentence_words))\n",
    "    sec_length=len(second_words.intersection(sentence_words))\n",
    "    third_length=len(third_person_words.intersection(sentence_words))\n",
    "    \n",
    "\n",
    "    if fps_length > (fpp_length + sec_length + third_length):\n",
    "        perspective = 'first person singular'\n",
    "    elif fpp_length > (fps_length + sec_length + third_length): \n",
    "        perspective = 'first person plural'\n",
    "    elif sec_length > (fps_length + fpp_length + third_length): \n",
    "        perspective = 'second person'\n",
    "    elif third_length > (fps_length + fpp_length + sec_length): \n",
    "        perspective = 'third person'\n",
    "    else:\n",
    "        perspective = 'mixed'\n",
    "    return perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tense(sentences):\n",
    "    \n",
    "    \"\"\" REQUIRES: from collections import Counter\"\"\"\n",
    "        \n",
    "    sents = Counter()\n",
    "    words = Counter()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        perspective = perse_sentence(sentence)\n",
    "        sents[perspective] += 1\n",
    "        words[perspective] +=len(sentence)\n",
    "        \n",
    "    return sents, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parper(text):\n",
    "    \n",
    "        \"\"\" REQUIRES: import nltk \"\"\"\n",
    "        \n",
    "        sentences = [\n",
    "            [word.lower() for word in nltk.word_tokenize(sentence)]\n",
    "            for sentence in nltk.sent_tokenize(text)\n",
    "        ]\n",
    "        \n",
    "        sents, words = count_perspective(sentences)\n",
    "        total = sum(words.values())\n",
    "        \n",
    "        for perspective, count in words.items():\n",
    "            pcent = (count / total) * 100\n",
    "            nsents = sents[perspective]\n",
    "            \n",
    "            print(f\"{pcent:.2f} {perspective} ( {nsents} ) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below simply establishes that gender is not straightforward: there is no guarantee that someone using gendered words, as constructed above, is actually talking about women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parper(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al Gore's text is a bellweather because it sits at `0` in the list index. Here we mix it up with Majora Carter's talk on urban renewal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parper(texts[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
