{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to establish the utility of the NLTK's sentence tokenizer for use in our current work. A particular concern is the possible effect that \"Dr.\" and other abbreviations might have in returning false positives: sentences that are not, in fact sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, the first thing we need to do is load our data. Here I'm using the most recent version, **6d**. (I'm reverting to 6d because I don't need the overall valence for each talk, and it looks like the first talk by Al Gore got dropped in **6e**.)\n",
    "\n",
    "In the code below, `header=0` tells **`pandas`** to use the first row in the CSV as the header row. If you run df.head(), you will see, however, that the index saved to the CSV gets re-imported as an unnamed column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('../data/talks_6d.csv', header = 0)\n",
    "talks = df.text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the talks are now in a list, as usual. I am also going to create a text to test the NLTK tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mrs. Brown loves chocolate. When she heard the news that Donald Trump has been \n",
      "elected president. She ate an entire plate of brownies. She doesn't feel the same way \n",
      "about chocolate.\n"
     ]
    }
   ],
   "source": [
    "test = \"\"\"\n",
    "Mrs. Brown loves chocolate. \n",
    "When she heard the news that Donald Trump has been \n",
    "elected president. She ate an entire plate of brownies. \n",
    "She doesn't feel the same way about chocolate.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ['\\nMrs. Brown loves chocolate.', 'When she heard the news that Donald Trump has been \\nelected president.', 'She ate an entire plate of brownies.', \"She doesn't feel the same way \\nabout chocolate.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "test_sent = tokenize.sent_tokenize(test)\n",
    "print(len(test_sent), test_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The abbreviation does not generate its own sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, when I've done sentiment analysis -- I've used Afinn, TextBlob, and the Indico libraries, I have used the following code. It returns the sentiment values as a list for each text -- a list as long as the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def afinn_sentiment(filename):\n",
    "    from afinn import Afinn\n",
    "    afinn = Afinn()\n",
    "    with open (my_file, \"r\") as myfile:\n",
    "        text = myfile.read().replace('\\n', ' ')   \n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        sentiments = []\n",
    "        for sentence in sentences:\n",
    "            sentsent = afinn.score(sentence)\n",
    "            sentiments.append(sentsent)\n",
    "        return sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the values for the first 5 talks from the dataframe, but which are now sitting in the `talks` list, I would just use a for loop and the version of the function you have. This example just prints the number of sentences for the first five talks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "247\n",
      "250\n",
      "192\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "for talk in talks[0:5]:\n",
    "    print(len(tokenize.sent_tokenize(talk)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
