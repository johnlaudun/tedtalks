{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-Data\" data-toc-modified-id=\"Loading-the-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading the Data</a></span></li><li><span><a href=\"#Parsing-the-Data\" data-toc-modified-id=\"Parsing-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parsing the Data</a></span></li><li><span><a href=\"#Working-with-the-Years\" data-toc-modified-id=\"Working-with-the-Years-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Working with the Years</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends\n",
    "\n",
    "We are going to do some re-importing of texts here by year. The first time around we are going to do the combined dataset and look for overall trends, and then we will follow that up by loading both the `only` and `plus` datasets separately to see if there are any differences worth noting. Our goal here is to see what words trend not only to learn about TED talks as a developing collection of events but it might also be possible to compare the trends glimpsed here against either trends from the BYU corpus or Google Trends itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Working with only the data from the release, we have two data sets, `TEDonly` and `TEDplus` that we have previously merged into `TEDall_speakers` with an additional column indicating from which data set a given talk is taken. The first thing we will do is to load those part of `TEDall` with which we would like to work -- that is, we do not need to load all of a data set. Once we have a set loaded into a dataframe, we can use `df.columns` to see its contents like this:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('../output/TEDall_speakers.csv')\n",
    "df.columns\n",
    "\n",
    "Index(['Set', 'Talk_ID', 'public_url', 'headline', 'description', 'event',\n",
    "       'duration', 'published', 'tags', 'views', 'text', 'speaker_1',\n",
    "       'speaker1_occupation', 'speaker1_introduction', 'speaker1_profile',\n",
    "       'speaker_2', 'speaker2_occupation', 'speaker2_introduction',\n",
    "       'speaker2_profile', 'speaker_3', 'speaker3_occupation',\n",
    "       'speaker3_introduction', 'speaker3_profile', 'speaker_4',\n",
    "       'speaker4_occupation', 'speaker4_introduction', 'speaker4_profile'],\n",
    "      dtype='object')\n",
    "In [6]:\n",
    "```\n",
    "\n",
    "We can do the same for its index:\n",
    "\n",
    "```python\n",
    "df.index\n",
    "\n",
    "RangeIndex(start=0, stop=1747, step=1)\n",
    "```\n",
    "\n",
    "But to get the column names without loading the entire data set, and in so doing save on memory use, we need to do this the old-fashioned way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Set', 'Talk_ID', 'public_url', 'headline', 'description', 'event', 'duration', 'published', 'tags', 'views', 'text', 'speaker_1', 'speaker1_occupation', 'speaker1_introduction', 'speaker1_profile', 'speaker_2', 'speaker2_occupation', 'speaker2_introduction', 'speaker2_profile', 'speaker_3', 'speaker3_occupation', 'speaker3_introduction', 'speaker3_profile', 'speaker_4', 'speaker4_occupation', 'speaker4_introduction', 'speaker4_profile']\n"
     ]
    }
   ],
   "source": [
    "with open('../output/TEDall_speakers.csv') as f:\n",
    "    columns = f.readline().strip().split(\",\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to load only the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../output/TEDall_speakers.csv', \n",
    "                    usecols=[\"public_url\",\"event\", \"published\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Data\n",
    "\n",
    "It looks like we have two places to get a date: the event at which a talk occurred and the date on which the talk was published on the TED website. For now, let's see if we can find a way to work with the date on which the talk first occurred -- the relationship between a talk and its time is complicated by the dates sometimes being years apart, but that is something we can test later: when does a talk reveal any concurrence? \n",
    "\n",
    "To get all the events we can create a list and then compile to a set: \n",
    "```python\n",
    "events = set(df.event.tolist())\n",
    "print(len(events), events)\n",
    "```\n",
    "Or we can do this the **pandas** way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 ['TED2006', 'TED2004', 'TED2005', 'TED2003', 'TED2007', 'TED2002', 'TED2008', 'TED1984', 'TED1990', 'TED1998', 'TED2001', 'TED2009', 'TED2010', 'TED2011', 'TED1994', 'TED2012', 'TED2013', 'TED2014', 'TED2015', 'TED2016', 'TED2017', 'TEDGlobal 2005', 'TEDGlobal 2007', 'TEDGlobal 2009', 'TEDMED 2009', 'TEDGlobal 2010', 'TED Senior Fellows at TEDGlobal 2010', 'TEDWomen 2010', 'TEDMED 2010', 'TEDActive 2011', 'TEDGlobal 2011', 'TEDMED 2011', 'TEDYouth 2011', 'TEDMED 2012', 'TEDGlobal 2012', 'TEDYouth 2012', 'TEDMED 2013', 'TEDGlobal 2013', 'TEDWomen 2013', 'TEDYouth 2013', 'TEDActive 2014', 'TEDMED 2014', 'TEDGlobal 2014', 'TEDYouth 2014', 'TEDWomen 2015', 'TEDGlobalLondon', 'TEDGlobal>London', 'TEDYouth 2015', 'TEDGlobal>Geneva', 'TEDMED 2015', 'TEDActive 2015', 'TEDSummit', 'TEDWomen 2016', 'TEDMED 2016', 'TEDGlobal 2017', 'TEDGlobal>NYC', 'TEDWomen 2017', 'TEDMED 2017']\n"
     ]
    }
   ],
   "source": [
    "events = df.event.unique().tolist()\n",
    "print(len(events), events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The events with years in them look good: we should be able to get the dates out using regex. In fact, let's build that now so that we can then filter out the events with dates in their name and get a list of the events for which we will need to assign a year. (Neither `datetime` nor `isdigit()` will work here. the former expects dates to be more systematic and the latter expects dates to be set apart from words, so **regex** it is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2009', '2015', '2016', '2009', '2008', '2014', '1990', '1994', '2014', '1984', '2012', '2001', '2015', '2012', '2015', '2011', '2002', '2014', '2005', '2017', '2017', '2009', '2006', '2016', '2011', '2014', '2011', '2012', '2005', '2004', '2011', '2007', '2014', '2003', '2007', '2017', '2010', '2015', '2013', '2013', '2010', '2016', '1998', '2010', '2015', '2017', '2013', '2013', '2010', '2011', '2012', '2010', '2013']\n"
     ]
    }
   ],
   "source": [
    "years = re.findall(r'\\d+', str(events))\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a moment, we will use this either to add a label which is only a year or to replace the label for a series with the year -- not in our original dataset, of course. \n",
    "\n",
    "It is probably possible to find the strings without years in them in the set above, but with only 58 items and only a half dozen, at a glance, we can do this by hand: `TEDGlobal>NYC, TEDGlobal>Geneva, TEDGlobal>London, TEDSummit, TEDGlobalLondon.`\n",
    "\n",
    "With that list in hand, let's search the web and find the dates we need.\n",
    "```\n",
    "TEDGlobal>NYC: 2017\n",
    "TEDGlobal>Geneva: 2015\n",
    "TEDGlobal>London: 2015\n",
    "TEDSummit: 2016\n",
    "TEDGlobalLondon: 2015\n",
    "```\n",
    "So now to do a **replace** on those strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replace *in situ*:\n",
    "\n",
    "```python\n",
    "df['event'].replace(\n",
    "    to_replace=['ABC', 'AB'],\n",
    "    value='A',\n",
    "    inplace=True\n",
    ")\n",
    "```\n",
    "\n",
    "To create a new column:\n",
    "\n",
    "```python\n",
    "df['elderly'] = np.where(df['age']>=50, 'yes', 'no')\n",
    "```\n",
    "\n",
    "Ack. The above code works on numbers, but we are working with strings:\n",
    "\n",
    "```python\n",
    "search = []    \n",
    "for values in df['col']:\n",
    "    search.append(re.search(r'\\d+', values).group())\n",
    "\n",
    "df['col1'] = search\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a7074785ca31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'event'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "search = []\n",
    "for event in df['event']:\n",
    "    search.append(re.search(r'\\d+', str(event)).group())\n",
    "    \n",
    "print(search[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm having a hard time with doing this in place in the dataframe itself, so let's try splitting this column off as a list that we will work on and then append it back as a new column with the year. So we are going from event to year.\n",
    "\n",
    "Here's the pseudo-code as I imagine it at this moment:\n",
    "\n",
    "```python\n",
    "for event in events:\n",
    "    if replace produces a four-digit number == True then done\n",
    "    else replace these things as follows:\n",
    "        TEDGlobal>NYC: 2017\n",
    "        TEDGlobal>Geneva: 2015\n",
    "        TEDGlobal>London: 2015\n",
    "        TEDSummit: 2016\n",
    "        TEDGlobalLondon: 2015\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start by re-populating the `events` list with all the values and not simply the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = df.event.tolist()\n",
    "type(events[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the five replacements listed above, that way we can insert the dates and those dates will be kept by the regex we use to filter out letters. To do this, we are going to use a simple dictionary that has the five events that need a date. The small test established the functionality, if not the efficiency, of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015', 'TED2006', 'TED 2007', '2015', '2016']\n"
     ]
    }
   ],
   "source": [
    "test = [\"TEDGlobalLondon\", \"TED2006\", \"TED 2007\", \"TEDGlobalLondon\", \"TEDSummit\"]\n",
    "\n",
    "replacements = {\"TEDGlobal>NYC\": \"2017\",\n",
    "                \"TEDGlobal>Geneva\": \"2015\",\n",
    "                \"TEDGlobal>London\": \"2015\",\n",
    "                \"TEDSummit\": \"2016\",\n",
    "                \"TEDGlobalLondon\": \"2015\"}\n",
    "\n",
    "for key, value in replacements.items():\n",
    "    test = [w.replace(key, value) for w in test]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same dictionary, `replacements` on our master list of events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in replacements.items():\n",
    "    events = [w.replace(key, value) for w in events]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we need to apply our regex to sweep out all the characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2007', '1994', '2011', '2002', '2001', '2009', '2006', '1984', '2016', '2015', '2014', '2005', '2008', '1990', '2013', '2010', '2017', '2003', '2004', '2012', '1998'}\n"
     ]
    }
   ],
   "source": [
    "# First, we need to repopulate the\n",
    "events = re.findall(r'\\d+', str(events))\n",
    "uniques = set(events) # We're working with a list, \n",
    "                      # so we use set here instead of pandas' \"unique\"\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2006', '2006', '2006', '2006', '2006', '2006', '2006', '2006', '2006', '2006', '2006', '2006', '2006', '2006', '2006', '2004', '2006', '2005', '2006', '2006', '2004', '2006', '2004', '2006', '2004', '2004', '2004', '2005', '2005', '2006', '2005', '2005', '2005', '2006', '2005', '2005', '2006', '2005', '2006', '2005', '2003', '2006', '2006', '2005', '2006', '2006', '2007', '2007', '2007', '2002']\n"
     ]
    }
   ],
   "source": [
    "print(events[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it looks like we have the years, and so we'll add this back to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../output/TEDall_years.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
