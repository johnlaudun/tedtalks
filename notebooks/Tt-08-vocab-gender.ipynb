{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gendered Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we want to examine the potential preferred usage of certain words by male/female speakers. We will not begin with filtering out stop words, both because they will account for a relatively small percentage of words but also because there may be something interesting there. \n",
    "\n",
    "To start, we will need to:\n",
    "\n",
    "* import the csv\n",
    "* grab all the texts and calculate word frequencies\n",
    "* then grab all the texts by gender and calculate word frequencies\n",
    "* compare frequencies\n",
    "* BONUS: create a useful visualization\n",
    "\n",
    "Double word-score bonus might be to look at trends: all and gendered. (I'm not sure what, if anything, that might reveal.)\n",
    "\n",
    "Okay, onto the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'citation', 'author', 'gender', 'title', 'date', 'length', 'text', 'occupation', 'numDate']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>citation</th>\n",
       "      <th>author</th>\n",
       "      <th>gender</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "      <th>occupation</th>\n",
       "      <th>numDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>citation</td>\n",
       "      <td>author</td>\n",
       "      <td>gender</td>\n",
       "      <td>title</td>\n",
       "      <td>date</td>\n",
       "      <td>length</td>\n",
       "      <td>text</td>\n",
       "      <td>occupation</td>\n",
       "      <td>numDate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Al Gore 2006</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>male</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>Jun 2006</td>\n",
       "      <td>957</td>\n",
       "      <td>Thank you so much  Chris. And it's truly a gre...</td>\n",
       "      <td>Climate advocate</td>\n",
       "      <td>200606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>David Pogue 2006</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>male</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>Jun 2006</td>\n",
       "      <td>1271</td>\n",
       "      <td>Hello voice mail  my old friend. I've called f...</td>\n",
       "      <td>Technology columnist</td>\n",
       "      <td>200606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Cameron Sinclair 2006</td>\n",
       "      <td>Cameron Sinclair</td>\n",
       "      <td>male</td>\n",
       "      <td>My wish: A call for open-source architecture</td>\n",
       "      <td>Jul 2006</td>\n",
       "      <td>1398</td>\n",
       "      <td>I'm going to take you on a journey very quickl...</td>\n",
       "      <td>Co-founder, Architecture for Humanity</td>\n",
       "      <td>200607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Sergey Brin + Larry Page 2007</td>\n",
       "      <td>Sergey Brin + Larry Page</td>\n",
       "      <td>male</td>\n",
       "      <td>The genesis of Google</td>\n",
       "      <td>May 2007</td>\n",
       "      <td>1205</td>\n",
       "      <td>Sergey Brin  I want to discuss a question I kn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             citation                    author  gender  \\\n",
       "0  NaN                       citation                    author  gender   \n",
       "1  1.0                   Al Gore 2006                   Al Gore    male   \n",
       "2  2.0               David Pogue 2006               David Pogue    male   \n",
       "3  3.0          Cameron Sinclair 2006          Cameron Sinclair    male   \n",
       "4  4.0  Sergey Brin + Larry Page 2007  Sergey Brin + Larry Page    male   \n",
       "\n",
       "                                          title      date  length  \\\n",
       "0                                         title      date  length   \n",
       "1                   Averting the climate crisis  Jun 2006     957   \n",
       "2                              Simplicity sells  Jun 2006    1271   \n",
       "3  My wish: A call for open-source architecture  Jul 2006    1398   \n",
       "4                         The genesis of Google  May 2007    1205   \n",
       "\n",
       "                                                text  \\\n",
       "0                                               text   \n",
       "1  Thank you so much  Chris. And it's truly a gre...   \n",
       "2  Hello voice mail  my old friend. I've called f...   \n",
       "3  I'm going to take you on a journey very quickl...   \n",
       "4  Sergey Brin  I want to discuss a question I kn...   \n",
       "\n",
       "                              occupation  numDate  \n",
       "0                             occupation  numDate  \n",
       "1                       Climate advocate   200606  \n",
       "2                   Technology columnist   200606  \n",
       "3  Co-founder, Architecture for Humanity   200607  \n",
       "4                                    NaN   200705  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data\n",
    "\n",
    "# Let python create the column names list:\n",
    "with open('../data/talks_6d.csv') as f:\n",
    "    colnames = f.readline().strip().split(\",\")\n",
    "print(colnames)\n",
    "\n",
    "# Now will import the csv as a dataframe\n",
    "import pandas\n",
    "TEDtalks = pandas.read_csv('../data/talks_6d.csv', names=colnames)\n",
    "\n",
    "TEDtalks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Talks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm importing all of the `nltk` below because I'm not sure what, if any, of the library might be useful here. Otherwise I would simply `from nltk.tokenize import WhitespaceTokenizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Create a list of just the texts\n",
    "texts = TEDtalks.text.tolist()\n",
    "\n",
    "# Mash all the talks together & then tokenize\n",
    "alltexts = \" \".join(texts).lower()\n",
    "tokens = nltk.tokenize.WhitespaceTokenizer().tokenize(alltexts)\n",
    "\n",
    "# Remove the name of the column which is the first item in the list:\n",
    "tokens.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4373823"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text thank you so much  chris. and it's truly a great honor to have the opportunity to come to this stage twice  i'm extremely grateful. i have been blown away by this conference  and i want to thank \n"
     ]
    }
   ],
   "source": [
    "print(alltexts[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past, I've used simply dictionaries to count word frequencies -- see `Tt-02a-words` -- but we not only want to use already available functionality but we want more then word frequencies, we want to normalize word frequencies as percentage of overall corpus so that we can distinguish words that are more frequent in one or another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 251860),\n",
       " ('the', 209937),\n",
       " ('and', 150879),\n",
       " ('to', 126676),\n",
       " ('of', 115963),\n",
       " ('a', 106332),\n",
       " ('that', 96280),\n",
       " ('i', 83494),\n",
       " ('in', 78852),\n",
       " ('it', 75480)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist()\n",
    "for sentence in nltk.sent_tokenize(alltexts):\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        fd.update([word])\n",
    "\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, it has to be `fd.update([word])` and not `fd.update(word)`: the latter returns a list of letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 54269 samples and 4746253 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means there are a total of 54,269 words with a total usage of 4,746,253. \n",
    "\n",
    "The difference between a raw token count above of 4,373,823 is not explained by adding back in the frequency of periods of 251,860. If you subtract the total of those two combined, you are still left with a difference of: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-120570"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens) - (4746253 - 251860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NLTK `FreqDist` is a list of tuples containing the word and its frequency, e.g. `('and', 110130)`. I need to iterate through these three million tuples and normalize:\n",
    "\n",
    "    percentages = []\n",
    "    for word, count in old_tuple:\n",
    "        percentage = count / total words\n",
    "        word, percentage in new_tuple\n",
    "\n",
    "Even my pseudo-code is kind of ugly, I'm afraid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4746253\n"
     ]
    }
   ],
   "source": [
    "# To doublecheck the \"outcomes\" listed above is also the \n",
    "# total number of words: FreqDist is a Python counter and \n",
    "# inherits those methods:\n",
    "\n",
    "total_words = sum(fd.values()) \n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to calculate relative frequencies\n",
    "\n",
    "freq_dist = dict(fd)\n",
    "\n",
    "# MODEL: d2 = dict((k, f(v)) for k, v in d1.items())\n",
    "rel_freq = {k: v/total_words for k, v in freq_dist.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.05306501781510594),\n",
       " ('the', 0.04423215534443697),\n",
       " ('and', 0.031789076562079605),\n",
       " ('to', 0.026689685526667038),\n",
       " ('of', 0.024432536571480704),\n",
       " ('a', 0.022403356921765444),\n",
       " ('that', 0.020285475721585008),\n",
       " ('i', 0.017591561174678215),\n",
       " ('in', 0.016613526501853146),\n",
       " ('it', 0.01590307132805605)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by value \n",
    "\n",
    "import operator\n",
    "\n",
    "rf = sorted(rel_freq.items(), key=operator.itemgetter(1))\n",
    "rf.reverse()\n",
    "rf[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gendered Talks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create two additional collections filtered by the `gender` column of the dataframe. What happens in the first line below is that we filter the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_talks = TEDtalks[TEDtalks.gender == 'male'].text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_talks = TEDtalks[TEDtalks.gender == 'female'].text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 2069 TED talks given, 607 were given by women and 1437 by men.\n"
     ]
    }
   ],
   "source": [
    "# A quick check of numbers:\n",
    "\n",
    "print(\"Of the {} TED talks given, {} were given by women and {} by men.\".format\n",
    "      (len(texts), len(f_talks), len(m_talks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what you just heard '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_alltalks[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 31063 samples and 1276991 outcomes>\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "f_fd = nltk.FreqDist()\n",
    "f_alltalks = \" \".join(f_talks).lower()\n",
    "for sentence in nltk.sent_tokenize(f_alltalks):\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        f_fd.update([word])\n",
    "        \n",
    "print(f_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_totals = sum(f_fd.values()) \n",
    "print(f_totals)\n",
    "\n",
    "f_dist = dict(f_fd)\n",
    "f_rf = {k: v/f_totals for k, v in f_dist.items()}\n",
    "\n",
    "f_rf_sorted = sorted(f_rf.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d3d65d640182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# f_rf.reverse()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf_rf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "# f_rf.reverse()\n",
    "f_rf[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fd = nltk.FreqDist()\n",
    "for sentence in nltk.sent_tokenize(m_talks):\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        fd.update([word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Other Talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many talks does that leave:\n",
    "\n",
    "len(texts) - (len(f_talks) + len(m_talks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_talks = TEDtalks[(TEDtalks.gender != 'male')&(TEDtalks.gender != 'female')]\n",
    "\n",
    "# This will show you all 25 rows:\n",
    "o_talks.head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
