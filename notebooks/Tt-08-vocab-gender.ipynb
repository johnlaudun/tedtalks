{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gendered Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we want to examine the potential preferred usage of certain words by male/female speakers. We will not begin with filtering out stop words, both because they will account for a relatively small percentage of words but also because there may be something interesting there. \n",
    "\n",
    "To start, we will need to:\n",
    "\n",
    "* import the csv\n",
    "* grab all the texts and calculate word frequencies\n",
    "* then grab all the texts by gender and calculate word frequencies\n",
    "* compare frequencies\n",
    "* BONUS: create a useful visualization\n",
    "\n",
    "Double word-score bonus might be to look at trends: all and gendered. (I'm not sure what, if anything, that might reveal.)\n",
    "\n",
    "Okay, onto the code.\n",
    "\n",
    "Initial work: 2018-03-22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'citation', 'author', 'gender', 'title', 'date', 'length', 'text', 'occupation', 'numDate']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>citation</th>\n",
       "      <th>author</th>\n",
       "      <th>gender</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "      <th>occupation</th>\n",
       "      <th>numDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>citation</td>\n",
       "      <td>author</td>\n",
       "      <td>gender</td>\n",
       "      <td>title</td>\n",
       "      <td>date</td>\n",
       "      <td>length</td>\n",
       "      <td>text</td>\n",
       "      <td>occupation</td>\n",
       "      <td>numDate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Al Gore 2006</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>male</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>Jun 2006</td>\n",
       "      <td>957</td>\n",
       "      <td>Thank you so much  Chris. And it's truly a gre...</td>\n",
       "      <td>Climate advocate</td>\n",
       "      <td>200606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>David Pogue 2006</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>male</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>Jun 2006</td>\n",
       "      <td>1271</td>\n",
       "      <td>Hello voice mail  my old friend. I've called f...</td>\n",
       "      <td>Technology columnist</td>\n",
       "      <td>200606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Cameron Sinclair 2006</td>\n",
       "      <td>Cameron Sinclair</td>\n",
       "      <td>male</td>\n",
       "      <td>My wish: A call for open-source architecture</td>\n",
       "      <td>Jul 2006</td>\n",
       "      <td>1398</td>\n",
       "      <td>I'm going to take you on a journey very quickl...</td>\n",
       "      <td>Co-founder, Architecture for Humanity</td>\n",
       "      <td>200607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Sergey Brin + Larry Page 2007</td>\n",
       "      <td>Sergey Brin + Larry Page</td>\n",
       "      <td>male</td>\n",
       "      <td>The genesis of Google</td>\n",
       "      <td>May 2007</td>\n",
       "      <td>1205</td>\n",
       "      <td>Sergey Brin  I want to discuss a question I kn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             citation                    author  gender  \\\n",
       "0  NaN                       citation                    author  gender   \n",
       "1  1.0                   Al Gore 2006                   Al Gore    male   \n",
       "2  2.0               David Pogue 2006               David Pogue    male   \n",
       "3  3.0          Cameron Sinclair 2006          Cameron Sinclair    male   \n",
       "4  4.0  Sergey Brin + Larry Page 2007  Sergey Brin + Larry Page    male   \n",
       "\n",
       "                                          title      date  length  \\\n",
       "0                                         title      date  length   \n",
       "1                   Averting the climate crisis  Jun 2006     957   \n",
       "2                              Simplicity sells  Jun 2006    1271   \n",
       "3  My wish: A call for open-source architecture  Jul 2006    1398   \n",
       "4                         The genesis of Google  May 2007    1205   \n",
       "\n",
       "                                                text  \\\n",
       "0                                               text   \n",
       "1  Thank you so much  Chris. And it's truly a gre...   \n",
       "2  Hello voice mail  my old friend. I've called f...   \n",
       "3  I'm going to take you on a journey very quickl...   \n",
       "4  Sergey Brin  I want to discuss a question I kn...   \n",
       "\n",
       "                              occupation  numDate  \n",
       "0                             occupation  numDate  \n",
       "1                       Climate advocate   200606  \n",
       "2                   Technology columnist   200606  \n",
       "3  Co-founder, Architecture for Humanity   200607  \n",
       "4                                    NaN   200705  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data\n",
    "\n",
    "# Let python create the column names list:\n",
    "with open('../data/talks_6d.csv') as f:\n",
    "    colnames = f.readline().strip().split(\",\")\n",
    "print(colnames)\n",
    "\n",
    "# Now will import the csv as a dataframe\n",
    "import pandas\n",
    "TEDtalks = pandas.read_csv('../data/talks_6d.csv', names=colnames)\n",
    "\n",
    "TEDtalks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Talks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm importing all of the `nltk` below because I'm not sure what, if any, of the library might be useful here. Otherwise I would simply `from nltk.tokenize import WhitespaceTokenizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4373823 text thank you so much  chris. and it's truly a great honor to have the opportunity to come to this stage twice  i'm extremely grateful. i have been blown away by this conference  and i want to thank \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Create a list of just the texts\n",
    "texts = TEDtalks.text.tolist()\n",
    "\n",
    "# Mash all the talks together & then tokenize\n",
    "alltexts = \" \".join(texts).lower()\n",
    "tokens = nltk.tokenize.WhitespaceTokenizer().tokenize(alltexts)\n",
    "\n",
    "# Remove the name of the column which is the first item in the list:\n",
    "tokens.pop(0)\n",
    "\n",
    "print(len(tokens), alltexts[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past, I've used simply dictionaries to count word frequencies -- see `Tt-02a-words` -- but we not only want to use already available functionality but we want more then word frequencies, we want to normalize word frequencies as percentage of overall corpus so that we can distinguish words that are more frequent in one or another.\n",
    "\n",
    "**TBH (2018-03-22)**: About the only thing that `nltk.FreqDist`, from what I can tell, does is deliver a containerized dictionary. I'll keep the code as is for now, in the belief that sticking with the NLTK is good for interoperability (or something)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below it has to be `fd.update([word])` and not `fd.update(word)`: \n",
    "# the latter returns a list of letters.\n",
    "\n",
    "fd = nltk.FreqDist()\n",
    "for sentence in nltk.sent_tokenize(alltexts):\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        fd.update([word])\n",
    "\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means there are a total of 54,269 words with a total usage of 4,746,253. The difference between a raw token count above of 4,373,823 is not explained by adding back in the frequency of periods of 251,860. If you subtract the total of those two combined, you are still left with a difference of: `4,746,253 - (4,373,823 + 251,860) = 120,570`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NLTK `FreqDist` is a list of tuples containing the word and its frequency, e.g. `('and', 110130)`. I need to iterate through these three million tuples and normalize:\n",
    "\n",
    "    percentages = []\n",
    "    for word, count in old_tuple:\n",
    "        percentage = count / total words\n",
    "        word, percentage in new_tuple\n",
    "\n",
    "Even my pseudo-code is kind of ugly, I'm afraid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To doublecheck the \"outcomes\" listed above is also the \n",
    "# total number of words: FreqDist is a Python counter and \n",
    "# inherits those methods:\n",
    "\n",
    "total_words = sum(fd.values()) \n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now to calculate relative frequencies\n",
    "\n",
    "freq_dist = dict(fd)\n",
    "\n",
    "# MODEL: d2 = dict((k, f(v)) for k, v in d1.items())\n",
    "rel_freq = {k: v/total_words for k, v in freq_dist.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It dawned on me we could have a function that does everything we need:\n",
    "\n",
    "import nltk\n",
    "import operator\n",
    "\n",
    "def RelaFreq (list_of_texts):\n",
    "    # Take the list and turn it into one long string\n",
    "    all_texts = \" \".join(list_of_texts).lower()\n",
    "    # Invoke the NLTK god\n",
    "    freqdist = nltk.FreqDist()\n",
    "    # We're getting sentence data here, but I'm not sure it's needed\n",
    "    # and I don't know how much it might be slowing the process.\n",
    "    for sentence in nltk.sent_tokenize(all_texts):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            freqdist.update([word])\n",
    "    # Get the total number of words so we can establish relative frequencies\n",
    "    total_words = sum(freqdist.values())\n",
    "    # Convert the FreqDist container to a dictionary\n",
    "    freqdist_dict = dict(freqdist)\n",
    "    # Create a new dictionary of relative frequencies\n",
    "    relative_frequency = {k: v/total_words for k, v in freqdist_dict.items()}\n",
    "    # Convert the dictionary to a rankable list of tuples\n",
    "    # & rank it with the most frequent word first\n",
    "    ranked = sorted(relative_frequency.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    # return the ranked list of tuples\n",
    "    return(ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.05306501781510594),\n",
       " ('the', 0.04423215534443697),\n",
       " ('and', 0.031789076562079605),\n",
       " ('to', 0.026689685526667038),\n",
       " ('of', 0.024432536571480704),\n",
       " ('a', 0.022403356921765444),\n",
       " ('that', 0.020285475721585008),\n",
       " ('i', 0.017591561174678215),\n",
       " ('in', 0.016613526501853146),\n",
       " ('it', 0.01590307132805605)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_talks = RelaFreq(texts)\n",
    "all_talks[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gendered Talks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create two additional collections filtered by the `gender` column of the dataframe. What happens in the first line below is that we filter the dataframe, in the line that follows we pull the text column out. Originally, I had this as one line:\n",
    "\n",
    "    f_talks = TEDtalks[TEDtalks.gender == 'female'].text.tolist()\n",
    "\n",
    "But that produced a string and not a list object. I don't know why the one line would not work.\n",
    "\n",
    "**2018-03-22**: Now it's working. Did I have `texts.tolist()` -- I'm not clear what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 2069 TED talks given, 607 were given by women and 1437 by men.\n"
     ]
    }
   ],
   "source": [
    "# Filter by gender\n",
    "\n",
    "m_talks = TEDtalks[TEDtalks.gender == 'male'].text.tolist()\n",
    "f_talks = TEDtalks[TEDtalks.gender == 'female'].text.tolist()\n",
    "\n",
    "# A quick check of numbers:\n",
    "\n",
    "print(\"Of the {} TED talks given, {} were given by women and {} by men.\".format\n",
    "      (len(texts), len(f_talks), len(m_talks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: One thing to do here is to compare the talks to see which words are used **only** by women or **only** by men."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_relative = RelaFreq(f_talks)\n",
    "m_relative = RelaFreq(m_talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315 ('coming', 0.0003093208957619905) ('body', 0.00029065466307744224)\n"
     ]
    }
   ],
   "source": [
    "# Just a way to check against rank\n",
    "import random\n",
    "random_num = random.randint(1,500)\n",
    "print(random_num, f_relative[random_num], m_relative[random_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Other Talks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity, how many talks are given by more than one speaker or by a non-single gender speaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many talks does that leave:\n",
    "\n",
    "len(texts) - (len(f_talks) + len(m_talks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o_talks = TEDtalks[(TEDtalks.gender != 'male')&(TEDtalks.gender != 'female')]\n",
    "\n",
    "# This will show you all 25 rows:\n",
    "o_talks.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing M/F Word Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to start by converting the list of tuples to a dictionary both because I think matching keys is going to be easier (at least based on my limited coding ability) and because, according to what I read, it appears to be faster. Since we don't really need a ranked listing for this work, it would probably be wise to rewrite the `RelaFreq` function so that it produces a dictionary. No reason to go back and forth like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_rf = dict(f_relative)\n",
    "\n",
    "m_rf = dict(m_relative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on how to compare -- as I write this I am trying to find a way to limit my for loop through the two dictionaries only to N results just so I can see if it's working. \n",
    "\n",
    "[Python - Return first N key:value pairs from dict](https://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a way to match words in the f/m dictionaries\n",
    "\n",
    "for key in f_rf:\n",
    "    if key in m_rf:\n",
    "        print(key, f_rf[key], m_rf[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I decided to do was run the cell and then stop it. The results from above look like this:\n",
    "\n",
    "    hour 8.613999628814925e-05 0.00012193460211598934\n",
    "    debased 7.830908753468114e-07 2.924091177841471e-07\n",
    "    deteriorate 3.1323635013872456e-06 1.7544547067048825e-06\n",
    "    perimeter 1.5661817506936228e-06 3.508909413409765e-06\n",
    "\n",
    "What we need now is to compare one list against the other for differences in usage. I am starting with twice as often to see what that turns up. >>> I need to re-read the literature here to see what comparison thresholds have been used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'giveaway'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-69c298d21b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# for key in f_rf:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mf_rf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mm_rf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_rf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_rf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'giveaway'"
     ]
    }
   ],
   "source": [
    "# Looking for matches where a word's relative frequency in f_rf is 2X m_rf:\n",
    "\n",
    "# for key in f_rf:\n",
    "if f_rf[key] > m_rf[key]:\n",
    "    print(key, f_rf[key], m_rf[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
