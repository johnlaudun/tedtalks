{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED Talk Transcription Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explore the various features of the TED talks documents that are a product of their being transcripts of oral performances. The transcription process itself is not entirely clear, since it is the product of volunteers. In practice, this means different volunteers, or groups of volunteers, transcribe the same or similar performance features differently. The goal of this notebook is to survey possible feature artifaces and to determine what, if any, of these artifacts need to be addressed when it comes to processing the documents for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load our functions and then our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re \n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import tokenize, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From our 992 x 14 CSV, we have a list of 992 talks.\n"
     ]
    }
   ],
   "source": [
    "# Loading the data in a gendered partitioned fashion: \n",
    "df_male = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "df_female = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "df_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "\n",
    "df_all = pd.concat([df_male, df_female, df_nog])\n",
    "\n",
    "texts = df_all.text.tolist()\n",
    "\n",
    "print(f\"From our {df_all.shape[0]} x {df_all.shape[1]} CSV, \\\n",
    "we have a list of {len(texts)} talks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing upon some of the early work in the *Terms* notebook, we run Sci-Kit Learn's `CountVectorizer` with only one parameter: for a word to be counted, it must occur in at least two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 22433)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default vectorizer = lowercase, remove punctuation, \n",
    "# tokens > 2 char, split contractions, no stopwords\n",
    "vectorizer = CountVectorizer(min_df = 2)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the words in an array, we can inspect various segments of the word count. Inside this notebook, we use the `head()` method which with 20 entries, begins to suggest that there a lot of numbers to be taken into consideration. For a fuller examination, we also save the complete count to a CSV. (Here commented out so that it doesn't run every time the notebook runs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00          4\n",
       "000      1187\n",
       "000th       6\n",
       "01          3\n",
       "02          2\n",
       "06          5\n",
       "07          2\n",
       "10        981\n",
       "100       545\n",
       "1000        5\n",
       "100th       5\n",
       "101        13\n",
       "102         3\n",
       "103         3\n",
       "105        10\n",
       "109         3\n",
       "10th       20\n",
       "11        223\n",
       "110        10\n",
       "111         3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_words = pd.DataFrame(X.toarray(), \n",
    "                      columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "wc_all = df_all_words.sum()\n",
    "wc_all.head(20)\n",
    "# wc_all.to_csv('../output/gender-part-word_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done that, and scrolling down to the 800th row before beginning to see letters forming something like words, we note that there are a lot of ways to spell out *ah*, which led us to ask survey *ah*'s occurrences in order to understand better.\n",
    "\n",
    "Please note that with the CSV, it's enough to sort the **word-count** pairings, in order to explore the distribution of terms across the corpus. Within this notebook the following works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the     93853\n",
       "and     67710\n",
       "to      57089\n",
       "of      52313\n",
       "that    44087\n",
       "it      35339\n",
       "in      34728\n",
       "you     34162\n",
       "we      30407\n",
       "is      28569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_sorted = wc_all.sort_values(ascending=False, inplace=False)\n",
    "wc_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pause Fillers and Interjections: Variations on \"ah\"\n",
    "\n",
    "The words from a search for \"ah\" were: \n",
    "\n",
    "    aaaah, aah, ah, ahh, ahhh\n",
    "    \n",
    "The words from a search for \"aa\" were: \n",
    "\n",
    "    aa, aaa, aaaa, aaaaa, aaaah, aah\n",
    "    \n",
    "Code used:\n",
    "\n",
    "```python\n",
    "for match in wc_all.index:\n",
    "    if \"aa\" in match:\n",
    "        print(match)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa        11\n",
       "aaa        6\n",
       "aah       10\n",
       "aaaah      2\n",
       "aah       10\n",
       "ah       116\n",
       "ahh        6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ahs_list = [\"aa\", \"aaa\", \"aaaa\", \"aaaaa\", \"aah\", \n",
    "            \"aaaah\", \"aah\", \"ah\", \"ahh\", \"ahhh\" ] \n",
    "\n",
    "ahs = wc_all.filter(items = ahs_list, axis=0)\n",
    "ahs.head(len(ahs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  thank you so much, chris. and it's truly a great honor to have the opportunity to come to this sta\n"
     ]
    }
   ],
   "source": [
    "alltexts = ' '.join(texts).lower()\n",
    "t = tokenize.WhitespaceTokenizer()\n",
    "corpse = text.Text(t.tokenize(alltexts))\n",
    "print(alltexts[0:100])\n",
    "\n",
    "# ALT: corpse = text.Text('\\n'.join(texts).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 14 of 14 matches:\n",
      "era got it? five, six, seven, eight — ah — nine, 10, the jack, jack of spades,\n",
      "t is that planes can fly upside down. ah ha, right. second question is, why is\n",
      ", ah, aah ♫ ♫ ah, ah, oh, ah, ah, oh, ah ♫ ♫ oh, ah, ah, ah ♫ ♫ ah, ah, oh, ah\n",
      "h, oh, ah, ah, oh, ah ♫ ♫ oh, ah, ah, ah ♫ ♫ ah, ah, oh, ah, ah, oh, oh ♫ ♫ if\n",
      "by crystal skulls. (applause) (music) ah yes, a table. we use these every day.\n",
      "them. thank you very much. (applause) ah yes, those university days, a heady m\n",
      "ess, i'd make it appear for them. ah, ah ah! — like that. and people were frea\n",
      "p, but don't fall apart. ooh ooh ooh, ah ah ah ah ah. ah ah, ah ah ah. when i \n",
      "but don't fall apart. ooh ooh ooh, ah ah ah ah ah. ah ah, ah ah ah. when i was\n",
      " don't fall apart. ooh ooh ooh, ah ah ah ah ah. ah ah, ah ah ah. when i was ou\n",
      "n't fall apart. ooh ooh ooh, ah ah ah ah ah. ah ah, ah ah ah. when i was out t\n",
      "l apart. ooh ooh ooh, ah ah ah ah ah. ah ah, ah ah ah. when i was out there, t\n",
      ". ooh ooh ooh, ah ah ah ah ah. ah ah, ah ah ah. when i was out there, the ocea\n",
      "oh ooh ooh, ah ah ah ah ah. ah ah, ah ah ah. when i was out there, the ocean w\n"
     ]
    }
   ],
   "source": [
    "corpse.concordance(\"ah\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "There appears to be a tokenizer mis-match between SciKit-Learn, from which the list of <b>ah</b>s above is drawn and the NLTK tokenizer which is not finding the same number of tokens: Sci-Kit Learn, 116; NLTK, 14.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performatives and Other Parenthetical Expressions <a class=\"anchor\" id=\"performatives\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Performatives\n",
    "\n",
    "As the concordance for *ah* above reveals: some TED talks are not talks but musical performances, and they feature textual insertions that are not drawn from the performance itself but, rather, are features of the performance event, e.g., \"(applause)\" marks the fact that an audience applauded and not the speaker said the word \"applause.\" \n",
    "\n",
    "Generally, the text of such performances is rather short. Using an arbitrary length of `500` characters, we can gather a number of these texts and inspect them to see what kinds of things are inserted parenthetically into the transcripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 183, 297, 388, 602, 983, 991]\n"
     ]
    }
   ],
   "source": [
    "shorts = [ texts.index(text) for text in texts if len(text) < 500 ]\n",
    "print(shorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84:\n",
      "  (Applause)    (Music)    (Applause)  \n",
      "\n",
      "183:\n",
      "  Let's just get started here.    Okay, just a moment.    (Whirring)    All right. (Laughter) Oh, sorry.    (Music) (Beatboxing)    Thank you.    (Applause)  \n"
     ]
    }
   ],
   "source": [
    "print(f\"{shorts[0]}:\\n{texts[shorts[0]]}\\n\\n{shorts[1]}:\\n{texts[shorts[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the two examples above highlights, some TED  do not actually contain any text, except for the performatives, while other talks contain a mix of textual and non-textual materials, some of which is part of the performance and some of which is the audience's response to the performance. \n",
    "\n",
    "When it comes time to process words in a text to explore semantic dimensions, we think the best bet is to remove the performatives. We do think, however, that having them means we can possibly explore sentiment using `(Applause)` and `(Laughter)` as contextual valuations.\n",
    "\n",
    "For now, we need some regex to remove the parentheses and their contents from our texts. An examination of `113` above reveals that it is only three parenthetical expressions.\n",
    "\n",
    "A quick check using two regexes give us one list without the parentheses and one with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whirring', 'Laughter', 'Music', 'Beatboxing', 'Applause']\n",
      "['(Whirring)', '(Laughter)', '(Music)', '(Beatboxing)', '(Applause)']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'(?<=\\().*?(?=\\))', texts[183]))\n",
    "print(re.findall(r'\\([^)]*\\)', texts[183]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Parenthetical Matters\n",
    "\n",
    "We can also use `CountVectorizer` to inventory all the parenthetical expressions in the corpus to see if we are missing anything. \n",
    "\n",
    "First, we test on a known text, `texts[183]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>applause</th>\n",
       "      <th>beatboxing</th>\n",
       "      <th>laughter</th>\n",
       "      <th>music</th>\n",
       "      <th>whirring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     applause  beatboxing  laughter  music  whirring\n",
       "183         1           1         1      1         1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(token_pattern = r'(?<=\\().*?(?=\\))')\n",
    "X183 = vec.fit_transform(texts[183:184])\n",
    "df183 = pd.DataFrame(X183.toarray(), columns=vec.get_feature_names_out())\n",
    "df183.index = [ texts.index(text) for text in texts[183:184] ]\n",
    "df183.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the same vectorizer with the regex pattern designed above to capture only parenthetical expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 449)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parentheticals = vec.fit_transform(texts)\n",
    "parentheticals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"although it's nothing serious, let's keep an eye on it to make sure it doesn't turn into a major lawsuit.\"</th>\n",
       "      <th>\"close it!\"</th>\n",
       "      <th>\"i sold my soul for about a tenth of what the damn things are going for now.\"</th>\n",
       "      <th>\"in order to remain competitive in today's marketplace, i'm afraid we're going to have to replace you with a sleezeball.\"</th>\n",
       "      <th>\"intrigue and murder among 16th century ottoman court painters.\"</th>\n",
       "      <th>\"michael crichton responds by fax:\"</th>\n",
       "      <th>\"the end\" by the doors</th>\n",
       "      <th>\"what's a jurassic park?\"</th>\n",
       "      <th>\"wow! fucking fantastic jacket\"</th>\n",
       "      <th>\"yes, books. you know, the bound volumes with ink on paper. you cannot turn them off with a switch. tell your kids.\"</th>\n",
       "      <th>...</th>\n",
       "      <th>whistles</th>\n",
       "      <th>whistling</th>\n",
       "      <th>whoosh</th>\n",
       "      <th>woman screaming</th>\n",
       "      <th>woman: have you ever done a kissing test before?</th>\n",
       "      <th>woman: okay.</th>\n",
       "      <th>woo-hoo-hoo-hoo</th>\n",
       "      <th>xylophone</th>\n",
       "      <th>yelling more loudly</th>\n",
       "      <th>your fathers bristles white and stiff now</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 449 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   \"although it's nothing serious, let's keep an eye on it to make sure it doesn't turn into a major lawsuit.\"  \\\n",
       "0                                                  0                                                             \n",
       "1                                                  0                                                             \n",
       "2                                                  0                                                             \n",
       "3                                                  0                                                             \n",
       "4                                                  0                                                             \n",
       "\n",
       "   \"close it!\"  \\\n",
       "0            0   \n",
       "1            0   \n",
       "2            0   \n",
       "3            0   \n",
       "4            0   \n",
       "\n",
       "   \"i sold my soul for about a tenth of what the damn things are going for now.\"  \\\n",
       "0                                                  0                               \n",
       "1                                                  0                               \n",
       "2                                                  0                               \n",
       "3                                                  0                               \n",
       "4                                                  0                               \n",
       "\n",
       "   \"in order to remain competitive in today's marketplace, i'm afraid we're going to have to replace you with a sleezeball.\"  \\\n",
       "0                                                  0                                                                           \n",
       "1                                                  0                                                                           \n",
       "2                                                  0                                                                           \n",
       "3                                                  0                                                                           \n",
       "4                                                  0                                                                           \n",
       "\n",
       "   \"intrigue and murder among 16th century ottoman court painters.\"  \\\n",
       "0                                                  0                  \n",
       "1                                                  0                  \n",
       "2                                                  0                  \n",
       "3                                                  0                  \n",
       "4                                                  0                  \n",
       "\n",
       "   \"michael crichton responds by fax:\"  \"the end\" by the doors  \\\n",
       "0                                    0                       0   \n",
       "1                                    0                       0   \n",
       "2                                    0                       0   \n",
       "3                                    0                       0   \n",
       "4                                    0                       0   \n",
       "\n",
       "   \"what's a jurassic park?\"  \"wow! fucking fantastic jacket\"  \\\n",
       "0                          0                                0   \n",
       "1                          0                                0   \n",
       "2                          0                                0   \n",
       "3                          0                                0   \n",
       "4                          0                                0   \n",
       "\n",
       "   \"yes, books. you know, the bound volumes with ink on paper. you cannot turn them off with a switch. tell your kids.\"  \\\n",
       "0                                                  0                                                                      \n",
       "1                                                  0                                                                      \n",
       "2                                                  0                                                                      \n",
       "3                                                  0                                                                      \n",
       "4                                                  0                                                                      \n",
       "\n",
       "   ...  whistles  whistling  whoosh  woman screaming  \\\n",
       "0  ...         0          0       0                0   \n",
       "1  ...         0          0       0                0   \n",
       "2  ...         0          0       0                0   \n",
       "3  ...         0          0       0                0   \n",
       "4  ...         0          0       0                0   \n",
       "\n",
       "   woman: have you ever done a kissing test before?  woman: okay.  \\\n",
       "0                                                 0             0   \n",
       "1                                                 0             0   \n",
       "2                                                 0             0   \n",
       "3                                                 0             0   \n",
       "4                                                 0             0   \n",
       "\n",
       "   woo-hoo-hoo-hoo  xylophone  yelling more loudly  \\\n",
       "0                0          0                    0   \n",
       "1                0          0                    0   \n",
       "2                0          0                    0   \n",
       "3                0          0                    0   \n",
       "4                0          0                    0   \n",
       "\n",
       "   your fathers bristles white and stiff now  \n",
       "0                                          0  \n",
       "1                                          0  \n",
       "2                                          0  \n",
       "3                                          0  \n",
       "4                                          0  \n",
       "\n",
       "[5 rows x 449 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parens = pd.DataFrame(parentheticals.toarray(), \n",
    "                         columns=vec.get_feature_names_out())\n",
    "df_parens.index = [ texts.index(text) for text in texts ]\n",
    "df_parens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the parenthetical expressions captured, we can determine the most frequent, take a look at their numbers, and then decide what's the best path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "laughter           4625\n",
       "applause           2446\n",
       "music               297\n",
       "video               185\n",
       "audio                33\n",
       "music ends           29\n",
       "singing              29\n",
       "applause ends        21\n",
       "laughs               16\n",
       "cheers               14\n",
       "guitar strum         14\n",
       "guitar               13\n",
       "sighs                13\n",
       "drum sounds          10\n",
       "marimba sounds       10\n",
       "sings                 8\n",
       "ball squeaks          7\n",
       "clicking              7\n",
       "drum sound            6\n",
       "drum roll             6\n",
       "cheering              6\n",
       "audio: laughing       6\n",
       "piano                 6\n",
       "beep                  6\n",
       "ss's voice            5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums = df_parens.sum(axis = 0)\n",
    "sums.sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the top 20 parentheticals could be inserted into a stopword list and we would remove, in the case of the top 4 especially, words or clauses that might affect results.\n",
    "\n",
    "The code below can be included in `CountVectorizer` as a preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Refined Preprocessor --\n",
    "# This one removes two-word phrases/clauses\n",
    "\n",
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\",  \n",
    "                  \"\\(video\\)\", \"\\(laughs\\)\", \"\\(applause ends\\)\", \n",
    "                  \"\\(audio\\)\", \"\\(singing\\)\", \"\\(music ends\\)\", \n",
    "                  \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \n",
    "                  \"\\(marimba sounds\\)\", \"\\(drum sounds\\)\" ]\n",
    "\n",
    "essential_parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\", \"\\(video\\)\"]\n",
    "\n",
    "def remove_parentheticals(text):\n",
    "    global parentheticals\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), \n",
    "                          flags=re.IGNORECASE)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laughter is the best medicine.   \n",
      "hold your applause; i'm not done yet.  \n"
     ]
    }
   ],
   "source": [
    "test = \"\"\"Laughter is the best medicine. (Laughter) \n",
    "Hold your applause; I'm not done yet. (Applause ends)\"\"\"\n",
    "\n",
    "print(remove_parentheticals(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 22389)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_vec = CountVectorizer( preprocessor = remove_parentheticals,\n",
    "                          max_df = 0.9, min_df = 2 )\n",
    "the_X = the_vec.fit_transform(texts)\n",
    "the_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers\n",
    "\n",
    "One of the dimensions of the corpus that arises out of a hand inspection of the terms is the frequency with which some numbers appear. The follow table captures the top ten numbers:\n",
    "\n",
    "| TERM | FREQUENCY |\n",
    "|------|-----------|\n",
    "| 000  | 2098 |\n",
    "| 10   | 1691 |\n",
    "|  20  | 1107 |\n",
    "| 100  |  902 |\n",
    "|  30  |  827 |\n",
    "|  50  |  784 |\n",
    "|  15  |  659 | \n",
    "|  40  |  494 |\n",
    "|  12  |  460 | \n",
    "|  25  |  410 |\n",
    "\n",
    "Other frequently occurring numbers: 60, 500, 200, 11, 18, 80, 14 (241 times!). \n",
    "\n",
    "In order to examine the appearance of the numbers in context, we make a giant string out of the list of strings, `texts`: in which text a number appears is less important than its immediate context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "corpse.concordance(\"000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 690 matches:\n",
      "thank you very much. (applause) about 10 years ago, i took on the task to teac\n",
      "tion of income of people. one dollar, 10 dollars or 100 dollars per day. there\n",
      " a long time, but they come out after 10 years very, very differently. and the\n",
      "at drives you in your life today? not 10 years ago. are you running the same p\n",
      "really heavy, but in the last five or 10 years, have there been some decisions\n",
      ". (laughter) are you sure? (laughter) 10 seconds! (laughter and applause) 10 s\n",
      ") 10 seconds! (laughter and applause) 10 seconds, i want to be respectful. all\n",
      "principle in the bible that says give 10 percent of what you get back to chari\n",
      "ional shelter that would last five to 10 years, that would be placed next to t\n",
      "tandards of five billion people? with 10 million solutions. so i wish to devel\n",
      " will not launch this without five to 10 million units in the first run. and t\n",
      " down, and that's why i said seven to 10 million there. and we're doing it wit\n",
      " your laptop costs, in rough numbers, 10 dollars a diagonal inch. that can dro\n",
      "s; i can just go right up and use all 10 fingers if i wanted to. you know, lik\n",
      "ll from earthlink that said, due to a 10 cents per megabyte overage charge, i \n",
      "and they simply point and they go for 10 percent, right in the middle. howard \n",
      "ss in this country. and over the next 10 years, they made 600 million dollars \n",
      " you today is that, in fact, based on 10 years of research, a unique opportuni\n",
      "in the housing project on and off for 10 years, hanging out in crack houses, g\n",
      " marathons back to back. 800 miles in 10 weeks. and i was dragging all the foo\n",
      "week. he described this expedition as 10 times as dangerous as everest. so for\n",
      "ernight stay. we were stuck there for 10 days. there was a kind of vodka-fuele\n",
      "ng into headwinds for nine out of the 10 weeks i was alone last year, and i wa\n",
      "n expedition that's been described as 10 times as dangerous as everest. it was\n",
      "do an elliptical orbit and miss it by 10 feet if you want. oh, it's going to b\n"
     ]
    }
   ],
   "source": [
    "corpse.concordance(\"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 219 matches:\n",
      "ing rooms, whose evolution in 20, 30, 40 years we can't predict. So that liter\n",
      "nd all the other teams have done this 40 Days of Purpose, based on the book. A\n",
      "nternet tools, and we ended up having 40 chapters starting up, thousands of ar\n",
      "cumented the Lower Ninth for the last 40 years. That was their home, and these\n",
      "things tend to happen every 25 years. 40 years long, with an overlap. You can \n",
      " all high-rises. So they'll put 20 or 40 up at a time, and they just go up in \n",
      "te, we've seen no side effects in the 40 or so patients in whom it's been impl\n",
      " terms of price performance, that's a 40 to 50 percent deflation rate. And eco\n",
      " people may increase their volume 30, 40 percent, but they won't keep up with \n",
      "r hunters could smell animal urine at 40 paces and tell you what species left \n",
      "percent of all the pregnancies. About 40 percent of all the people — I said 40\n",
      "I said 400,000. I meant 40,000. About 40 percent of all the people who need TB\n",
      "tt-hours per kilogram nickel-cadmium, 40 watt-hours per kilogram in nickel-met\n",
      "ater. Imagine what the world was like 40 years ago, with just the Saturday Eve\n",
      " to India. India and China used to be 40 percent of the global economy just at\n",
      "Hong Kong, with six million people in 40 square miles. During the dry season, \n",
      " farming the same piece of ground for 40 centuries. You can't farm the same pi\n",
      "n't farm the same piece of ground for 40 centuries without understanding nutri\n",
      "o measure it — but it's anywhere from 40 to 60 years. It goes on a long time. \n",
      "hree years later, by 1908, it weighed 40 pounds. Now, not all these things wer\n",
      "is — to sequence the human genome for 40 million dollars by the end of this ye\n",
      "ppened in Brazil is, they've replaced 40 percent of the gasoline consumed by t\n",
      "ing for a solution to deal with 30 or 40 thousand people a day who were going \n",
      "you have this lag of more or less 30, 40 years' difference on the health. And \n",
      "you can see that right around age 45, 40 to 45, there's a sudden spike in the \n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"40\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to note here:\n",
    "\n",
    "First, there is a discrepancy in the count between `sklearn` and the NLTK: the former counted 2098 occurrences of `000`, the latter none. In all the counts that follow, there is a similar mismatch:\n",
    "\n",
    "| TERM | `sklearn` | `nltk` |\n",
    "|------|-----------|--------|\n",
    "| 000  | 2098 | \"no match\" |\n",
    "|  10  | 1691 | 1216 |\n",
    "|  20  | 1107 | 879 |\n",
    "| 100  |  902 | 647 |\n",
    "|  30  |  827 | 650 |\n",
    "|  50  |  784 | 594 | \n",
    "|  15  |  659 | 512 | \n",
    "|  40  |  494 | 387 | \n",
    "| ...               | \n",
    "|  14  |  241 | 148 | \n",
    "\n",
    "We don't have a ready explanation for this.\n",
    "\n",
    "Second, the frequency of some numbers are readily explained:\n",
    "\n",
    "* Round numbers like 10, 20, 30, 50, and 100 are approximations -- though it would be interesting to explore how often they are attached to large scalars like \"thousand\" or million.\" \n",
    "* Some numbers seem to represent alternate ways of counting: 25 reagularly stands in for \"one-quarter\" -- though not as often as we might imagine -- and 18 is regularly paired with *month* as a more precise way to say \" a year and a half.\"\n",
    "* There are some numbers, like 11 and 14 which seem to have power all their own, perhaps tied to particular ages in humans. \n",
    "\n",
    "Next up is some code to explore the most common occurring words with these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collocations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mz/k5p3f5wj0czgd_5m4dw59py00000gn/T/ipykernel_52100/3358661373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbigram_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollocations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'collocations' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "bigram_measures = collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All my searches for \"collocations with specific words\" took me to the NLTK, which means, so far as I can tell, generating all the bigrams and then filtering to get the one(s) you want. This seems backwards to me: wouldn't it be faster simply to find the word and then what comes after it? I'll take a look at regex for this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bigrams\n",
    "finder = BigramCollocationFinder.from_words(onetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bigram_measures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mz/k5p3f5wj0czgd_5m4dw59py00000gn/T/ipykernel_52100/1601270248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_ngram_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# return the 10 n-grams with the highest PMI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_measures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bigram_measures' is not defined"
     ]
    }
   ],
   "source": [
    "## Here's the filter operation:\n",
    "the_number = lambda *w: '14' not in w\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain the number\n",
    "finder.apply_ngram_filter(the_number)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not return a count. *Oi!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there's the missing `000`! It's in the idiomatic transcription practices of TED wherein a number like \"sixty thousand\" is rendered as \"60,000.\" \n",
    "\n",
    "One thing we know now: reporting large numbers is a part of TED talks.\n",
    "\n",
    "**TO DO**: How to keep the comma marker between numbers? (Or should we just look to 000 as a possible collocate with the other numbers?) One solution from the [Regex Cookbook][]:\n",
    "\n",
    "```python\n",
    "\\b[0-9]{1,3}(,[0-9]{3})*(\\.[0-9]+)?\\b|\\.[0-9]+\\b\n",
    "```\n",
    "\n",
    "[Regex Cookbook]: https://www.oreilly.com/library/view/regular-expressions-cookbook/9781449327453/ch06s11.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
