{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we focus only on talks sponsored by the TED organization itself, 1747 texts, and not the TEDx talks. With a focus on gender, we have further restricted the set to the 99 talks given at the main TED event, removing the effects that the *TED Women* event may have had. \n",
    "\n",
    "This is a small corpus, and so the usual reasons for shrinking the feature set for texts do not (necessarily) apply, but as we begin our survey of the contents of the TED talks we want to be mindful of standards that have emerged both so that our results are comparable to the work of others but also so that we can potentially scale up the work here without having to re-think the foundations. (At the same time, we can enjoy taking our time to explore various methodological and topical issues that a small corpus presents. For more on that, see the Transcription Effects notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "* [Summary](#summary)\n",
    "* [Tokenization(s)](#tokenizations)\n",
    "* [Vectorizer Variations](#variations)\n",
    "  - Frequencies of Terms with a 2-Document Minimum\n",
    "  - Frequencies of Terms Appearing in All/Most Documents\n",
    "  - [One Doc Wonders](#wonders)\n",
    "    - A Very Peculiar Vector: `max_df = 1`\n",
    "    - Words That Appear More Than Once But Only in One Document\n",
    "* [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary <a class=\"anchor\" id=\"summary\"></a>\n",
    "\n",
    "Summing the words to get a total for each word across all talks in the dataset, we then hand-inspect the totals and discover that there is an assortment of numbers that recur: an exploration of that phenomenon appears in a separate notebook. The primary task of this notebook is to count the words in the text, which means addressing the matter of parentheticals head on. Much of this work occurs in a separate notebook, but everything comes down to the fact that there are two kinds of parenthetical expressions in TED talks:\n",
    "\n",
    "* those which indicate actions and events outside the speaker's discourse, such as the audience laughing or applauding or the speaker sighing or playing music. \n",
    "* those which somehow the TED transcription practices have discerned as being digressive material within the speech of the author. \n",
    "\n",
    "We want to remove the former and keep the latter. In the end, the decision is made to remove, through `regex` matching, the top 20 parentheticals and to leave the others, most of which occur less than a dozen times. The affect on the overall study is thus reduced significantly.\n",
    "\n",
    "After removing the parentheticals, a second term matrix is derived with a total vocabulary of 50377. (This after discovering a slight bug, or at least weirdness, in `sklearn` that is now in their bug-tracking system.) \n",
    "\n",
    "The final step in this notebook is to raise the threshold for a word to be included in the TF-matrix to 2 documents. With that done, we convert the matrix to a pandas dataframe and then save to CSV. (QUESTION: better/more efficient to save as pickle?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re \n",
    "import nltk\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From our 992x14 CSV, we have a list of 992 talks.\n"
     ]
    }
   ],
   "source": [
    "# Remove quotes at top and bottom to uncomment this block\n",
    "\n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "\n",
    "# Load binary gendered talks \n",
    "talks_male = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_female = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "\n",
    "# No one gender ==> NOG\n",
    "talks_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "\n",
    "all_talks = pd.concat([talks_male,talks_female,talks_nog])\n",
    "\n",
    "texts = all_talks.text.tolist()\n",
    "\n",
    "print(f\"From our {all_talks.shape[0]}x{all_talks.shape[1]} CSV, \\\n",
    "we have a list of {len(texts)} talks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization(s) <a class=\"anchor\" id=\"tokenizations\"></a>\n",
    "\n",
    "Before we can count words and establish frequencies, we need to settle upon what we are going to consider words, which means determining our method of tokenizing our strings of characters into lists of tokens.\n",
    "\n",
    "- The first tokenizer is regex that John has long used in order to keep contractions as single words, but it throws away all other forms of punctuation, including numbers which, as we will see later in this notebook, feature prominently in the TED talk transcripts.\n",
    "- The NLTK's `word_tokenize()` function is based on a TreebankWordTokenizer: basically it tokenizes text like in the Penn Treebank, which means apostrophes break contractions into their distinct parts — e.g., `I'm` becomes `I` + `'m`. Whereas `wordpunct_tokenize()` is a regex that breaks the apostrophes of contractions into their own tokens.\n",
    "- SciKit Learn's tokenization comes up the leanest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex:       2146\n",
      "nltk words:  2576\n",
      "nltk wpunct: 2686\n",
      "scikit:      2035\n"
     ]
    }
   ],
   "source": [
    "# REGEX\n",
    "regex = [word for word in re.sub(\"[^a-zA-Z']\",\" \", texts[0]).lower().split()]\n",
    "\n",
    "# NLTK\n",
    "w_tokens = [word.lower() for word in nltk.word_tokenize(texts[0])]\n",
    "wp_tokens = [word.lower() for word in nltk.wordpunct_tokenize(texts[0])]\n",
    "\n",
    "# SciKit-Learn\n",
    "vectorizer = CountVectorizer( lowercase = True ) # We are vectorizing\n",
    "x = vectorizer.fit_transform([texts[0]])         # the same text as above\n",
    "count = np.sum(x.toarray(), axis = 1)            # then summing the freq count\n",
    "\n",
    "# Print to Compare\n",
    "print(f\"regex:       {len(regex)}\")\n",
    "print(f\"nltk words:  {len(w_tokens)}\")\n",
    "print(f\"nltk wpunct: {len(wp_tokens)}\")\n",
    "print(f\"scikit:      {count[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When examined as the set of tokens making up the word count, the differences do not seem so great, a matter of a little more than a dozen words, as seen in the first cell below, and then in the following cell we take a look at the words involved: it's not clear why the NLTK tokenizer is making hyphenated words as one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METHOD : TOKEN SET\n",
      "regex  :  727\n",
      "NLTK   :  725\n",
      "SciKit :  711\n"
     ]
    }
   ],
   "source": [
    "print(f\"METHOD : TOKEN SET\")\n",
    "print(f\"regex  :  {len(set(regex))}\")\n",
    "print(f\"NLTK   :  {len(set(w_tokens))}\")\n",
    "print(f\"SciKit :  {x.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'d\", '30-second', 're-brand', '``', \"'ve\", 'hot-button', 'beach-combing', 'i-40', '.', 'limited-use', 'carbon-neutral', \"n't\", 'has', 'low-hanging', 'climatecrisis.net', 'low-cost', 'end-use', \"''\", 'a', 'short-term', \"'ll\", '2.0', 'g-v', '?', '39.5', '—', '!', 'click-through', 'does', \"'my\", ':', '(', 'rear-view', \"'s\", \"'named\", 'ok.', 'i', \"'re\", '28-second', ',', \"'\", ')', \"'m\", ';', 'consumer-friendly', 'u.s.', 're-purposed'}\n"
     ]
    }
   ],
   "source": [
    "difference = set(w_tokens) - set(vectorizer.get_feature_names_out())\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer Variations <a class=\"anchor\" id=\"variations\"></a>\n",
    "\n",
    "With the short run of experiments above solidifying that SciKit-Learn's tokenizer as sufficient, we first run `CountVectorizer` unadorned. \n",
    "\n",
    "The default options are: \n",
    "\n",
    "- lowercase everything, \n",
    "- get rid of all punctuation, \n",
    "- make a word out of anything more than two characters long, \n",
    "- split contractions, and \n",
    "- no stopwords.\n",
    "\n",
    "The tokenizer is not without its problems: while it breaks contractions at the apostrophe, like NLTK, it then throws away anything less than two letters, which means `I'm` disappears entirely. And pity the indefinite article *a(n)*, which is pitched while the definite article *the* remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 39515)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going with the defaults, \n",
    "# so no options/arguments are being passed:\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the model to the data \n",
    "# vecs = vectorizer.fit(texts)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# see how many features we have\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50,379 tokens is our raw, unfiltered, no pre-processing baseline. It includes, as we will see, a number of artifacts of the TED transcription process, including a variety of ways to spell out *ah*, the use of numerals for a host of things -- from dates to counts and everything in between, and some things which actually take place outside the talk itself, like audience laughter, which we here term *parentheticals*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "raw_series = raw_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00              4\n",
       "000          1187\n",
       "000000004       1\n",
       "000001          1\n",
       "000042          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over two thousand occurrences of `000`! That's unexpected (but not unexplainable). We explore this oddity among other transcription artifacts below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies of Terms with a 2-Document Minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a `min_df = 2` removes all words that appear in only document. (An exploration of *one document wonders* is elsewhere.) As we can see from the `shape` below, we dropped 50379 to 29340, resulting in a reduction of 42% of the possible features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 22433)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min2_vec = CountVectorizer(min_df = 2)\n",
    "min2_X = min2_vec.fit_transform(texts)\n",
    "min2_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above we can sum our terms and write the results to a CSV file for closer inspection. Here, we highlight just a particular series of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alternating       5\n",
       "alternative      82\n",
       "alternatively     2\n",
       "alternatives     13\n",
       "alters            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe\n",
    "min2_df = pd.DataFrame(min2_X.toarray(), columns = min2_vec.get_feature_names_out())\n",
    "\n",
    "# Create a series of sums\n",
    "min2_series = min2_df.sum()\n",
    "\n",
    "# Write sums to CSV:\n",
    "# min2_series.to_csv('../output/word_freq_clean.csv')\n",
    "\n",
    "# Inspect the sums\n",
    "min2_series[1000:1005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies of Terms Appearing in All/Most Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trimmed terms that occur in only document from our feature set, we also need to determine what words occur across all the documents and thus are not particularly effective discriminators of topic. \n",
    "\n",
    "Note that this is a somewhat more dangerous move to make, since it has been shown that one of the keys to genre and gender in documents are the varying uses of so-called function words. In fact, as our explorations establish, using sklearn's built-in algorithms reveals a focus on removing common words does not really result in much of a reduction in the overall feature set, only 8 words occurring across 99% or more documents and that number only rising to 46 when we change the minimum threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs_vec = CountVectorizer( min_df = 0.99 )\n",
    "alldocs_X = alldocs_vec.fit_transform(texts)\n",
    "alldocs_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and    67710\n",
       "of     52313\n",
       "the    93853\n",
       "to     57089\n",
       "you    34162\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs_df = pd.DataFrame(alldocs_X.toarray(), columns = alldocs_vec.get_feature_names_out())\n",
    "alldocs_series = alldocs_df.sum()\n",
    "alldocs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 43)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostdocs_vec = CountVectorizer( min_df = 0.90 )\n",
    "mostdocs_X = mostdocs_vec.fit_transform(texts)\n",
    "mostdocs_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostdocs_df = pd.DataFrame(mostdocs_X.toarray(), columns = mostdocs_vec.get_feature_names())\n",
    "# mostdocs_series = mostdocs_df.sum()\n",
    "# mostdocs_series.head(46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last run simply verifies that the exploration above with `min_df` holds true when we switch to `max_df`. That is, that our exploration of the parameters were not asking something unexpected by the code and possibly generating inexplicable and undesirable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 22390)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tnt = topsntails\n",
    "tnt_vec = CountVectorizer( max_df = 0.9, min_df = 2 )\n",
    "tnt_X = tnt_vec.fit_transform(texts)\n",
    "tnt_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers add up. With that done, my recommendation is that we do not, for the time being, throw away common and/or function words: that is, let's not use a stopword list -- at least not in the usual way. Those words could be important in other ways, and dropping them has only a limited impact on the actual document-term matrix, one that pales by comparison to the words that occur only in one document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Doc Wonders <a class=\"anchor\" id=\"wonders\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After establishing words that may have limited usage in terms of separating one group of documents from another but may have other uses, we also need to explore words that only occur in one document and thus, while they could carry a great deal of meaning within that one document, do not have much meaning outside of that document. \n",
    "\n",
    "We first take a look at all words that occur only in one document, and then take a somewhat closer look at words that appear multiply within a given document, attempting to distinguish between nonce words and words that may carry some weight, if only at the local level of the particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Very Peculiar Vector: `max_df = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 17082)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer( max_df = 1)\n",
    "X = vec.fit_transform(texts)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bf           75\n",
       "gk           46\n",
       "telomeres    40\n",
       "abed         39\n",
       "indus        33\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx = pd.DataFrame(X.toarray(), columns = vec.get_feature_names_out())\n",
    "\n",
    "# Create a pandas series of just the terms \n",
    "# and the number of times they occur in total\n",
    "sums = dfx.sum()\n",
    "\n",
    "# Save to CSV\n",
    "#sums.to_csv('../output/one_doc_wonders.csv')\n",
    "\n",
    "# Sort and View\n",
    "sums = sums.sort_values(ascending = False)\n",
    "sums.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words That Appear More Than Once But Only in One Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering a series is just like filtering a dateframe, and here we create a new series with that data and save it to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words that appear more than once but only in one document: 3024.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bf           75\n",
       "gk           46\n",
       "telomeres    40\n",
       "abed         39\n",
       "indus        33\n",
       "fonio        32\n",
       "teszler      30\n",
       "mycelium     29\n",
       "ems          29\n",
       "edi          27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated = sums [ sums > 1]\n",
    "print(f\"The number of words that appear more than once but \\\n",
    "only in one document: {len(repeated)}.\")\n",
    "\n",
    "# Write repeated to CSV:\n",
    "# repeated.to_csv('../output/one_doc_repeated.csv')\n",
    "\n",
    "# The Top 10\n",
    "repeated.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to take see these terms in context. For now, we will convert the list of texts into a single NLTK text, but after that we will want to see in which texts these words occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "onetext = nltk.Text(re.sub(\"[^a-zA-Z0-9']\",\" \",'\\n'.join(texts)).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 39 matches:\n",
      " very ends of chromosomes known as telomeres Now when I started my quest all we\n",
      "tant but I wanted to find out what telomeres consisted of and for that I needed\n",
      "romosomes around 20 000 so lots of telomeres And I discovered that telomeres co\n",
      "of telomeres And I discovered that telomeres consisted of special segments of n\n",
      "cells never got old and died Their telomeres weren't shortening as time marched\n",
      "e that could replenish make longer telomeres and we named it telomerase And whe\n",
      "d our pond scum's telomerase their telomeres ran down and they died So it was t\n",
      "urns out that as we humans age our telomeres do shorten and remarkably that sho\n",
      "Generally speaking the longer your telomeres the better off you are It's the ov\n",
      "you are It's the overshortening of telomeres that leads us to feel and see sign\n",
      "we look and we feel older yeah Our telomeres are losing the war of attrition fa\n",
      "l youthful longer it turns out our telomeres are staying longer for longer peri\n",
      " seems like a no brainer Now if my telomeres are connected to how quickly I'm g\n",
      "'m going to feel and get old if my telomeres can be renewed by my telomerase th\n",
      "hing for us humans in the story of telomeres and their maintenance But I want t\n",
      "ely scrutinizing little teeny tiny telomeres very happily for many years when i\n",
      "th question for me What happens to telomeres in people who are chronically stre\n",
      "l this time I had been thinking of telomeres as those miniscule molecular struc\n",
      "hey are and the genes that control telomeres And when Elissa asked me about stu\n",
      "studying caregivers I suddenly saw telomeres in a whole new light I saw beyond \n",
      "worn down So was it possible their telomeres were worn down as well So our coll\n",
      " to ask What's the length of their telomeres compared with the number of years \n",
      "atter her age the shorter were her telomeres And the more she perceived her sit\n",
      "elomerase and the shorter were her telomeres So we had discovered something unh\n",
      "ess you are under the shorter your telomeres meaning the more likely you were t\n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"telomeres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Observations\n",
    "\n",
    "* Unexplained (artifacts of some kind?): bf, gk, jf, tkm.\n",
    "* Yup, used: telomeres (biological), Indus, fonio (some kind of grain).\n",
    "* Name: Abed, Teszler.\n",
    "* Song lyric: **mzuri** (in a transliterated language)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a class=\"anchor\" id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into consideration all the peculiarities of the TED talks corpus and the strengths and weaknesses of the various ways to process the corpus, we suggest the following setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A preprocessor to handle parentheticals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laughter is the best medicine.   \n",
      "hold your applause; i'm not done yet.  \n"
     ]
    }
   ],
   "source": [
    "# A Refined Preprocessor --\n",
    "# This one removes two-word phrases/clauses\n",
    "\n",
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\",  \n",
    "                  \"\\(video\\)\", \"\\(laughs\\)\", \"\\(applause ends\\)\", \n",
    "                  \"\\(audio\\)\", \"\\(singing\\)\", \"\\(music ends\\)\", \n",
    "                  \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \n",
    "                  \"\\(marimba sounds\\)\", \"\\(drum sounds\\)\" ]\n",
    "\n",
    "def remove_parentheticals(text):\n",
    "    global parentheticals\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), \n",
    "                          flags=re.IGNORECASE)\n",
    "    return new_text\n",
    "\n",
    "test = \"\"\"Laughter is the best medicine. (Laughter) \n",
    "Hold your applause; I'm not done yet. (Applause ends)\"\"\"\n",
    "\n",
    "print(remove_parentheticals(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 22389)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_vec = CountVectorizer( preprocessor = remove_parentheticals,\n",
    "                          max_df = 0.9, min_df = 2 )\n",
    "the_X = the_vec.fit_transform(texts)\n",
    "the_X.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
