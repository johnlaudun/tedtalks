{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing only on talks sponsored by the TED organization itself and not any of the TEDx talks, we have a total of 1747 texts. This is a small corpus, and so the usual reasons for shrinking the feature set for texts do not (necessarily) apply, but as we begin our survey of the contents of the TED talks we want to be mindful of standards that have emerged both so that our results are comparable to the work of others but also so that we could potentially scale up the work here without having to re-think the foundations. At the same time, we can enjoy taking our time to explore various methodological and topical issues that a small corpus presents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "* [Summary](#summary)\n",
    "* [Tokenization(s)](#tokenizations)\n",
    "* [Vectorizer Variations](#variations)\n",
    "  - Frequencies of Terms with a 2-Document Minimum\n",
    "  - Frequencies of Terms Appearing in All/Most Documents\n",
    "  - [One Doc Wonders](#wonders)\n",
    "    - A Very Peculiar Vector: `max_df = 1`\n",
    "    - Words That Appear More Than Once But Only in One Document\n",
    "* [Transcription Artifacts](#artifacts)\n",
    "  - Parentheticals\n",
    "  - Numbers\n",
    "  - Other Artifacts\n",
    "* [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary <a class=\"anchor\" id=\"summary\"></a>\n",
    "\n",
    "Summing the words to get a total for each word across all talks in the dataset, we then hand-inspect the totals and discover that there is an assortment of numbers that recur: an exploration of that phenomenon appears in a separate notebook. The primary task of this notebook is to count the words in the text, which means addressing the matter of parentheticals head on. Much of this work occurs in a separate notebook, but everything comes down to the fact that there are two kinds of parenthetical expressions in TED talks:\n",
    "\n",
    "* those which indicate actions and events outside the speaker's discourse, such as the audience laughing or applauding or the speaker sighing or playing music. \n",
    "* those which somehow the TED transcription practices have discerned as being digressive material within the speech of the author. \n",
    "\n",
    "We want to remove the former and keep the latter. In the end, the decision is made to remove, through `regex` matching, the top 20 parentheticals and to leave the others, most of which occur less than a dozen times. The affect on the overall study is thus reduced significantly.\n",
    "\n",
    "After removing the parentheticals, a second term matrix is derived with a total vocabulary of 50377. (This after discovering a slight bug, or at least weirdness, in `sklearn` that is now in their bug-tracking system.) \n",
    "\n",
    "The final step in this notebook is to raise the threshold for a word to be included in the TF-matrix to 2 documents. With that done, we convert the matrix to a pandas dataframe and then save to CSV. (QUESTION: better/more efficient to save as pickle?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re \n",
    "import nltk\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From our 1747x27 CSV, we have a list of 1747 talks.\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "df = pd.read_csv('../output/TEDall_speakers.csv') # Skipping `index_col='Talk_ID'`\n",
    "\n",
    "# GRAB the talks and create a list of strings\n",
    "texts = df.text.tolist()\n",
    "\n",
    "print(f\"From our {df.shape[0]}x{df.shape[1]} CSV, \\\n",
    "we have a list of {len(texts)} talks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# Loading the Data in a gendered partitioned fashion: \\n\\n# Load binary gendered talks \\ntalks_male = pd.read_csv(\\'talks_male.csv\\', index_col=\\'Talk_ID\\')\\ntalks_female = pd.read_csv(\\'talks_female.csv\\', index_col=\\'Talk_ID\\')\\n\\n# No one gender ==> NOG\\ntalks_nog = pd.read_csv(\\'talks_nog.csv\\', index_col=\\'Talk_ID\\')\\n\\nall_talks = pd.concat([talks_male,talks_female,talks_nog])\\n\\ntexts = all_talks.text.tolist()\\n\\nprint(f\"From our {all_talks.shape[0]}x{all_talks.shape[1]} CSV, we have a list of {len(texts)} talks.\")\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove quotes at top and bottom to uncomment this block\n",
    "\n",
    "\"\"\" \n",
    "# Loading the Data in a gendered partitioned fashion: \n",
    "\n",
    "# Load binary gendered talks \n",
    "talks_male = pd.read_csv('talks_male.csv', index_col='Talk_ID')\n",
    "talks_female = pd.read_csv('talks_female.csv', index_col='Talk_ID')\n",
    "\n",
    "# No one gender ==> NOG\n",
    "talks_nog = pd.read_csv('talks_nog.csv', index_col='Talk_ID')\n",
    "\n",
    "all_talks = pd.concat([talks_male,talks_female,talks_nog])\n",
    "\n",
    "texts = all_talks.text.tolist()\n",
    "\n",
    "print(f\"From our {all_talks.shape[0]}x{all_talks.shape[1]} CSV, \\\n",
    "we have a list of {len(texts)} talks.\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization(s) <a class=\"anchor\" id=\"tokenizations\"></a>\n",
    "\n",
    "Before we can count words and establish frequencies, we need to settle upon what we are going to consider words, which means determining our method of tokenizing our strings of characters into lists of tokens.\n",
    "\n",
    "- The first tokenizer is regex that John has long used in order to keep contractions as single words, but it throws away all other forms of punctuation, including numbers which, as we will see later in this notebook, feature prominently in the TED talk transcripts.\n",
    "- The NLTK's `word_tokenize()` function is based on a TreebankWordTokenizer: basically it tokenizes text like in the Penn Treebank, which means apostrophes break contractions into their distinct parts — e.g., `I'm` becomes `I` + `'m`. Whereas `wordpunct_tokenize()` is a regex that breaks the apostrophes of contractions into their own tokens.\n",
    "- SciKit Learn's tokenization comes up the leanest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex:       2146\n",
      "nltk words:  2576\n",
      "nltk wpunct: 2686\n",
      "scikit:      2035\n"
     ]
    }
   ],
   "source": [
    "# REGEX\n",
    "regex = [word for word in re.sub(\"[^a-zA-Z']\",\" \", texts[0]).lower().split()]\n",
    "\n",
    "# NLTK\n",
    "w_tokens = [word.lower() for word in nltk.word_tokenize(texts[0])]\n",
    "wp_tokens = [word.lower() for word in nltk.wordpunct_tokenize(texts[0])]\n",
    "\n",
    "# SciKit-Learn\n",
    "vectorizer = CountVectorizer( lowercase = True ) # We are vectorizing\n",
    "x = vectorizer.fit_transform([texts[0]])         # the same text as above\n",
    "count = np.sum(x.toarray(), axis = 1)            # then summing the freq count\n",
    "\n",
    "# Print to Compare\n",
    "print(f\"regex:       {len(regex)}\")\n",
    "print(f\"nltk words:  {len(w_tokens)}\")\n",
    "print(f\"nltk wpunct: {len(wp_tokens)}\")\n",
    "print(f\"scikit:      {count[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When examined as the set of tokens making up the word count, the differences do not seem so great, a matter of a little more than a dozen words, as seen in the first cell below, and then in the following cell we take a look at the words involved: it's not clear why the NLTK tokenizer is making hyphenated words as one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METHOD : TOKEN SET\n",
      "regex  :  727\n",
      "NLTK   :  725\n",
      "SciKit :  711\n"
     ]
    }
   ],
   "source": [
    "print(f\"METHOD : TOKEN SET\")\n",
    "print(f\"regex  :  {len(set(regex))}\")\n",
    "print(f\"NLTK   :  {len(set(w_tokens))}\")\n",
    "print(f\"SciKit :  {x.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(', 'i-40', 're-brand', 'short-term', 'end-use', 'g-v', \"'s\", ';', '—', \"'ll\", 'beach-combing', 'a', \"'\", 'rear-view', 'low-cost', '?', 'limited-use', 'ok.', '``', '28-second', 'does', 'click-through', 'u.s.', 'has', \"'m\", \"'ve\", '39.5', ')', ',', 'climatecrisis.net', \"'my\", 'consumer-friendly', 'low-hanging', 'i', ':', 'hot-button', '30-second', 'carbon-neutral', \"'re\", \"'d\", \"'named\", '!', '2.0', \"n't\", 're-purposed', \"''\", '.'}\n"
     ]
    }
   ],
   "source": [
    "difference = set(w_tokens) - set(vectorizer.get_feature_names_out())\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer Variations <a class=\"anchor\" id=\"variations\"></a>\n",
    "\n",
    "With the short run of experiments above solidifying that SciKit-Learn's tokenizer as sufficient, we first run `CountVectorizer` unadorned. \n",
    "\n",
    "The default options are: \n",
    "\n",
    "- lowercase everything, \n",
    "- get rid of all punctuation, \n",
    "- make a word out of anything more than two characters long, \n",
    "- split contractions, and \n",
    "- no stopwords.\n",
    "\n",
    "The tokenizer is not without its problems: while it breaks contractions at the apostrophe, like NLTK, it then throws away anything less than two letters, which means `I'm` disappears entirely. And pity the indefinite article *a(n)*, which is pitched while the definite article *the* remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 50379)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going with the defaults, \n",
    "# so no options/arguments are being passed:\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the model to the data \n",
    "# vecs = vectorizer.fit(texts)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# see how many features we have\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50,379 tokens is our raw, unfiltered, no pre-processing baseline. It includes, as we will see, a number of artifacts of the TED transcription process, including a variety of ways to spell out *ah*, the use of numerals for a host of things -- from dates to counts and everything in between, and some things which actually take place outside the talk itself, like audience laughter, which we here term *parentheticals*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "raw_series = raw_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00             15\n",
       "000          2098\n",
       "000000004       1\n",
       "0000001         1\n",
       "000001          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over two thousand occurrences of `000`! That's unexpected (but not unexplainable). We explore this oddity among other transcription artifacts below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies of Terms with a 2-Document Minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a `min_df = 2` removes all words that appear in only document. (An exploration of *one document wonders* is elsewhere.) As we can see from the `shape` below, we dropped 50379 to 29340, resulting in a reduction of 42% of the possible features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 29343)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min2_vec = CountVectorizer(min_df = 2)\n",
    "min2_X = min2_vec.fit_transform(texts)\n",
    "min2_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above we can sum our terms and write the results to a CSV file for closer inspection. Here, we highlight just a particular series of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "affiliations    3\n",
       "affinity        3\n",
       "affirm          7\n",
       "affirmation     4\n",
       "affirmations    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe\n",
    "min2_df = pd.DataFrame(min2_X.toarray(), columns = min2_vec.get_feature_names_out())\n",
    "\n",
    "# Create a series of sums\n",
    "min2_series = min2_df.sum()\n",
    "\n",
    "# Write sums to CSV:\n",
    "# min2_series.to_csv('../output/word_freq_clean.csv')\n",
    "\n",
    "# Inspect the sums\n",
    "min2_series[1000:1005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies of Terms Appearing in All/Most Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trimmed terms that occur in only document from our feature set, we also need to determine what words occur across all the documents and thus are not particularly effective discriminators of topic. \n",
    "\n",
    "Note that this is a somewhat more dangerous move to make, since it has been shown that one of the keys to genre and gender in documents are the varying uses of so-called function words. In fact, as our explorations establish, using sklearn's built-in algorithms reveals a focus on removing common words does not really result in much of a reduction in the overall feature set, only 8 words occurring across 99% or more documents and that number only rising to 46 when we change the minimum threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs_vec = CountVectorizer( min_df = 0.99 )\n",
    "alldocs_X = alldocs_vec.fit_transform(texts)\n",
    "alldocs_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and    118989\n",
       "in      62673\n",
       "is      50072\n",
       "it      59191\n",
       "of      92416\n",
       "the    166093\n",
       "to     102276\n",
       "you     56296\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs_df = pd.DataFrame(alldocs_X.toarray(), columns = alldocs_vec.get_feature_names_out())\n",
    "alldocs_series = alldocs_df.sum()\n",
    "alldocs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 47)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostdocs_vec = CountVectorizer( min_df = 0.90 )\n",
    "mostdocs_X = mostdocs_vec.fit_transform(texts)\n",
    "mostdocs_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostdocs_df = pd.DataFrame(mostdocs_X.toarray(), columns = mostdocs_vec.get_feature_names())\n",
    "# mostdocs_series = mostdocs_df.sum()\n",
    "# mostdocs_series.head(46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last run simply verifies that the exploration above with `min_df` holds true when we switch to `max_df`. That is, that our exploration of the parameters were not asking something unexpected by the code and possibly generating inexplicable and undesirable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 29296)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tnt = topsntails\n",
    "tnt_vec = CountVectorizer( max_df = 0.9, min_df = 2 )\n",
    "tnt_X = tnt_vec.fit_transform(texts)\n",
    "tnt_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers add up. With that done, my recommendation is that we do not, for the time being, throw away common and/or function words: that is, let's not use a stopword list -- at least not in the usual way. Those words could be important in other ways, and dropping them has only a limited impact on the actual document-term matrix, one that pales by comparison to the words that occur only in one document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Doc Wonders <a class=\"anchor\" id=\"wonders\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After establishing words that may have limited usage in terms of separating one group of documents from another but may have other uses, we also need to explore words that only occur in one document and thus, while they could carry a great deal of meaning within that one document, do not have much meaning outside of that document. \n",
    "\n",
    "We first take a look at all words that occur only in one document, and then take a somewhat closer look at words that appear multiply within a given document, attempting to distinguish between nonce words and words that may carry some weight, if only at the local level of the particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Very Peculiar Vector: `max_df = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 21036)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer( max_df = 1)\n",
    "X = vec.fit_transform(texts)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bf           75\n",
       "gk           46\n",
       "telomeres    40\n",
       "abed         39\n",
       "mzuri        36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx = pd.DataFrame(X.toarray(), columns = vec.get_feature_names_out())\n",
    "\n",
    "# Create a pandas series of just the terms \n",
    "# and the number of times they occur in total\n",
    "sums = dfx.sum()\n",
    "\n",
    "# Save to CSV\n",
    "#sums.to_csv('../output/one_doc_wonders.csv')\n",
    "\n",
    "# Sort and View\n",
    "sums = sums.sort_values(ascending = False)\n",
    "sums.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words That Appear More Than Once But Only in One Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering a series is just like filtering a dateframe, and here we create a new series with that data and save it to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words that appear more than once but only in one document: 3894.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bf           75\n",
       "gk           46\n",
       "telomeres    40\n",
       "abed         39\n",
       "mzuri        36\n",
       "jf           36\n",
       "indus        33\n",
       "fonio        32\n",
       "teszler      30\n",
       "tkm          30\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated = sums [ sums > 1]\n",
    "print(f\"The number of words that appear more than once but \\\n",
    "only in one document: {len(repeated)}.\")\n",
    "\n",
    "# Write repeated to CSV:\n",
    "# repeated.to_csv('../output/one_doc_repeated.csv')\n",
    "\n",
    "# The Top 10\n",
    "repeated.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to take see these terms in context. For now, we will convert the list of texts into a single NLTK text, but after that we will want to see in which texts these words occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "onetext = nltk.Text(re.sub(\"[^a-zA-Z0-9']\",\" \",'\\n'.join(texts)).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 30 matches:\n",
      "w Laughter Siphumeze Khundayi Hi guys TKM Hello everyone TKM So you think you k\n",
      "e Khundayi Hi guys TKM Hello everyone TKM So you think you know about sex Chanc\n",
      "e going to keep things safe and spicy TKM So now the act of rubbing our naked b\n",
      "are going to live our best sexy lives TKM And we're going to tell you how to ha\n",
      "out the things that we need to change TKM And the things we need to embrace in \n",
      " From the top of my head rape culture TKM How tradition and culture limit ideas\n",
      "hen he's trying to turn up the volume TKM Like that is a personal pet peeve of \n",
      "ve of mine SK We are so scared of sex TKM And we need somebody to blame for our\n",
      " see how well that goes down Laughter TKM Does not go down well I once challeng\n",
      "at you saw on the internet by mistake TKM Mhm So now in order to cure this ailm\n",
      "ach us to help us upgrade the present TKM So now if I had a glass of Merlot whi\n",
      "t be named SK Whispering Colonization TKM Came through Within African societies\n",
      "es of old were so important for women TKM There were African sexual practices t\n",
      " in particular that's named osunality TKM Also known as the African erotic Yes \n",
      "women and creating pleasure for women TKM Mhm Shout out to the Kama Sutra but b\n",
      "elfconsciously And she goes on to say TKM There you go you got that line this t\n",
      "n about the power of this inner force TKM So within the African continent there\n",
      " are all trying to subvert each other TKM We are pounding the pussy using sex a\n",
      "ays a loser when it comes to this war TKM So now the ability to openly brandish\n",
      "e it begins to get really really good TKM So what does it mean to reconceptuali\n",
      "exual oppression and erotic injustice TKM And sex positivity is one of the real\n",
      "ica SK We do sex and sexuality online TKM We would be foolhardy not to mention \n",
      "lingus SK I like the word cunnilingus TKM I bet you do But that's not the point\n",
      " we don't end up in sticky situations TKM That's true And another space that we\n",
      " does not understand consent Laughter TKM So one fascinating subset of kink is \n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"tkm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Observations\n",
    "\n",
    "* Unexplained (artifacts of some kind?): bf, gk, jf, tkm.\n",
    "* Yup, used: telomeres (biological), Indus, fonio (some kind of grain).\n",
    "* Name: Abed, Teszler.\n",
    "* Song lyric: **mzuri** (in a transliterated language)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription Artifacts <a class=\"anchor\" id=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hand inspection of the raw output above turned up a couple of interesting issues: \n",
    "\n",
    "* that there are **parentheticals** mixed in with the text of the talks,\n",
    "* that **numbers** feature in the talks regularly,\n",
    "* and that there are also some interesting tokenization practices that emerge out of a crowd-sourced transcription technology.\n",
    "\n",
    "One example of \"interesting tokenization\" is the number of ways that the pause filler *ah* can be transcribed:\n",
    "\n",
    "| Term    | Freq |\n",
    "|:---------|---:|\n",
    "|aa       |12 |\n",
    "|aaa      |7  |\n",
    "|aaaa     |2  |\n",
    "|aaaaa    |1  |\n",
    "|aaaaaaaah|1  |\n",
    "|aaaaaaah |1  |\n",
    "|aaaaaah  |3  |\n",
    "|aaaah    |2  |\n",
    "|aaaahhh  |1  |\n",
    "|aaah     |3  |\n",
    "|aag      |1  |\n",
    "|aah      |10 |\n",
    "\n",
    "There are an equal number of alternate spellings for *shh*. \n",
    "\n",
    "There's also this oddity:\n",
    "\n",
    "| Term    | Freq |\n",
    "|:---------|---:|\n",
    "|FALSE    |115|\n",
    "|TRUE     |909|\n",
    "\n",
    "It appears to be simply the occurrences of *true* and *false* but somehow the vectorizer thinks they are Booleans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parentheticals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier explorations of the corpus revealed something we knew but had not realized could affect our work: some TED talks are not talks but musical performances. Generally, the text of such performances are rather short. Using an arbitrary length of `500` characters, we can see what these texts look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Applause)    (Music)    (Applause)  \n",
      "  Let's just get started here.    Okay, just a moment.    (Whirring)    All right. (Laughter) Oh, sorry.    (Music) (Beatboxing)    Thank you.    (Applause)  \n",
      "  (Music)    (Applause)    (Music)    (Music) (Applause)    (Music) (Applause) (Applause)    Herbie Hancock: Thank you. Marcus Miller. (Applause) Harvey Mason. (Applause)    Thank you. Thank you very much. (Applause)  \n",
      "  (Music)    (Applause)    (Music)    (Applause)  \n",
      "  (Music)    (Applause)    (Music)    (Applause)    (Music)    (Applause)    (Music)    (Applause)  \n",
      "  (Music)    (Music) (Applause)    (Applause)  \n",
      "  (Guitar music starts)    (Cheers)    (Cheers)    (Music ends)  \n",
      "  (Music)    (Applause)  \n",
      "  (Guitar music starts)    (Music ends)    (Applause)    (Distorted guitar music starts)    (Music ends)    (Applause)    (Ambient/guitar music starts)    (Music ends)    (Applause)  \n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    if len(text) < 500:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113, 235, 382, 496, 573, 799, 899, 1484, 1564]\n"
     ]
    }
   ],
   "source": [
    "# Text Indices\n",
    "shorts = [ texts.index(text) for text in texts if len(text) < 500 ]\n",
    "print(shorts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes time to process words in a text, our best bet will be to remove the parentheticals, though, having them means we can possibly explore sentiment using `(Applause)` and `(Laughter)` as contextual valuations.\n",
    "\n",
    "For now, we will need some regex to remove the parentheses and their contents from our texts. An examination of `113` above reveals that it is only three parenthetical expressions:\n",
    "\n",
    "    (Applause)    (Music)    (Applause)\n",
    "\n",
    "We need a sample text that is a mix, and so we will use `235`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Let's just get started here.    Okay, just a moment.    (Whirring)    All right. (Laughter) Oh, sorry.    (Music) (Beatboxing)    Thank you.    (Applause)  \n"
     ]
    }
   ],
   "source": [
    "print(texts[235])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different regexes give us one list without the parentheses and one with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whirring', 'Laughter', 'Music', 'Beatboxing', 'Applause']\n",
      "['(Whirring)', '(Laughter)', '(Music)', '(Beatboxing)', '(Applause)']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'(?<=\\().*?(?=\\))', texts[235]))\n",
    "print(re.findall(r'\\([^)]*\\)', texts[235]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells we use the regex pattern extablished above on a subset of the larger list, bracketing a text, `235` known to have parentheticals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parenvec = CountVectorizer(token_pattern = r'(?<=\\().*?(?=\\))')\n",
    "parentheticals = parenvec.fit_transform(texts[230:240])\n",
    "parentheticals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>applause</th>\n",
       "      <th>beatboxing</th>\n",
       "      <th>laugher</th>\n",
       "      <th>laughter</th>\n",
       "      <th>music</th>\n",
       "      <th>whirring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     applause  beatboxing  laugher  laughter  music  whirring\n",
       "230         1           0        0         2      0         0\n",
       "231         0           0        0         0      0         0\n",
       "232         3           0        0        10      7         0\n",
       "233         3           0        1        10      0         0\n",
       "234         0           0        0         9      0         0\n",
       "235         1           1        0         1      1         1\n",
       "236         3           0        0        12      0         0\n",
       "237         0           0        0         2      0         0\n",
       "238         1           0        0         6      0         0\n",
       "239         1           0        0        25      0         0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(parentheticals.toarray(), columns=parenvec.get_feature_names_out())\n",
    "df.index = [ texts.index(text) for text in texts[230:240] ]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the regex pattern to the entire list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1747, 620)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"actually about ... 1%\"</th>\n",
       "      <th>\"although it's nothing serious, let's keep an eye on it to make sure it doesn't turn into a major lawsuit.\"</th>\n",
       "      <th>\"close it!\"</th>\n",
       "      <th>\"do architects have ears?\"</th>\n",
       "      <th>\"i sold my soul for about a tenth of what the damn things are going for now.\"</th>\n",
       "      <th>\"in order to remain competitive in today's marketplace, i'm afraid we're going to have to replace you with a sleezeball.\"</th>\n",
       "      <th>\"intrigue and murder among 16th century ottoman court painters.\"</th>\n",
       "      <th>\"kill him.\"</th>\n",
       "      <th>\"michael crichton responds by fax:\"</th>\n",
       "      <th>\"sure\"</th>\n",
       "      <th>...</th>\n",
       "      <th>whistling</th>\n",
       "      <th>whoosh</th>\n",
       "      <th>with 4 attempts</th>\n",
       "      <th>woman screaming</th>\n",
       "      <th>woman: have you ever done a kissing test before?</th>\n",
       "      <th>woman: okay.</th>\n",
       "      <th>woo-hoo-hoo-hoo</th>\n",
       "      <th>xylophone</th>\n",
       "      <th>yelling more loudly</th>\n",
       "      <th>your fathers bristles white and stiff now</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 620 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   \"actually about ... 1%\"  \\\n",
       "0                        0   \n",
       "1                        0   \n",
       "2                        0   \n",
       "3                        0   \n",
       "4                        0   \n",
       "5                        0   \n",
       "6                        0   \n",
       "7                        0   \n",
       "8                        0   \n",
       "9                        0   \n",
       "\n",
       "   \"although it's nothing serious, let's keep an eye on it to make sure it doesn't turn into a major lawsuit.\"  \\\n",
       "0                                                  0                                                             \n",
       "1                                                  0                                                             \n",
       "2                                                  0                                                             \n",
       "3                                                  0                                                             \n",
       "4                                                  0                                                             \n",
       "5                                                  0                                                             \n",
       "6                                                  0                                                             \n",
       "7                                                  0                                                             \n",
       "8                                                  0                                                             \n",
       "9                                                  0                                                             \n",
       "\n",
       "   \"close it!\"  \"do architects have ears?\"  \\\n",
       "0            0                           0   \n",
       "1            0                           0   \n",
       "2            0                           0   \n",
       "3            0                           0   \n",
       "4            0                           0   \n",
       "5            0                           0   \n",
       "6            0                           0   \n",
       "7            0                           0   \n",
       "8            0                           0   \n",
       "9            0                           0   \n",
       "\n",
       "   \"i sold my soul for about a tenth of what the damn things are going for now.\"  \\\n",
       "0                                                  0                               \n",
       "1                                                  0                               \n",
       "2                                                  0                               \n",
       "3                                                  0                               \n",
       "4                                                  0                               \n",
       "5                                                  0                               \n",
       "6                                                  0                               \n",
       "7                                                  0                               \n",
       "8                                                  0                               \n",
       "9                                                  0                               \n",
       "\n",
       "   \"in order to remain competitive in today's marketplace, i'm afraid we're going to have to replace you with a sleezeball.\"  \\\n",
       "0                                                  0                                                                           \n",
       "1                                                  0                                                                           \n",
       "2                                                  0                                                                           \n",
       "3                                                  0                                                                           \n",
       "4                                                  0                                                                           \n",
       "5                                                  0                                                                           \n",
       "6                                                  0                                                                           \n",
       "7                                                  0                                                                           \n",
       "8                                                  0                                                                           \n",
       "9                                                  0                                                                           \n",
       "\n",
       "   \"intrigue and murder among 16th century ottoman court painters.\"  \\\n",
       "0                                                  0                  \n",
       "1                                                  0                  \n",
       "2                                                  0                  \n",
       "3                                                  0                  \n",
       "4                                                  0                  \n",
       "5                                                  0                  \n",
       "6                                                  0                  \n",
       "7                                                  0                  \n",
       "8                                                  0                  \n",
       "9                                                  0                  \n",
       "\n",
       "   \"kill him.\"  \"michael crichton responds by fax:\"  \"sure\"  ...  whistling  \\\n",
       "0            0                                    0       0  ...          0   \n",
       "1            0                                    0       0  ...          0   \n",
       "2            0                                    0       0  ...          0   \n",
       "3            0                                    0       0  ...          0   \n",
       "4            0                                    0       0  ...          0   \n",
       "5            0                                    0       0  ...          0   \n",
       "6            0                                    0       0  ...          0   \n",
       "7            0                                    0       0  ...          0   \n",
       "8            0                                    0       0  ...          0   \n",
       "9            0                                    0       0  ...          0   \n",
       "\n",
       "   whoosh  with 4 attempts  woman screaming  \\\n",
       "0       0                0                0   \n",
       "1       0                0                0   \n",
       "2       0                0                0   \n",
       "3       0                0                0   \n",
       "4       0                0                0   \n",
       "5       0                0                0   \n",
       "6       0                0                0   \n",
       "7       0                0                0   \n",
       "8       0                0                0   \n",
       "9       0                0                0   \n",
       "\n",
       "   woman: have you ever done a kissing test before?  woman: okay.  \\\n",
       "0                                                 0             0   \n",
       "1                                                 0             0   \n",
       "2                                                 0             0   \n",
       "3                                                 0             0   \n",
       "4                                                 0             0   \n",
       "5                                                 0             0   \n",
       "6                                                 0             0   \n",
       "7                                                 0             0   \n",
       "8                                                 0             0   \n",
       "9                                                 0             0   \n",
       "\n",
       "   woo-hoo-hoo-hoo  xylophone  yelling more loudly  \\\n",
       "0                0          0                    0   \n",
       "1                0          0                    0   \n",
       "2                0          0                    0   \n",
       "3                0          0                    0   \n",
       "4                0          0                    0   \n",
       "5                0          0                    0   \n",
       "6                0          0                    0   \n",
       "7                0          0                    0   \n",
       "8                0          0                    0   \n",
       "9                0          0                    0   \n",
       "\n",
       "   your fathers bristles white and stiff now  \n",
       "0                                          0  \n",
       "1                                          0  \n",
       "2                                          0  \n",
       "3                                          0  \n",
       "4                                          0  \n",
       "5                                          0  \n",
       "6                                          0  \n",
       "7                                          0  \n",
       "8                                          0  \n",
       "9                                          0  \n",
       "\n",
       "[10 rows x 620 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parentheticals = parenvec.fit_transform(texts)\n",
    "print(parentheticals.shape)\n",
    "\n",
    "df_parens = pd.DataFrame(parentheticals.toarray(), columns=parenvec.get_feature_names_out())\n",
    "df_parens.index = [ texts.index(text) for text in texts ]\n",
    "df_parens.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we discover more TED randomness! \n",
    "\n",
    "Let's determine the top parentheticals, take a look at their numbers, and then decide what's the best path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "laughter            7275\n",
       "applause            4271\n",
       "music                424\n",
       "video                269\n",
       "laughs                41\n",
       "audio                 37\n",
       "applause ends         37\n",
       "singing               35\n",
       "music ends            33\n",
       "cheers                30\n",
       "cheering              21\n",
       "recording             19\n",
       "beatboxing            18\n",
       "audience              15\n",
       "guitar strum          14\n",
       "clicks metronome      13\n",
       "sighs                 13\n",
       "guitar                13\n",
       "drum sounds           10\n",
       "marimba sounds        10\n",
       "voice-over             8\n",
       "clicking               8\n",
       "sings                  8\n",
       "clapping               7\n",
       "in chinese             7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums = df_parens.sum(axis = 0)\n",
    "sums.sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the top 20 parentheticals could be inserted into a stopword list and we would remove, in the case of the top 4 especially, words or clauses that might affect results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parentheticals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further hand inspection revealed that there is a a reasonable amount of speaker discourse being parenthesized, making removing all such parenthetical material less than optimal. However, the parentheticals appear to follow a Zipf distribution, and so we can effectively remove 80% of the parentheticals with a relatively small number of them either included in a stop word list or fed to the vectorizer as a pre-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parenthetical Token List\n",
    "parentheticals = [ \"(laughter)\", \"(applause ends)\", \"(applause)\", \"(music)\", \n",
    "                  \"(video)\", \"(laughs)\", \"(audio)\", \"(singing)\", \"(music ends)\", \n",
    "                  \"(cheers)\", \"(cheering)\", \"(recording)\", \"(beatboxing)\", \n",
    "                  \"(audience)\", \"(guitar strum)\", \"(clicks metronome)\", \"(sighs)\", \n",
    "                  \"(guitar)\", \"(marimba sounds)\", \"(drum sounds)\" ]\n",
    "\n",
    "def remove_parentheticals (text):\n",
    "    result = ' '.join([ word for word in re.sub(\"[^a-zA-Z0-9'()]\",\" \", text).lower().split() \n",
    "              if word not in parentheticals ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells that follow, we create a test string that has two terms, *laughter* and *applause*, both in the speech of the imaginary speaker as well as in a parenthetical. We want to keep the former and remove the latter.\n",
    "\n",
    "Then we develop a function, `clean_text`, that goes through the list of terms to be removed as it goes through a text. This is a loop within a loop, and I am not sure if this is the most efficient way to do this, but it works. \n",
    "\n",
    "We then test the `clean_text` function on the `test` string and then on one of the talks we know is mostly a performance. The results, at long last, are what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laughter is the best medicine hold your applause i'm not done yet (applause ends)\n"
     ]
    }
   ],
   "source": [
    "test = \"\"\"Laughter is the best medicine. (Laughter) \n",
    "Hold your applause; I'm not done yet. (Applause ends)\"\"\"\n",
    "\n",
    "print(remove_parentheticals(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success in this small-scale experiment here: `(laughter)` has been removed and `laughter` remains. Unfortunately, `(applause ends)` also remains. (José Blanco's [\"Hacking Scikit-Learn’s Vectorizers\"](https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af) may hold some ideas here.)\n",
    "\n",
    "`'(mock', 'sob)'` is problematic. And, also a bit of a problem, `sklearn` expects input as strings -- I have not found a way to bypass this, save writing custom preprocessors or tokenizers like the one above.\n",
    "\n",
    "A simpler approach might be to feed the vectorizer the stopped parenthetical tokens, but when we tried this we got the following error:\n",
    "\n",
    "```\n",
    "UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['applause', 'audience', 'audio', 'beatboxing', 'cheering', 'cheers', 'clicks', 'drum', 'ends', 'guitar', 'laughs', 'laughter', 'marimba', 'metronome', 'music', 'recording', 'sighs', 'singing', 'sounds', 'strum', 'video'] not in stop_words.\n",
    "  % sorted(inconsistent)\n",
    "```\n",
    "\n",
    "Removing multiple words remains unresolved at this time, but the frequency is low enough that we think it can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers\n",
    "\n",
    "One of the dimensions of the corpus that arises out of a hand inspection of the terms is the frequency with which some numbers appear. The follow table captures the top ten numbers:\n",
    "\n",
    "| TERM | FREQUENCY |\n",
    "|------|-----------|\n",
    "| 000  | 2098 |\n",
    "| 10   | 1691 |\n",
    "|  20  | 1107 |\n",
    "| 100  |  902 |\n",
    "|  30  |  827 |\n",
    "|  50  |  784 |\n",
    "|  15  |  659 | \n",
    "|  40  |  494 |\n",
    "|  12  |  460 | \n",
    "|  25  |  410 |\n",
    "\n",
    "Other frequently occurring numbers: 60, 500, 200, 11, 18, 80, 14 (241 times!). \n",
    "\n",
    "In order to examine the appearance of the numbers in context, we make a giant string out of the list of strings, `texts`: in which text a number appears is less important than its immediate context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank', 'you', 'so', 'much,', 'Chris.', 'And', \"it's\", 'truly', 'a', 'great']\n"
     ]
    }
   ],
   "source": [
    "onetext = nltk.Text('\\n'.join(texts).split())\n",
    "# And here's what an NLTK text object looks like: a list of words, really\n",
    "print(onetext[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers: Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"000\") # Best guess for lack of matches: doesn't stand alone as a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 1216 matches:\n",
      "Thank you very much. (Applause) About 10 years ago, I took on the task to teac\n",
      "tion of income of people. One dollar, 10 dollars or 100 dollars per day. There\n",
      " a long time, but they come out after 10 years very, very differently. And the\n",
      "at drives you in your life today? Not 10 years ago. Are you running the same p\n",
      "really heavy, but in the last five or 10 years, have there been some decisions\n",
      ". (Laughter) Are you sure? (Laughter) 10 seconds! (Laughter and applause) 10 s\n",
      ") 10 seconds! (Laughter and applause) 10 seconds, I want to be respectful. All\n",
      "principle in the Bible that says give 10 percent of what you get back to chari\n",
      "ional shelter that would last five to 10 years, that would be placed next to t\n",
      "tandards of five billion people? With 10 million solutions. So I wish to devel\n",
      " to go see Central Command, which was 10 minutes away. And that way, I could g\n",
      " will not launch this without five to 10 million units in the first run. And t\n",
      " down, and that's why I said seven to 10 million there. And we're doing it wit\n",
      " your laptop costs, in rough numbers, 10 dollars a diagonal inch. That can dro\n",
      "s; I can just go right up and use all 10 fingers if I wanted to. You know, lik\n",
      "ade by hand, and where, when you work 10 hours a day, you're still only earnin\n",
      "years; you have to be looking five or 10 years ahead. But I think with the vis\n",
      "ll from Earthlink that said, due to a 10 cents per megabyte overage charge, I \n",
      "umcised against her will when she was 10 years old, and she really made a deci\n",
      "and they simply point and they go for 10 percent, right in the middle. Howard \n",
      "ss in this country. And over the next 10 years, they made 600 million dollars \n",
      " you today is that, in fact, based on 10 years of research, a unique opportuni\n",
      "in the housing project on and off for 10 years, hanging out in crack houses, g\n",
      " marathons back to back. 800 miles in 10 weeks. And I was dragging all the foo\n",
      "week. He described this expedition as 10 times as dangerous as Everest. So for\n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 387 matches:\n",
      "w York City already handled more than 40 percent of the entire city's commerci\n",
      "ing rooms, whose evolution in 20, 30, 40 years we can't predict. So that liter\n",
      "nd all the other teams have done this 40 Days of Purpose, based on the book. A\n",
      "nternet tools, and we ended up having 40 chapters starting up, thousands of ar\n",
      "cumented the Lower Ninth for the last 40 years. That was their home, and these\n",
      "me. And a long time ago — well, about 40 years ago — my mom had an exchange st\n",
      " world where women and children spend 40 billion hours a year fetching water. \n",
      " age category of 76 to 85, as much as 40 percent of people have nothing really\n",
      "things tend to happen every 25 years. 40 years long, with an overlap. You can \n",
      " all high-rises. So they'll put 20 or 40 up at a time, and they just go up in \n",
      "te, we've seen no side effects in the 40 or so patients in whom it's been impl\n",
      " terms of price performance, that's a 40 to 50 percent deflation rate. And eco\n",
      " people may increase their volume 30, 40 percent, but they won't keep up with \n",
      "r hunters could smell animal urine at 40 paces and tell you what species left \n",
      "percent of all the pregnancies. About 40 percent of all the people — I said 40\n",
      "I said 400,000. I meant 40,000. About 40 percent of all the people who need TB\n",
      "tt-hours per kilogram nickel-cadmium, 40 watt-hours per kilogram in nickel-met\n",
      "n 60 in captivity, so we've only done 40 years in the wild so far. And we find\n",
      "ater. Imagine what the world was like 40 years ago, with just the Saturday Eve\n",
      " to India. India and China used to be 40 percent of the global economy just at\n",
      "ongo River. Canopied trees as tall as 40 meters, 130 feet, grow densely in the\n",
      "Hong Kong, with six million people in 40 square miles. During the dry season, \n",
      " farming the same piece of ground for 40 centuries. You can't farm the same pi\n",
      "n't farm the same piece of ground for 40 centuries without understanding nutri\n",
      "o measure it — but it's anywhere from 40 to 60 years. It goes on a long time. \n"
     ]
    }
   ],
   "source": [
    "onetext.concordance(\"40\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to note here:\n",
    "\n",
    "First, there is a discrepancy in the count between `sklearn` and the NLTK: the former counted 2098 occurrences of `000`, the latter none. In all the counts that follow, there is a similar mismatch:\n",
    "\n",
    "| TERM | `sklearn` | `nltk` |\n",
    "|------|-----------|--------|\n",
    "| 000  | 2098 | \"no match\" |\n",
    "|  10  | 1691 | 1216 |\n",
    "|  20  | 1107 | 879 |\n",
    "| 100  |  902 | 647 |\n",
    "|  30  |  827 | 650 |\n",
    "|  50  |  784 | 594 | \n",
    "|  15  |  659 | 512 | \n",
    "|  40  |  494 | 387 | \n",
    "| ...               | \n",
    "|  14  |  241 | 148 | \n",
    "\n",
    "I don't have a ready explanation for this, except that there is a difference between the two tokenizers in how they parse a number like `1,000`.\n",
    "\n",
    "Second, the frequency of some numbers are readily explained:\n",
    "\n",
    "* Round numbers like 10, 20, 30, 50, and 100 are approximations -- though it would be interesting to explore how often they are attached to large scalars like \"thousand\" or million.\" \n",
    "* Some numbers seem to represent alternate ways of counting: 25 reagularly stands in for \"one-quarter\" -- though not as often as we might imagine -- and 18 is regularly paired with *month* as a more precise way to say \" a year and a half.\"\n",
    "* There are some numbers, like 11 and 14 which seem to have power all their own, perhaps tied to particular ages in humans. \n",
    "\n",
    "Next up is some code to explore the most common occurring words with these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All my searches for \"collocations with specific words\" took me to the NLTK, which means, so far as I can tell, generating all the bigrams and then filtering to get the one(s) you want. This seems backwards to me: wouldn't it be faster simply to find the word and then what comes after it? I'll take a look at regex for this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bigrams\n",
    "finder = BigramCollocationFinder.from_words(onetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('14', 'years'), ('14', 'billion'), ('was', '14'), ('14', 'years,'), ('14', 'hours'), ('14', 'orders'), ('14', 'million'), ('14', 'percent'), ('14', 'feet'), ('14', 'times')]\n"
     ]
    }
   ],
   "source": [
    "## Here's the filter operation:\n",
    "the_number = lambda *w: '14' not in w\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain the number\n",
    "finder.apply_ngram_filter(the_number)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not return a count. *Oi!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers: Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank', 'you', 'so', 'much', 'chris', 'and', \"it's\", 'truly', 'a', 'great']\n"
     ]
    }
   ],
   "source": [
    "the_one = nltk.Text(re.sub(\"[^a-zA-Z0-9']\",\" \",'\\n'.join(texts)).lower().split())\n",
    "# And here's what an NLTK text object looks like: a list of words, really\n",
    "print(the_one[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 494 matches:\n",
      "oking for a place to eat we were on i 40 we got to exit 238 lebanon tennessee \n",
      "w york city already handled more than 40 percent of the entire city's commerci\n",
      "eading rooms whose evolution in 20 30 40 years we can't predict so that litera\n",
      "nd all the other teams have done this 40 days of purpose based on the book and\n",
      "internet tools and we ended up having 40 chapters starting up thousands of arc\n",
      "cumented the lower ninth for the last 40 years that was their home and these a\n",
      "e time and a long time ago well about 40 years ago my mom had an exchange stud\n",
      " world where women and children spend 40 billion hours a year fetching water t\n",
      "be someone coming to rescue me cut to 40 some odd years later we go to kenya a\n",
      "t age category of 76 to 85 as much as 40 percent of people have nothing really\n",
      " is how do you go to the loo at minus 40 ben i've read somewhere that at minus\n",
      "ben i've read somewhere that at minus 40 exposed skin becomes frostbitten in l\n",
      "ou answer the call of nature at minus 40 the answer of course to which is a tr\n",
      " things tend to happen every 25 years 40 years long with an overlap you can pu\n",
      "e all high rises so they'll put 20 or 40 up at a time and they just go up in t\n",
      "se my prize so i would take 30 000 or 40 000 dollars of the winnings and the r\n",
      "ate we've seen no side effects in the 40 or so patients in whom it's been impl\n",
      "n terms of price performance that's a 40 to 50 percent deflation rate and econ\n",
      "n people may increase their volume 30 40 percent but they won't keep up with i\n",
      "r hunters could smell animal urine at 40 paces and tell you what species left \n",
      " percent of all the pregnancies about 40 percent of all the people i said 400 \n",
      "all the people i said 400 000 i meant 40 000 about 40 percent of all the peopl\n",
      "e i said 400 000 i meant 40 000 about 40 percent of all the people who need tb\n",
      "att hours per kilogram nickel cadmium 40 watt hours per kilogram in nickel met\n",
      "lture what we find is that over these 40 odd years that i and others have been\n"
     ]
    }
   ],
   "source": [
    "the_one.concordance(\"40\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there's the missing `000`! It's in the idiomatic transcription practices of TED wherein a number like \"sixty thousand\" is rendered as \"60,000.\" \n",
    "\n",
    "One thing we know now: reporting large numbers is a part of TED talks.\n",
    "\n",
    "**TO DO**: How to keep the comma marker between numbers? (Or should we just look to 000 as a possible collocate with the other numbers?) One solution from the [Regex Cookbook][]:\n",
    "\n",
    "```python\n",
    "\\b[0-9]{1,3}(,[0-9]{3})*(\\.[0-9]+)?\\b|\\.[0-9]+\\b\n",
    "```\n",
    "\n",
    "[Regex Cookbook]: https://www.oreilly.com/library/view/regular-expressions-cookbook/9781449327453/ch06s11.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hand inspection of the output above turned up a couple of interesting issues: that there are parentheticals mixed in with the text of the talks (see 01-Terms-02-Parentheticals) and numbers feature in the talks regularly (see 01-Terms-03-Numbers). \n",
    "\n",
    "There are also some interesting tokenization dimensions to a crowd-sourced transcription technology:\n",
    "\n",
    "| Term    | Freq |\n",
    "|:---------|---:|\n",
    "|aa       |12 |\n",
    "|aaa      |7  |\n",
    "|aaaa     |2  |\n",
    "|aaaaa    |1  |\n",
    "|aaaaaaaah|1  |\n",
    "|aaaaaaah |1  |\n",
    "|aaaaaah  |3  |\n",
    "|aaaah    |2  |\n",
    "|aaaahhh  |1  |\n",
    "|aaah     |3  |\n",
    "|aag      |1  |\n",
    "|aah      |10 |\n",
    "\n",
    "There are an equal number of alternate spellings for *shh*. There's also this oddity:\n",
    "\n",
    "| Term    | Freq |\n",
    "|:---------|---:|\n",
    "|FALSE    |115|\n",
    "|TRUE     |909|\n",
    "\n",
    "It appears to be simply the occurrences of *true* and *false* but somehow the vectorizer thinks they are Booleans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the hand-examination turns up no other issues, so the basic vectorization built into `sklearn` appears to be satisfactory, with the only exception being its breaking of contractions at the apostrophe: e.g., *isn't* becomes *isn* and *'t*, with the latter being thrown away as too small -- which means that the indefinite article *a* is also not present in the frequencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section that follows, we take the array holding all the talks and their terms and transpose it so that the terms are on the rows. We then sum the rows and turn the sum column into a series which is then sorted and saved to CSV for hand inspection. What the hand inspection revealed was, beyond the usual stopwords, there were enough terms in the top 300 terms that had semantic possibilities that they should be kept. The best bet moving forward is to use an available, and widely-used, stopword list. We chose the one provided by the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21036, 1747)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix = pd.DataFrame(X.todense(), columns=vec.get_feature_names_out())\n",
    "tfdf = term_matrix.transpose()\n",
    "tfdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1737</th>\n",
       "      <th>1738</th>\n",
       "      <th>1739</th>\n",
       "      <th>1740</th>\n",
       "      <th>1741</th>\n",
       "      <th>1742</th>\n",
       "      <th>1743</th>\n",
       "      <th>1744</th>\n",
       "      <th>1745</th>\n",
       "      <th>1746</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000000004</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000042</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00046</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0015</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>004</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1747 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "000000004     0     0     0     0     0     0     0     0     0     0  ...   \n",
       "0000001       0     0     0     0     0     0     0     0     0     0  ...   \n",
       "000001        0     0     0     0     0     0     0     0     0     0  ...   \n",
       "00001         0     0     0     0     0     0     0     0     0     0  ...   \n",
       "000042        0     0     0     0     0     0     0     0     0     0  ...   \n",
       "00046         0     0     0     0     0     0     0     0     0     0  ...   \n",
       "0015          0     0     0     0     0     0     0     0     0     0  ...   \n",
       "003           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "004           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "005           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "           1737  1738  1739  1740  1741  1742  1743  1744  1745  1746  \n",
       "000000004     0     0     0     0     0     0     0     0     0     0  \n",
       "0000001       0     0     0     0     0     0     0     0     0     0  \n",
       "000001        0     0     0     0     0     0     0     0     0     0  \n",
       "00001         0     0     0     0     0     0     0     0     0     0  \n",
       "000042        0     0     0     0     0     0     0     0     0     0  \n",
       "00046         0     0     0     0     0     0     0     0     0     0  \n",
       "0015          0     0     0     0     0     0     0     0     0     0  \n",
       "003           0     0     0     0     0     0     0     0     0     0  \n",
       "004           0     0     0     0     0     0     0     0     0     0  \n",
       "005           0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[10 rows x 1747 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the counts for each term into a column titled \"sum\":\n",
    "tfdf['sum'] = tfdf.sum(axis=1)\n",
    "\n",
    "# Isolate the sums in a series\n",
    "sums = pd.Series(tfdf['sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sums.to_csv('../output/tf_main.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a class=\"anchor\" id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into consideration all the peculiarities of the TED talks corpus and the strengths and weaknesses of the various ways to process the corpus, we suggest the following setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A preprocessor to handle parentheticals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laughter is the best medicine.   \n",
      "hold your applause; i'm not done yet.  \n"
     ]
    }
   ],
   "source": [
    "# A Refined Preprocessor --\n",
    "# This one removes two-word phrases/clauses\n",
    "\n",
    "parentheticals = [ \"\\(laughter\\)\", \"\\(applause\\)\", \"\\(music\\)\",  \n",
    "                  \"\\(video\\)\", \"\\(laughs\\)\", \"\\(applause ends\\)\", \n",
    "                  \"\\(audio\\)\", \"\\(singing\\)\", \"\\(music ends\\)\", \n",
    "                  \"\\(cheers\\)\", \"\\(cheering\\)\", \"\\(recording\\)\", \n",
    "                  \"\\(beatboxing\\)\", \"\\(audience\\)\", \"\\(guitar strum\\)\", \n",
    "                  \"\\(clicks metronome\\)\", \"\\(sighs\\)\", \"\\(guitar\\)\", \n",
    "                  \"\\(marimba sounds\\)\", \"\\(drum sounds\\)\" ]\n",
    "\n",
    "def remove_parentheticals(text):\n",
    "    global parentheticals\n",
    "    new_text = text\n",
    "    for rgx_match in parentheticals:\n",
    "        new_text = re.sub(rgx_match, ' ', new_text.lower(), \n",
    "                          flags=re.IGNORECASE)\n",
    "    return new_text\n",
    "\n",
    "test = \"\"\"Laughter is the best medicine. (Laughter) \n",
    "Hold your applause; I'm not done yet. (Applause ends)\"\"\"\n",
    "\n",
    "print(remove_parentheticals(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 29294)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_vec = CountVectorizer( preprocessor = remove_parentheticals,\n",
    "                          max_df = 0.9, min_df = 2 )\n",
    "the_X = the_vec.fit_transform(texts)\n",
    "the_X.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
