We keep getting pulled into too many directions at once. We need to break our work up more clearly and then tackle it in chunks: there's a lot to be done here, and, I think we are frustrating ourselves. I am probably the chief architect of the frustration/confusion on a regular basis, and so I took a walk around the block to try to get a bit of a perspective, to see if I couldn't sort out units of work. The clearest distinction I came up with is the difference between treating the talks as *bags of words* versus as *strings of words*, or **vocabulary** versus **discourse**. 

Clearly, the place to start is with the texts as bags of words, which opens the door to topic modeling and to hedenometrics and a number of methods that are, relatively, closer to where we are now than the methods necessary for discursive analysis. 

So, what we have now is this:

* a **history of publication** for the TED talks starting in 2006 and continuing until the present moment. If we focus on the first 10 years of publishing, which has been our alliterative title for the project, we can hold out the eleventh year, 2017, for testing any predictive outputs which emerge. 

    2006
    2007
    2008
    2009
    2010
    2011
    2012
    2013
    2014
    2015
    2016
    ----
    2017

* Within this history, there is some **slippage** between some performances and their publication. This most notably occurs with talks that occur before 2006, but it might occur elsewhere -- once KK finishes with the metadata merge, we can investigate this further. 
  - How many talks reveal significant slippage? (And what is the threshold for significance?)
  - Is there a pattern to which talks get published sooner or later?

* Within this history we can count talks by (and we can do this by year, too):
  - gender
  - discipline
  - topic
  - etc. (length of talk, vocabulary ... )


## More Historical Matters

* Hedenometrics
* Noah Smith has newspapers from 1980 to 2012. 


## Topic Modeling

* Conventional topic modeling - might lead to a comparison between the topics as derived algorithmically versus those assigned, ostensibly, by humans (in the descriptions). [*We have yet to deal with the task of assigning disciplines ourselves: the TED approach creates too wide a range.*]

* Dynamic LDA for 10+ years of talks.

* [DELAY]: Schmidt's *plot arceology*: Schmidt's method is to generate a "cloud" of topics and then see which topics cluster around which shows and then looking at how these associated topics feature over the length of a text. 