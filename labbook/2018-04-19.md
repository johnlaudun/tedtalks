# Notes on Years


## 2018-04-19

While the work on gendered differences in word usage has not been completed, I also made some notes on a possible way to deal with years. I should note upfront that this work assumes we will be doing topic modeling "later" -- quotation marks are used here because we did topic modeling right away and realized that we needed to understand the corpus more clearly before doing it "for real."

With that noted, it could be argued that a lot of work in the digital humanities begins and also ends with topic modeling -- or, before the arrival of LDA and MALLET, with principle component analysis. Topic modeling is compelling to humanists because it focuses on individual documents within a larger corpus, but in our rush to stay focused on texts, we miss the opportunity to understand other, perhaps broaders, trends that might reveal something about the nature of the texts in a collection.

At the level of vocabulary, what words get used, in TED talks, it would be interested to know if there are frequencies available for other corpora against which we could compare frequencies in these texts. All we would need is a list of words and their relative frequency against which we could compare our list of words and their relative frequencies.

Barring inter-corpora comparisons, there is also the possibility of making intra-corpora comparisons, dividing the texts into piles by, say, gender, as we already have. When I came across an implementation of  dynamic topic modeling in Python using NMF, I realized that much the same thing could be done by years, and, it might be something simple enough for me to do, since it doesn't need to be also broken down by text: we don't really care about texts, except, as was the case in the gendered frequencies, where a single text or small set of texts might unfairly affect results.


## 2018-04-24

Okay, we need to face some truths here: we are two years into this project and both of us are anxious to see some results. Aware of the sunk cost fallacy -- that is, "well, we've already put this much into it" -- I think results are possible and that we can produce at least one, if not two, outputs that we can submit to _Digital Humanities Quarterly_ or to _Cultural Analytics_.

For me, TED talks were never the ends, but the means, the means to producing a set of tools (ways of working) and ideas (ways of thinking) that we could apply to other corpora. TED talks have the advantage of having a high profile: almost anything we do will attract at least a little attention because of that, and, with luck, open some doors to other kinds of projects.

So, let's figure out what all we can do.

First, I think we stick to words for now and not sequences of words larger than n-grams -- so no syntax, no shapes by sentiment, etc. This is the work I really want to get to, so I am going to use my desire to do that work as a means to propel us through this work.

That means, second, we need to outline what we mean by a focus on words. In most instances in the humanities right now, the analysis of words is largely wrapped up in topic models. And usually that topic model is of a large corpora of texts, sometimes over a hundred years in scope. A few studies have used topic models to discuss changes across time, either using static topics of sufficient number, like I did with Jonathan Goodwin, or using dynamic topics. (Reviewing this literature is at the top of my summer reading list, which begins next week.)

But we have a chance to propose analyzing texts for vocabularies before getting to topics. There is, I think, two ways to approach vocabularies: *intracorpus* or *intercorpus*.

**Intracorpus** studies include the work on gender shifts among speakers as well as changes in word use across the years. ... *What else could we do here?*

**Intercorpus** studies would focus on some of the work you have suggested: how do TED talks compare to other corpora? This might reveal if TED talks do have influence and might suggest ways to trace influence. (I don't know if words would work here, purely on frequency, of if we might want to look to n-grams or something that would turn up phrases.) The question I have is: *to what corpora would we compare this one?* >>> I have a note out to a colleague about studies that have done this work and the corpora involved.

So, for now, an initial outline of our first paper might look like this:

1. Vocabularies
    * Overall
    * By year
    * By gender
    * N-grams and phrases of note
   
2. Topic Models
    * Overall (and possible comparison of LDA vs. NMF)
    * By year (as static and as dynamic)
    * By gender

To be clear, in making this outline, I *know* I am forgetting a number of things we've discussed and I know we should include in this round, I just am forgetting them right now.
