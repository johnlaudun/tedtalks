{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Some Useful Sample Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discussion and scripts below are from [Sujit Pal](http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html). I'm copying and pasting them here because his website is a little unreadable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, I read the text of each file, pass the words through gensim's tokenizer and filter out stopwords (from NLTK's English stopword list) using our custom MyCorpus class. These words are used to create a dictionary and BoW corpus, which is serialized to files for use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: bow_model.py\n",
    "import logging\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "def iter_docs(topdir, stoplist):\n",
    "    for fn in os.listdir(topdir):\n",
    "        fin = open(os.path.join(topdir, fn), 'rb')\n",
    "        text = fin.read()\n",
    "        fin.close()\n",
    "        yield (x for x in \n",
    "            gensim.utils.tokenize(text, lowercase=True, deacc=True, \n",
    "                                  errors=\"ignore\")\n",
    "            if x not in stoplist)\n",
    "\n",
    "class MyCorpus(object):\n",
    "\n",
    "    def __init__(self, topdir, stoplist):\n",
    "        self.topdir = topdir\n",
    "        self.stoplist = stoplist\n",
    "        self.dictionary = gensim.corpora.Dictionary(iter_docs(topdir, stoplist))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for tokens in iter_docs(self.topdir, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "                    level=logging.INFO)\n",
    "\n",
    "TEXTS_DIR = \"/path/to/texts/dir\"\n",
    "MODELS_DIR = \"/path/to/models/dir\"\n",
    "\n",
    "stoplist = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "corpus = MyCorpus(TEXTS_DIR, stoplist)\n",
    "\n",
    "corpus.dictionary.save(os.path.join(MODELS_DIR, \"mtsamples.dict\"))\n",
    "gensim.corpora.MmCorpus.serialize(os.path.join(MODELS_DIR, \"mtsamples.mm\"), \n",
    "                                  corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't know (and didn't have an opinion about) how many topics this corpus should yield so I decided to compute this by reducing the features to two dimensions, then clustering the points for different values of K (number of clusters) to find an optimum value. Gensim offers various transforms that allow us to project the vectors in a corpus to a different coordinate space. One such transform is the Latent Semantic Indexing (LSI) transform, which we use to project the original data to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: lsi_model.py\n",
    "import logging\n",
    "import os\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "                    level=logging.INFO)\n",
    "\n",
    "MODELS_DIR = \"/path/to/models/dir\"\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load(os.path.join(MODELS_DIR, \n",
    "                                            \"mtsamples.dict\"))\n",
    "corpus = gensim.corpora.MmCorpus(os.path.join(MODELS_DIR, \"mtsamples.mm\"))\n",
    "\n",
    "tfidf = gensim.models.TfidfModel(corpus, normalize=True)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# project to 2 dimensions for visualization\n",
    "lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n",
    "\n",
    "# write out coordinates to file\n",
    "fcoords = open(os.path.join(MODELS_DIR, \"coords.csv\"), 'wb')\n",
    "for vector in lsi[corpus]:\n",
    "    if len(vector) != 2:\n",
    "        continue\n",
    "    fcoords.write(\"%6.4f\\t%6.4f\\n\" % (vector[0][1], vector[1][1]))\n",
    "fcoords.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I clustered the points in the reduced 2D LSI space using KMeans, varying the number of clusters (K) from 1 to 10. The objective function used is the Inertia of the cluster, defined as the sum of squared differences of each point to its cluster centroid. This value is provided directly from the [Scikit-Learn KMeans algorithm][skk]. Other popular functions include Distortion (Inertia divided by the number of points) or the Percentage of Variance Explained, as described on this [StackOverflow post][so].\n",
    "\n",
    "[skk]: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "[so]: http://stackoverflow.com/questions/6645895/calculating-the-percentage-of-variance-measure-for-k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: num_topics.py\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "MODELS_DIR = \"/path/to/models/dir\"\n",
    "MAX_K = 10\n",
    "\n",
    "X = np.loadtxt(os.path.join(MODELS_DIR, \"coords.csv\"), delimiter=\"\\t\")\n",
    "ks = range(1, MAX_K + 1)\n",
    "\n",
    "inertias = np.zeros(MAX_K)\n",
    "diff = np.zeros(MAX_K)\n",
    "diff2 = np.zeros(MAX_K)\n",
    "diff3 = np.zeros(MAX_K)\n",
    "for k in ks:\n",
    "    kmeans = KMeans(k).fit(X)\n",
    "    inertias[k - 1] = kmeans.inertia_\n",
    "    # first difference    \n",
    "    if k > 1:\n",
    "        diff[k - 1] = inertias[k - 1] - inertias[k - 2]\n",
    "    # second difference\n",
    "    if k > 2:\n",
    "        diff2[k - 1] = diff[k - 1] - diff[k - 2]\n",
    "    # third difference\n",
    "    if k > 3:\n",
    "        diff3[k - 1] = diff2[k - 1] - diff2[k - 2]\n",
    "\n",
    "elbow = np.argmin(diff3[3:]) + 3\n",
    "\n",
    "plt.plot(ks, inertias, \"b*-\")\n",
    "plt.plot(ks[elbow], inertias[elbow], marker='o', markersize=12,\n",
    "         markeredgewidth=2, markeredgecolor='r', markerfacecolor=None)\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plotted the Inertias for different values of K, then used Vincent Granville's approach of calculating the third differential to find an [elbow point][ep]. The elbow point happens here for K=5 and is marked with a red dot in the graph below.\n",
    "\n",
    "[ep]: http://radimrehurek.com/2014/03/data-streaming-in-python-generators-iterators-iterables/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then re-ran the KMeans algorithm with K=5 and generated the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: viz_topics_scatter.py\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "MODELS_DIR = \"/path/to/models/dir\"\n",
    "NUM_TOPICS = 5\n",
    "\n",
    "X = np.loadtxt(os.path.join(MODELS_DIR, \"coords.csv\"), delimiter=\"\\t\")\n",
    "kmeans = KMeans(NUM_TOPICS).fit(X)\n",
    "y = kmeans.labels_\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"m\", \"c\"]\n",
    "for i in range(X.shape[0]):\n",
    "    plt.scatter(X[i][0], X[i][1], c=colors[y[i]], s=10)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then ran the full [LDA transform][ldat] against the BoW corpus, with the number of topics set to 5. As in LSI, I load up the corpus and dictionary from files, then apply the transform to project the documents into the LDA Topic space. Notice that LDA and LSI are conceptually similar in gensim - both are transforms that map one vector space to another.\n",
    "\n",
    "[ldat]: http://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source: lda_model.py\n",
    "import logging\n",
    "import os\n",
    "import gensim\n",
    "\n",
    "MODELS_DIR = \"/path/to/models/dir\"\n",
    "NUM_TOPICS = 5\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "                    level=logging.INFO)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load(os.path.join(MODELS_DIR, \n",
    "                                            \"mtsamples.dict\"))\n",
    "corpus = gensim.corpora.MmCorpus(os.path.join(MODELS_DIR, \"mtsamples.mm\"))\n",
    "\n",
    "# Project to LDA space\n",
    "lda = gensim.models.LdaModel(corpus, id2word=dictionary, num_topics=NUM_TOPICS)\n",
    "lda.print_topics(NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are as follows. As you can see, each topic is made up of a mixture of terms. The top 10 terms from each topic is shown in the output below and covers between 69-80% of the probability space for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wordcloud\n",
    "\n",
    "MODELS_DIR = \"models\"\n",
    "\n",
    "final_topics = open(os.path.join(MODELS_DIR, \"final_topics.txt\"), 'rb')\n",
    "curr_topic = 0\n",
    "for line in final_topics:\n",
    "    line = line.strip()[line.rindex(\":\") + 2:]\n",
    "    scores = [float(x.split(\"*\")[0]) for x in line.split(\" + \")]\n",
    "    words = [x.split(\"*\")[1] for x in line.split(\" + \")]\n",
    "    freqs = []\n",
    "    for word, score in zip(words, scores):\n",
    "        freqs.append((word, score))\n",
    "    elements = wordcloud.fit_words(freqs, width=120, height=120)\n",
    "    wordcloud.draw(elements, \"gs_topic_%d.png\" % (curr_topic),\n",
    "                   width=120, height=120)\n",
    "    curr_topic += 1\n",
    "final_topics.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
