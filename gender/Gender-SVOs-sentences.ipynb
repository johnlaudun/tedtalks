{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import spacy, textacy\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# if needed, run the following in terminal: python3 -m spacy download en_core_web_sm\n",
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('../output/talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('../output/talks_female.csv', index_col='Talk_ID')\n",
    "talks_nog = pd.read_csv('../output/talks_nog.csv', index_col='Talk_ID')\n",
    "talks_all = pd.concat([talks_m, talks_f, talks_nog])\n",
    "\n",
    "# Make this work with the one dataframe approach\n",
    "# print(f\"From our {talks_all.shape[0]}x{talks_all.shape[1]} CSV, \\\n",
    "# we have a list of {len(texts_all)} talks: {len(texts_women)} by women and \\\n",
    "# {len(texts_men)} by men.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>talk_gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you so much, Chris. And it's truly a g...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garf...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Good morning. How are you?    (Laughter)    ...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>About 10 years ago, I took on the task to te...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Thank you. I have to tell you I'm both chall...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>Pat Mitchell: That day, January 8, 2011, began...</td>\n",
       "      <td>No one gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>Alec Soth: So about 10 years ago, I got a ca...</td>\n",
       "      <td>No one gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>(Music)    I went down to St. James Infirmar...</td>\n",
       "      <td>No one gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>(Music)    Amanda Palmer (singing): Ground Con...</td>\n",
       "      <td>No one gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>(Guitar music starts)    (Cheers)    (Cheers...</td>\n",
       "      <td>No one gender</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>992 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text    talk_gender\n",
       "Talk_ID                                                                  \n",
       "1          Thank you so much, Chris. And it's truly a g...           male\n",
       "7          (Music: \"The Sound of Silence,\" Simon & Garf...           male\n",
       "66         Good morning. How are you?    (Laughter)    ...           male\n",
       "92         About 10 years ago, I took on the task to te...           male\n",
       "96         Thank you. I have to tell you I'm both chall...           male\n",
       "...                                                    ...            ...\n",
       "1972     Pat Mitchell: That day, January 8, 2011, began...  No one gender\n",
       "2300       Alec Soth: So about 10 years ago, I got a ca...  No one gender\n",
       "2611       (Music)    I went down to St. James Infirmar...  No one gender\n",
       "2481     (Music)    Amanda Palmer (singing): Ground Con...  No one gender\n",
       "2684       (Guitar music starts)    (Cheers)    (Cheers...  No one gender\n",
       "\n",
       "[992 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of all the columns\n",
    "# talks_m.columns.tolist()\n",
    "drop = ['public_url', 'headline', 'duration', 'published', 'views',\n",
    "        'description', 'tags','event', 'speaker_1', 'speaker_2', 'speaker_3', 'speaker_4']\n",
    "df_origin = talks_all.drop(columns=drop)\n",
    "df_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index => Talk_ID.\n",
      "                                                      text talk_gender   \n",
      "Talk_ID                                                                  \n",
      "1          Thank you so much, Chris. And it's truly a g...        male  \\\n",
      "7          (Music: \"The Sound of Silence,\" Simon & Garf...        male   \n",
      "66         Good morning. How are you?    (Laughter)    ...        male   \n",
      "\n",
      "         text_id  \n",
      "Talk_ID           \n",
      "1              1  \n",
      "7              7  \n",
      "66            66  \n"
     ]
    }
   ],
   "source": [
    "# This leaves us only with: Talk_ID, text, talk_gender\n",
    "# Talk_ID is the dataframe's index!\n",
    "print(f\"Index => {df_origin.index.name}.\")\n",
    "df_origin['text_id'] = df_origin.index\n",
    "print(df_origin.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Texts to Sentences to SVOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texts to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Talk_ID</th>\n",
       "      <th>text</th>\n",
       "      <th>talk_gender</th>\n",
       "      <th>sentence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Thank you so much, Chris.</td>\n",
       "      <td>male</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>And it's truly a great honor to have the oppor...</td>\n",
       "      <td>male</td>\n",
       "      <td>1-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I have been blown away by this conference, and...</td>\n",
       "      <td>male</td>\n",
       "      <td>1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>And I say that sincerely, partly because (Mock...</td>\n",
       "      <td>male</td>\n",
       "      <td>1-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>(Laughter)    Put yourselves in my position.</td>\n",
       "      <td>male</td>\n",
       "      <td>1-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Talk_ID                                               text talk_gender   \n",
       "0        1                          Thank you so much, Chris.        male  \\\n",
       "1        1  And it's truly a great honor to have the oppor...        male   \n",
       "2        1  I have been blown away by this conference, and...        male   \n",
       "3        1  And I say that sincerely, partly because (Mock...        male   \n",
       "4        1       (Laughter)    Put yourselves in my position.        male   \n",
       "\n",
       "  sentence_id  \n",
       "0         1-0  \n",
       "1         1-1  \n",
       "2         1-2  \n",
       "3         1-3  \n",
       "4         1-4  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_origin.copy()\n",
    "\n",
    "# Lowercase the texts\n",
    "df['text'].str.lower()\n",
    "\n",
    "# Break the each text into a list of sentences\n",
    "df['text'] = df['text'].apply(sent_tokenize)\n",
    "\n",
    "# Break each sentence into its own row\n",
    "df = df.explode('text').reset_index().rename(columns={'text_id' : 'row_id'})\n",
    "\n",
    "# Count the rows\n",
    "df['row_id'] = df.groupby('Talk_ID').cumcount()\n",
    "\n",
    "# Create a unique sentence identifier\n",
    "df['sentence_id'] = df['Talk_ID'].astype('str') + \"-\" + df[\"row_id\"].astype('str')\n",
    "\n",
    "# Drop the unneeded column\n",
    "df = df.drop(columns=['row_id'])\n",
    "\n",
    "# Check the results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129256, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences to SVOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m s, v \u001b[38;5;241m=\u001b[39m textacy\u001b[38;5;241m.\u001b[39mextract\u001b[38;5;241m.\u001b[39mtriples\u001b[38;5;241m.\u001b[39msubject_verb_object_triples(nlp(df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(s,v)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "df.iloc[1,1]\n",
    "s, v = textacy.extract.triples.subject_verb_object_triples(nlp(df.iloc[1,1]))\n",
    "print(s,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, v = textacy.extract.triples.subject_verb_object_triples(nlp(df.iloc[2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVOTriple(subject=[I], verb=[have, been, blown], object=[conference]) SVOTriple(subject=[I], verb=[want], object=[to, thank, all, of, you, for, the, many, nice, comments, about, what, I, had, to, say, the, other, night])\n"
     ]
    }
   ],
   "source": [
    "print(s, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_w = list(nlp.pipe(texts_w))\n",
    "\n",
    "def actions(doc_id, doc, svo_list):\n",
    "    svotriples = list(textacy.extract.triples.subject_verb_object_triples(doc))\n",
    "    for item in svotriples:\n",
    "        svo_list.append(\n",
    "            {\n",
    "                'doc': doc_id,\n",
    "                'subject': str(item[0][-1]), \n",
    "                'verb': str(item[1][-1]), \n",
    "                'object': str(item[2])\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two lists\n",
    "all_svos_m = []\n",
    "all_svos_w = []\n",
    "doc_id = 0\n",
    "\n",
    "# Populate the lists with SVO triples\n",
    "for doc in docs_m:\n",
    "    actions(doc_id, doc, all_svos_m)\n",
    "    doc_id += 1\n",
    "\n",
    "for doc in docs_w:\n",
    "    actions(doc_id, doc, all_svos_w)\n",
    "    doc_id += 1\n",
    "\n",
    "# Convert the lists to dataframes\n",
    "svos_w = pd.DataFrame(all_svos_w)\n",
    "svos_m = pd.DataFrame(all_svos_m)\n",
    "\n",
    "print(svos_m.shape[0], svos_w.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files \n",
    "# >>> Commented out once run\n",
    "#svos_w.to_csv(\"../output/svos_w.csv\")\n",
    "#svos_m.to_csv(\"../output/svos_m.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Counts of Sentences vs SVOs <a id='sentences'></a>\n",
    "\n",
    "The code above suggests that 70% of the SVOs in TED talks have `'i', 'we', 'she', 'he', 'they', 'it', 'you'` as their subject. It's not clear, however, how much the SVO pattern represents all sentences in the talks. In this section we explore counting sentences, both through NLTK and spaCy, but also a hand count of a few sample texts to see how well our code is reflecting underlying realities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_w = [ sent_tokenize(text) for text in texts_w ]    \n",
    "sents_m = [ sent_tokenize(text) for text in texts_m ]\n",
    "\n",
    "print(len(sents_w[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_count_m = 0\n",
    "for text in texts_m:\n",
    "    sent_count_m += len(sent_tokenize(text))\n",
    "\n",
    "sent_count_w = 0\n",
    "for text in texts_w:\n",
    "    sent_count_w += len(sent_tokenize(text))\n",
    "\n",
    "print(f\" Female corp sent count: {sent_count_w}\\n Male corp sent count: {sent_count_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That results in the following percentages of SVOs out of the total number of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Female subcorpora: {svos_w.shape[0] / sent_count_w}\")\n",
    "print(f\"Male subcorpora: {svos_m.shape[0] / sent_count_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "Our spaCy documents already exist, so we just need to use the `.sents` method to call the sentences and count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt_cnt_w = 0\n",
    "for doc in docs_w:\n",
    "    snt_cnt_w += len(list(doc.sents))\n",
    "\n",
    "snt_cnt_m = 0\n",
    "for doc in docs_m:\n",
    "    snt_cnt_m += len(list(doc.sents))\n",
    "\n",
    "print(f\"F: {snt_cnt_w}, M: {snt_cnt_m}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F: {svos_w.shape[0] / snt_cnt_w}\")\n",
    "print(f\"M: {svos_m.shape[0] / snt_cnt_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total sentence counts are:\n",
    "```\n",
    "Women - NLTK : 30,799 with SVO ratio of 86%\n",
    "        spaCy: 31,673 with SVO ratio of 84%\n",
    "Men -   NLTK : 96,342 with SVO ratio of 83%\n",
    "        spaCy: 99,039 with SVO ratio of 80%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-SVO Lemmatizing\n",
    "\n",
    "Two possible approaches to lemmatizing verbs in a dataframe:\n",
    "* [How to lemmatise a dataframe column Python - Stack Overflow](https://stackoverflow.com/questions/61987040/how-to-lemmatise-a-dataframe-column-python)\n",
    "* [dataframe - lemmatizing a verb list in a data frame in Python - Stack Overflow](https://stackoverflow.com/questions/72394840/lemmatizing-a-verb-list-in-a-data-frame-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "wnl = WordNetLemmatizer()\n",
    "svos_w.verb = svos_w.verb.map(lambda word: wnl.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svos_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svos_m.verb = svos_m.verb.map(lambda word: wnl.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files \n",
    "# >>> Commented out once run\n",
    "# svos_w.to_csv(\"../output/svos_w_lem.csv\")\n",
    "# svos_m.to_csv(\"../output/svos_m_lem.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
