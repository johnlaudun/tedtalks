{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import spacy, textacy\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# if needed, run the following in terminal: python3 -m spacy download en_core_web_sm\n",
    "# Load the Space pipeline to be used\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data in a gendered partitioned fashion: \n",
    "talks_m = pd.read_csv('../output/talks_male.csv', index_col='Talk_ID')\n",
    "talks_f = pd.read_csv('../output/talks_female.csv', index_col='Talk_ID')\n",
    "talks_nog = pd.read_csv('../output/talks_nog.csv', index_col='Talk_ID')\n",
    "talks_all = pd.concat([talks_m, talks_f, talks_nog])\n",
    "\n",
    "# Make this work with the one dataframe approach\n",
    "# print(f\"From our {talks_all.shape[0]}x{talks_all.shape[1]} CSV, \\\n",
    "# we have a list of {len(texts_all)} talks: {len(texts_women)} by women and \\\n",
    "# {len(texts_men)} by men.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>talk_gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you so much, Chris. And it's truly a g...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garf...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Good morning. How are you?    (Laughter)    ...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>About 10 years ago, I took on the task to te...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Thank you. I have to tell you I'm both chall...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>Pat Mitchell: That day, January 8, 2011, began...</td>\n",
       "      <td>No one gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>Alec Soth: So about 10 years ago, I got a ca...</td>\n",
       "      <td>No one gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>(Music)    I went down to St. James Infirmar...</td>\n",
       "      <td>No one gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>(Music)    Amanda Palmer (singing): Ground Con...</td>\n",
       "      <td>No one gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>(Guitar music starts)    (Cheers)    (Cheers...</td>\n",
       "      <td>No one gender</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>992 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text    talk_gender\n",
       "Talk_ID                                                                  \n",
       "1          Thank you so much, Chris. And it's truly a g...           male\n",
       "7          (Music: \"The Sound of Silence,\" Simon & Garf...           male\n",
       "66         Good morning. How are you?    (Laughter)    ...           male\n",
       "92         About 10 years ago, I took on the task to te...           male\n",
       "96         Thank you. I have to tell you I'm both chall...           male\n",
       "...                                                    ...            ...\n",
       "1972     Pat Mitchell: That day, January 8, 2011, began...  No one gender\n",
       "2300       Alec Soth: So about 10 years ago, I got a ca...  No one gender\n",
       "2611       (Music)    I went down to St. James Infirmar...  No one gender\n",
       "2481     (Music)    Amanda Palmer (singing): Ground Con...  No one gender\n",
       "2684       (Guitar music starts)    (Cheers)    (Cheers...  No one gender\n",
       "\n",
       "[992 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of all the columns\n",
    "# talks_m.columns.tolist()\n",
    "drop = ['public_url', 'headline', 'duration', 'published', 'views',\n",
    "        'description', 'tags','event', 'speaker_1', 'speaker_2', 'speaker_3', 'speaker_4']\n",
    "df_origin = talks_all.drop(columns=drop)\n",
    "df_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index => Talk_ID.\n",
      "                                                      text talk_gender   \n",
      "Talk_ID                                                                  \n",
      "1          Thank you so much, Chris. And it's truly a g...        male  \\\n",
      "7          (Music: \"The Sound of Silence,\" Simon & Garf...        male   \n",
      "66         Good morning. How are you?    (Laughter)    ...        male   \n",
      "\n",
      "         text_id  \n",
      "Talk_ID           \n",
      "1              1  \n",
      "7              7  \n",
      "66            66  \n"
     ]
    }
   ],
   "source": [
    "# This leaves us only with: Talk_ID, text, talk_gender\n",
    "# Talk_ID is the dataframe's index!\n",
    "print(f\"Index => {df_origin.index.name}.\")\n",
    "df_origin['text_id'] = df_origin.index\n",
    "print(df_origin.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Texts to Sentences to SVOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texts to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Talk_ID</th>\n",
       "      <th>text</th>\n",
       "      <th>talk_gender</th>\n",
       "      <th>sentence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Thank you so much, Chris.</td>\n",
       "      <td>male</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>And it's truly a great honor to have the oppor...</td>\n",
       "      <td>male</td>\n",
       "      <td>1-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I have been blown away by this conference, and...</td>\n",
       "      <td>male</td>\n",
       "      <td>1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>And I say that sincerely, partly because (Mock...</td>\n",
       "      <td>male</td>\n",
       "      <td>1-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>(Laughter)    Put yourselves in my position.</td>\n",
       "      <td>male</td>\n",
       "      <td>1-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Talk_ID                                               text talk_gender   \n",
       "0        1                          Thank you so much, Chris.        male  \\\n",
       "1        1  And it's truly a great honor to have the oppor...        male   \n",
       "2        1  I have been blown away by this conference, and...        male   \n",
       "3        1  And I say that sincerely, partly because (Mock...        male   \n",
       "4        1       (Laughter)    Put yourselves in my position.        male   \n",
       "\n",
       "  sentence_id  \n",
       "0         1-0  \n",
       "1         1-1  \n",
       "2         1-2  \n",
       "3         1-3  \n",
       "4         1-4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_origin.copy()\n",
    "\n",
    "# Lowercase the texts\n",
    "df['text'].str.lower()\n",
    "\n",
    "# Break the each text into a list of sentences\n",
    "df['text'] = df['text'].apply(sent_tokenize)\n",
    "\n",
    "# Break each sentence into its own row\n",
    "df = df.explode('text').reset_index().rename(columns={'text_id' : 'row_id'})\n",
    "\n",
    "# Count the rows\n",
    "df['row_id'] = df.groupby('Talk_ID').cumcount()\n",
    "\n",
    "# Create a unique sentence identifier\n",
    "df['sentence_id'] = df['Talk_ID'].astype('str') + \"-\" + df[\"row_id\"].astype('str')\n",
    "\n",
    "# Drop the unneeded column\n",
    "df = df.drop(columns=['row_id'])\n",
    "\n",
    "# Check the results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129256, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the sentence counts are:\n",
    "```\n",
    "Women - NLTK : 30,799 with SVO ratio of 86%\n",
    "        spaCy: 31,673 with SVO ratio of 84%\n",
    "Men -   NLTK : 96,342 with SVO ratio of 83%\n",
    "        spaCy: 99,039 with SVO ratio of 80%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences to SVOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence in the **text** column, we need to populate subject, verb, and object columns. (The object column is actually less necessary since we will now have the sentence right next to the subject and verb pair.)\n",
    "\n",
    "The problem is that some sentences, like Row 2 above generate TWO SVOs, and some sentences like Rows 0, 1, and 8 produce no SVO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL CELLS below are formative attempts to set up a function\n",
    "# that can be plugged into apply() or assign() or used in a for loop\n",
    "# to generate SVOs from the sentence in the text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svo(text):\n",
    "    svotriple = textacy.extract.triples.subject_verb_object_triples(nlp(text))\n",
    "    for item in svotriple:\n",
    "        svo_tuple.append(\n",
    "            {\n",
    "                'subject': , \n",
    "                'verb': str(item[1][-1]), \n",
    "                'object': str(item[2])\n",
    "            }\n",
    "        )\n",
    "    return svo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i,\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     svos \u001b[38;5;241m=\u001b[39m \u001b[43msvo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m svo \u001b[38;5;129;01min\u001b[39;00m svos:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msvos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sentence = df.iloc[i,1]\n",
    "    svos = svo(sentence)\n",
    "    for svo in svos:\n",
    "        print(f\"Row {i}: {svos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two ways to do this:\n",
    "df.assign(b=df['a'], c=df['a'])\n",
    "\n",
    "for row in df:\n",
    "    print(df['sentence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files \n",
    "# >>> Commented out once run\n",
    "#df.to_csv(\"../output/svos-sentences.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-SVO Lemmatizing\n",
    "\n",
    "Two possible approaches to lemmatizing verbs in a dataframe:\n",
    "* [How to lemmatise a dataframe column Python - Stack Overflow](https://stackoverflow.com/questions/61987040/how-to-lemmatise-a-dataframe-column-python)\n",
    "* [dataframe - lemmatizing a verb list in a data frame in Python - Stack Overflow](https://stackoverflow.com/questions/72394840/lemmatizing-a-verb-list-in-a-data-frame-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "wnl = WordNetLemmatizer()\n",
    "svos_w.verb = svos_w.verb.map(lambda word: wnl.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svos_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svos_m.verb = svos_m.verb.map(lambda word: wnl.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files \n",
    "# >>> Commented out once run\n",
    "# svos_w.to_csv(\"../output/svos_w_lem.csv\")\n",
    "# svos_m.to_csv(\"../output/svos_m_lem.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a02b88778d8fb8b6aeb4ad427a942bc53dfcda9d7e3737237788289e0d2d23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": 2,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
