{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED Talk Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge here was to transform 2000+ html files into a managable/meaningful data stores. The first section focused on creating the csv file that would contain information focused on the talks, while the second section focuses on storing information on the speakers and, later, we created a csv of just the descriptions to see if we could create a network based on a jacquard analysis of the talks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Beautiful Soup 1\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "import glob, re, csv\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "the_file = \"/Users/john/Code/tedtalks/test/transcript?language=en.0\"\n",
    "holding = soup(open(the_file).read(), \"lxml\")\n",
    "at = holding.find(\"title\").text\n",
    "author = at[0:at.find(':')]\n",
    "title  = at[at.find(\":\")+1 : at.find(\"|\") ]\n",
    "date = re.sub('[^a-zA-Z0-9]',' ', holding.select_one(\"span.meta__val\").text)\n",
    "length_data = holding.find_all('data', {'class' : 'talk-transcript__para__time'})\n",
    "(m, s) = ([x.get_text().strip(\"\\n\\r\")\n",
    "      for x in length_data if re.search(r\"(?s)\\d{2}:\\d{2}\",\n",
    "                                        x.get_text().strip(\"\\n\\r\"))][-1]).split(':')\n",
    "length = int(m) * 60 + int(s)\n",
    "firstpass = re.sub(r'\\([^)]*\\)', '', holding.find('div', class_ = 'talk-transcript__body').text)\n",
    "text = re.sub('[^a-zA-Z\\.\\']',' ', firstpass)\n",
    "data = [str(author), str(title)]\n",
    "# print(data)\n",
    "with open(\"./output.csv\", \"w\", newline = \"\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        for item in data:\n",
    "            writer.writerow(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting that to work, I imported some boilerplate that has worked for me in the past:\n",
    "\n",
    "```\n",
    "file_list = glob.glob('/Users/john/Code/tedtalks/test/*') # produces list\n",
    "print(file_list)\n",
    "\n",
    "['/Users/john/Code/tedtalks/test/transcript?language=en.0', '/Users/john/Code/tedtalks/test/transcript?language=en.1', '/Users/john/Code/tedtalks/test/transcript?language=en.2']\n",
    "```\n",
    "\n",
    "But handing that off to the initial script proved very tricky. It was clearly time to learn how to `define` functions. With more gratitude than I can express, [Padraic Cunningham][] not only developed the script below, but was also very patient in diagnosing a particular problem I encountered.\n",
    "\n",
    "The script below is available in the repo as `talks_to_csv.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Beautiful Soup 2\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse(the_soup):\n",
    "    # both title and author are can be parsed in separate tags.\n",
    "    author = the_soup.select_one(\"h4.h12.talk-link__speaker\").text.encode(\"utf-8\")\n",
    "    title = the_soup.select_one(\"h4.h9.m5\").text\n",
    "    # just need to strip the text from the date string, no regex needed.\n",
    "    date = the_soup.select_one(\"span.meta__val\").text.strip()      \n",
    "    # we want the last time which is the talk-transcript__para__time previous to the footer.\n",
    "    mn, sec = map(int, the_soup.select_one(\"footer.footer\").find_previous(\"data\", {\n",
    "    \"class\": \"talk-transcript__para__time\"}).text.split(\":\"))\n",
    "    length = (mn * 60 + sec)        \n",
    "    # to ignore (Applause) etc.. we can just pull from the actual text fragment checking for (\n",
    "    text = \" \".join(d.text for d in the_soup.select(\"span.talk-transcript__fragment\") if not d.text.startswith(\"(\"))        \n",
    "    # clean the text\n",
    "    text = re.sub('[^a-zA-Z\\.\\']', ' ', text)\n",
    "    return  author.strip(), title.strip(), date, length, text\n",
    "\n",
    "def to_csv(pth, out):\n",
    "    # open file to write to.\n",
    "    with open(out, \"w\") as out:\n",
    "        # create csv.writer. \n",
    "        wr = csv.writer(out)\n",
    "        # write our headers.\n",
    "        wr.writerow([\"author\", \"title\", \"date\", \"length\", \"text\"])\n",
    "        # get all our html files.\n",
    "        for html in os.listdir(pth):\n",
    "            with open(os.path.join(pth, html)) as f:\n",
    "                # parse the file are write the data to a row.\n",
    "                wr.writerow(parse(BeautifulSoup(f, \"lxml\")))\n",
    "                \n",
    "to_csv(\"./test\",\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix below is to remove parentheses and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def parse(soup):\n",
    "    # both title and author are can be parsed in separate tags.\n",
    "    author = soup.select_one(\"h4.h12.talk-link__speaker\").text\n",
    "    title = soup.select_one(\"h4.h9.m5\").text\n",
    "    # just need to strip the text from the date string, no regex needed.\n",
    "    date = soup.select_one(\"span.meta__val\").text.strip()\n",
    "    # we want the last time which is the talk-transcript__para__time previous to the footer.\n",
    "    mn, sec = map(int, soup.select_one(\"footer.footer\").find_previous(\"data\", {\n",
    "        \"class\": \"talk-transcript__para__time\"}).text.split(\":\"))\n",
    "    length = (mn * 60 + sec)\n",
    "    # to ignore time etc.. we can just pull from the actual text fragment and remove noise i.e (Applause).\n",
    "    text = re.sub(r'\\([^)]*\\)',\"\", \" \".join(d.text for d in soup.select(\"span.talk-transcript__fragment\")))\n",
    "    return author.strip(), title.strip(), date, length, re.sub('[^a-zA-Z\\.\\']', ' ', text)\n",
    "\n",
    "def to_csv(pth, out):\n",
    "    # open file to write to.\n",
    "    with open(out, \"w\") as out:\n",
    "        # create csv.writer.\n",
    "        wr = csv.writer(out)\n",
    "        # write our headers.\n",
    "        wr.writerow([\"author\", \"title\", \"date\", \"length\", \"text\"])\n",
    "        # get all our html files.\n",
    "        for html in os.listdir(pth):\n",
    "            with open(os.path.join(pth, html)) as f:\n",
    "                print(html)\n",
    "                # parse the file are write the data to a row.\n",
    "                wr.writerow(parse(BeautifulSoup(f, \"lxml\")))\n",
    "                \n",
    "to_csv(\"./talks\",\"talks.csv\") # This is to the test directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# name: <h1 class=\"h2 profile-header__name\">\n",
    "# occupation: <div class=\"p2 profile-header__summary\">\n",
    "# intro: <div class=\"profile-intro\">\n",
    "# profile: <div class=\"section section--minor\">\n",
    "\n",
    "\n",
    "def parse(soup):\n",
    "    # both title and views are can be parsed in separate tags.\n",
    "    name = soup.find('h1', {'class' : \"h2 profile-header__name\"}).text.strip('\\n')\n",
    "    occupation = soup.find('div', {'class' : \"p2 profile-header__summary\"}).text.strip('\\n')\n",
    "    intro = soup.find('div', {'class' : \"profile-intro\"}).text.strip('\\n')\n",
    "    profile = soup.find('div', {'class' : \"section section--minor\"}).text.strip('\\n')\n",
    "    return name, occupation, intro, profile\n",
    "\n",
    "def to_csv(pth, out):\n",
    "    # open file to write to.\n",
    "    with open(out, \"w\") as out:\n",
    "        # create csv.writer.\n",
    "        wr = csv.writer(out)\n",
    "        # write our headers.\n",
    "        wr.writerow([\"title\", \"views\", \"descr\"])\n",
    "        # get all our html files.\n",
    "        for html in os.listdir(pth):\n",
    "            with open(os.path.join(pth, html)) as f:\n",
    "                print(html)\n",
    "                # parse the file and write the data to a row.\n",
    "                wr.writerow(parse(BeautifulSoup(f, \"lxml\")))\n",
    "\n",
    "# This is the ACTION:\n",
    "to_csv(\"./html_files/speakers/\",\"speakers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "colnames = ['title', 'views' , 'descr']\n",
    "data = pandas.read_csv('./descriptions.csv', names=colnames)\n",
    "titles = data.title.tolist()\n",
    "views = data.views.tolist()\n",
    "descriptions = data.descr.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
