{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Katherine Kinnaird and I continue our work on the Tedtalks, we have found ourselves drawn to examine more closely the notion of topics, which we both feel have been underexamined in their usage in the humanities. \n",
    "\n",
    "Most humanists use an implementation of LDA, which we will probably also use simply to stay in parallel, but at some point in our work, frustrated with my ability to get LDA to work within Python, I picked up Alan Riddell's DARIAH tutorial and drafted an implementation of NMF topic modeling for our corpus. One advantage I noticed right away, in comparing the results to earlier work I had done with Jonathan Goodwin, was what seemed like a much more stable set of word clusters in the algorithmically-derived topics. \n",
    "\n",
    "Okay, good, but Kinnaird noticed that stopwords kept creeping into the topics and that raised larger issues about how NMF does what it does and that meant, because she's so thorough, backing up a bit and making sure we understand how NMF works.\n",
    "\n",
    "What follows is an experiment to understand the shape and nature of the tf matrix, the tfidf matrix, and the output of the `sklearn` NMF algorithm. Some of this is driven by the following essays:\n",
    "\n",
    "* [Improving the Interpretation of Topic Models][]\n",
    "* [Practical Topic Finding for Short-Sentence Texts][]\n",
    "\n",
    "[Improving the Interpretation of Topic Models]: https://medium.com/towards-data-science/improving-the-interpretation-of-topic-models-87fd2ee3847d\n",
    "[Practical Topic Finding for Short-Sentence Texts]: https://nbviewer.jupyter.org/github/dolaameng/tutorials/blob/master/topic-finding-for-short-texts/topics_for_short_texts.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start our adventure, we needed a small set of texts with sufficient overlap that we could later successfully derive topics from them. I set myself the task of creating ten sentences, each of approximately ten words. Careful readers who take the time to read the sentences themselves will, I hope, forgive me for the texts being rather reflexive in nature, but it did seem appropriate given the overall reflexive nature of this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# The Toy Corpus\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "sentences = [\"Green grow the rushes along the banks of the river.\", \n",
    "             \"The sea refuses no river, and this river is homeward flowing.\", \n",
    "             \"I have run this river during high and low water.\", \n",
    "             \"A river grows larger as it collects water from more tributaries along its course.\", \n",
    "             \"The river regularly broke its banks before the levee system.\", \n",
    "             \"The global economy is too dependent upon a limited number of banks.\", \n",
    "             \"Clinton's campaign was too dependent on the saying about the economy.\", \n",
    "             \"No economy can survive if its banks go broke.\", \n",
    "             \"We are too dependent upon the economy as a measure.\", \n",
    "             \"Many banks went broke when the river topped its banks.\"]\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# The Stopwords for this corpus\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "stopwords = [\"a\", \"about\", \"along\", \"and\", \"are\", \"as\", \"before\", \"can\", \"during\", \n",
    "             \"from\", \"go\", \"have\", \"high\", \"i\", \"if\", \"is\", \"it\", \"its\", \"low\", \n",
    "             \"many\", \"more\", \"no\", \"of\", \"on\", \"run\", \"the\", \"this\", \"too\", \"upon\", \n",
    "             \"was\", \"we\", \"went\", \"when\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each text is simply a sentence in a list of strings. Below the texts is the custom stopword list for this corpus. For those curious, there are a total of 107 tokens in the corpus and 33 stopwords. Once the stopwords are applied, 49 tokens remain for a total of 30 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=\n",
    "# Clean & Tokenize\n",
    "# =-=-=-=-=-=\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "# stopwords = re.split('\\s+', open('../data/tt_stop.txt', 'r').read().lower())\n",
    "\n",
    "# Loop to tokenize, stop, and stem (if needed) texts.\n",
    "tokenized = []\n",
    "for sentence in sentences:   \n",
    "    raw = re.sub(r\"[^\\w\\d'\\s]+\",'', sentence).lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [word for word in tokens if not word in stopwords]\n",
    "    tokenized.append(stopped_tokens)\n",
    "\n",
    "    \n",
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Re-Assemble Texts as Strings from Lists of Words\n",
    "# (because this is what sklearn expects)\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "texts = []\n",
    "for item in tokenized:\n",
    "    the_string = ' '.join(item)\n",
    "    texts.append(the_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green grow rushes banks river\n",
      "sea refuses river river homeward flowing\n",
      "river water\n",
      "river grows larger collects water tributaries course\n",
      "river regularly broke banks levee system\n",
      "global economy dependent limited number banks\n",
      "clinton's campaign dependent saying economy\n",
      "economy survive banks broke\n",
      "dependent economy measure\n",
      "banks broke river topped banks\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 49 tokens representing 30 words.\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(texts).split()\n",
    "print(\"There are {} tokens representing {} words.\"\n",
    "      .format(len(all_words), len(set(all_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore below the possibility of using the sklearn module's built-in tokenization and stopword abilities, but while I continue to teach myself that functionality, we can move ahead with understanding the vectorization of a corpus. \n",
    "\n",
    "There are a lot of ways to turn a series of words into a series of numbers. One of the principle ways of doing so ignores any individuated context for a particular word as we might understand it within the context of a given sentence but simply considers a word in relationship to other words in a text. That is, one way to turn words into numbers is simply to count the words in a text, reducing a text to what is known as a \"bag of words.\" (There's a lot of linguistics and information science that validates this approach, but it will always chafe most humanists.)\n",
    "\n",
    "If we run our corpus of ten sentences through the `CountVectorizer`, we will get a representation of it as a series of numbers, each representing the count of a particular word within a particular text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30)\n",
      "  (0, 21)\t2\n",
      "  (0, 24)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 8)\t1\n",
      "[[1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 2 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# TF\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "tf_data = vec.fit_transform(texts)\n",
    "tf_data_array = tf_data.toarray()\n",
    "print(tf_data.shape)\n",
    "print(tf_data[1])\n",
    "print(tf_data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term frequency vectorizer in sklearn creates a set of words out of all the tokens, like we did above, then counts the number of times a given word occurs within a given text, returning that text as a vector. Thus, the second sentence above:\n",
    "\n",
    "    \"The sea refuses no river, and this river is homeward flowing.\"\n",
    "\n",
    "which we had tokenized and stopworded to become:\n",
    "\n",
    "    sea refuses river river homeward flowing\n",
    "\n",
    "becomes a list of numbers, or a vector, that looks like this:\n",
    "\n",
    "      0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 2 0 0 1 0 0 0 0 0\n",
    "\n",
    "I chose the second sentence because it has a word that occurs twice, *river*, and thus it stands out a bit and, too, it doesn't look like a line of binary. If you stack all ten texts on top of each other, then you get a matrix of 10 rows, each row a text, and 30 columns, each column one of the important, lexical, words.\n",
    "\n",
    "Based on the location of the two, my guess is that the `CountVectorizer` alphabetizes its list of words, which can also be considered as **features** of a text. A quick check of our set of words, sorted alphabetically is our first step in confirmation. (This particular corpus avoids the problem of multiple word forms: so there is no \"bank\" only \"banks\". Stemming is a different, and difficult conversation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banks', 'broke', 'campaign', \"clinton's\", 'collects', 'course', 'dependent', 'economy', 'flowing', 'global', 'green', 'grow', 'grows', 'homeward', 'larger', 'levee', 'limited', 'measure', 'number', 'refuses', 'regularly', 'river', 'rushes', 'saying', 'sea', 'survive', 'system', 'topped', 'tributaries', 'water']\n"
     ]
    }
   ],
   "source": [
    "the_words = list(set(all_words))\n",
    "the_words.sort()\n",
    "print(the_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually get that same list from the vectorizer itself with the `get_feature_names` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banks', 'broke', 'campaign', 'clinton', 'collects', 'course', 'dependent', 'economy', 'flowing', 'global', 'green', 'grow', 'grows', 'homeward', 'larger', 'levee', 'limited', 'measure', 'number', 'refuses', 'regularly', 'river', 'rushes', 'saying', 'sea', 'survive', 'system', 'topped', 'tributaries', 'water']\n"
     ]
    }
   ],
   "source": [
    "features = vec.get_feature_names()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually get the count for each term with the `vocabulary_` method, which reveals that sklearn stores the information as a dictionary with the term as the key and the column index as the value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'water': 29, 'homeward': 13, 'global': 9, 'tributaries': 28, 'dependent': 6, 'grows': 12, 'collects': 4, 'measure': 17, 'grow': 11, 'topped': 27, 'refuses': 19, 'limited': 16, 'larger': 14, 'sea': 24, 'flowing': 8, 'banks': 0, 'survive': 25, 'clinton': 3, 'saying': 23, 'system': 26, 'regularly': 20, 'river': 21, 'course': 5, 'levee': 15, 'green': 10, 'broke': 1, 'campaign': 2, 'rushes': 22, 'number': 18, 'economy': 7}\n"
     ]
    }
   ],
   "source": [
    "occurrences = vec.vocabulary_\n",
    "print(occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also worth pointing out that we can get a count of particular terms within our corpus by feeding the `CountVectorizer` a `vocabulary` argument. Here I've prepopulated a list with three of our terms -- \"sentence\", \"stories\", and \"vocabulary\" -- and the function returns an array which counts only the occurrence of those three terms across all ten texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [0 2 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 1 1]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [1 0 1]\n",
      " [0 0 0]\n",
      " [2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Controlled Vocabulary Count\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "tags = ['banks', 'river', 'broke']\n",
    "cv = CountVectorizer(vocabulary=tags)\n",
    "data = cv.fit_transform(texts).toarray()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been trafficking in raw counts, or occurrences, of a word -- aka term, aka feature -- in our corpus. Chances are, longer, or bigger, texts which simply have more words will have more of any given word, which means they may come to be overvalued (overweighted?) if we rely only on occurrences. Fortunately, we can simply normalize by length of a text to get a value that can be used to compare how often a word is used in relationship to the size of the text across all texts in a corpus. That is, we can get a term's **frequency**.\n",
    "\n",
    "As I was working on this bit of code, I learned that sklearn stores this information in a compressed sparse row matrix, wherein a series of `(text, term)` coordinates are followed by a value. I have captured the first two texts below. (Note the commented out `toarray` method in the second-to-last line. It's there so often in sklearn code that I had come to take it for granted.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21)\t0.707106781187\n",
      "  (0, 24)\t0.353553390593\n",
      "  (0, 19)\t0.353553390593\n",
      "  (0, 13)\t0.353553390593\n",
      "  (0, 8)\t0.353553390593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(tf_data)\n",
    "words_tf = tf_transformer.transform(tf_data)#.toarray()\n",
    "print(words_tf[1]) # values for second setnence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's that same information represented as an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4472136   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.4472136   0.4472136   0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.4472136   0.4472136   0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.35355339  0.          0.          0.          0.\n",
      "   0.35355339  0.          0.          0.          0.          0.\n",
      "   0.35355339  0.          0.70710678  0.          0.          0.35355339\n",
      "   0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "words_tf_array = words_tf.toarray()\n",
    "print(words_tf_array[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also weight words within a document contra the number of times they occur within the overall corpus, thus lowering the value of common words.\n",
    "\n",
    "The original sentence was reduced to 6 tokens of 5 words: river, word 24, occurs twice in this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30)\n",
      "  (0, 8)\t0.44053555152\n",
      "  (0, 13)\t0.44053555152\n",
      "  (0, 19)\t0.44053555152\n",
      "  (0, 24)\t0.44053555152\n",
      "  (0, 21)\t0.472983838401\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# TFIDF\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True) # This defaults to True without `use_idf` passed to it.\n",
    "tfidf_data = tfidf.fit_transform(texts)#.toarray()\n",
    "print(tfidf_data.shape)\n",
    "print(tfidf_data[1]) # values for second sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, again, in the more common form of an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.44053555  0.          0.          0.          0.\n",
      "  0.44053555  0.          0.          0.          0.          0.\n",
      "  0.44053555  0.          0.47298384  0.          0.          0.44053555\n",
      "  0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "tfidf_array = tfidf_data.toarray()\n",
    "print(tfidf_array[1]) # values for second sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banks', 'broke', 'campaign', 'clinton', 'collects', 'course', 'dependent', 'economy', 'flowing', 'global', 'green', 'grow', 'grows', 'homeward', 'larger', 'levee', 'limited', 'measure', 'number', 'refuses', 'regularly', 'river', 'rushes', 'saying', 'sea', 'survive', 'system', 'topped', 'tributaries', 'water']\n"
     ]
    }
   ],
   "source": [
    "tfidf_feature_names = tfidf.get_feature_names()\n",
    "print(tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our older code, it looks like the moment to populate the various facets of the vectorizer is when you are first setting it up:\n",
    "\n",
    "    vectorizer = sk_text.TfidfVectorizer(max_df = max_percent, \n",
    "                                         min_df = min_percent,\n",
    "                                         max_features = n_features,\n",
    "                                         stop_words = tt_stopwords)\n",
    "    tfidf = vectorizer.fit_transform(strungs)\n",
    "    \n",
    "We'll revisit that possibility in a moment in the next section. For now, let's just continue with the exploration of NMF. In the code below, I should note that you can feed the model either the date in the CSR matrix or the full array. The results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF(alpha=0, beta=1, eta=0.1, init=None, l1_ratio=0, max_iter=200,\n",
      "  n_components=2, nls_max_iter=2000, random_state=1, shuffle=False,\n",
      "  solver='cd', sparseness=None, tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "topics = 2\n",
    "model = NMF(n_components = topics,\n",
    "          random_state = 1,\n",
    "          alpha = 0,\n",
    "          l1_ratio = 0).fit(tfidf_array)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40440648  0.        ]\n",
      " [ 0.32888381  0.        ]\n",
      " [ 0.41970176  0.        ]\n",
      " [ 0.29288554  0.        ]\n",
      " [ 0.50431328  0.        ]\n",
      " [ 0.04788035  0.70818205]\n",
      " [ 0.          0.72139888]\n",
      " [ 0.32382609  0.37050242]\n",
      " [ 0.          0.80814917]\n",
      " [ 0.58151533  0.02546852]]\n",
      "[[ 0.62470522  0.46205882  0.          0.          0.09686473  0.09686473\n",
      "   0.          0.05076776  0.1174786   0.          0.17185199  0.17185199\n",
      "   0.09686473  0.1174786   0.09686473  0.19967539  0.          0.          0.\n",
      "   0.1174786   0.19967539  0.69965254  0.17185199  0.          0.1174786\n",
      "   0.15525365  0.19967539  0.26147722  0.09686473  0.37008406]\n",
      " [ 0.14171045  0.06209536  0.19919591  0.19919591  0.          0.\n",
      "   0.52254373  0.54813629  0.          0.1874398   0.          0.          0.\n",
      "   0.          0.          0.          0.1874398   0.31596346  0.1874398\n",
      "   0.          0.          0.          0.          0.19919591  0.\n",
      "   0.11906477  0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "W = model.fit_transform(tfidf_array)\n",
    "H = model.components_\n",
    "\n",
    "print(W)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62470522  0.46205882  0.          0.          0.09686473  0.09686473\n",
      "  0.          0.05076776  0.1174786   0.          0.17185199  0.17185199\n",
      "  0.09686473  0.1174786   0.09686473  0.19967539  0.          0.          0.\n",
      "  0.1174786   0.19967539  0.69965254  0.17185199  0.          0.1174786\n",
      "  0.15525365  0.19967539  0.26147722  0.09686473  0.37008406]\n"
     ]
    }
   ],
   "source": [
    "print(H[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to see if we can get a grid of words-to-topics. The first step, I think, is to get a tuple out of the dictionary of `occurrences` above, sort by the numerical values for position, and then zip that into a pandas dataframe along with the H array above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure `occurrences is a dictionary\n",
    "type(occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a list comprehension on a dictionary to create a list of tuples (Yeah Huh)\n",
    "vocab_list = [(val, key) for key, val in occurrences.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(29, 'water'), (13, 'homeward'), (9, 'global'), (28, 'tributaries'), (6, 'dependent'), (12, 'grows'), (4, 'collects'), (17, 'measure'), (11, 'grow'), (27, 'topped'), (19, 'refuses'), (16, 'limited'), (14, 'larger'), (24, 'sea'), (8, 'flowing'), (0, 'banks'), (25, 'survive'), (3, 'clinton'), (23, 'saying'), (26, 'system'), (20, 'regularly'), (21, 'river'), (5, 'course'), (15, 'levee'), (10, 'green'), (1, 'broke'), (2, 'campaign'), (22, 'rushes'), (18, 'number'), (7, 'economy')]\n"
     ]
    }
   ],
   "source": [
    "# Check the outcome\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort\n",
    "vocab_list.sort() # (reverse=False) for ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'banks'), (1, 'broke'), (2, 'campaign'), (3, 'clinton'), (4, 'collects'), (5, 'course'), (6, 'dependent'), (7, 'economy'), (8, 'flowing'), (9, 'global'), (10, 'green'), (11, 'grow'), (12, 'grows'), (13, 'homeward'), (14, 'larger'), (15, 'levee'), (16, 'limited'), (17, 'measure'), (18, 'number'), (19, 'refuses'), (20, 'regularly'), (21, 'river'), (22, 'rushes'), (23, 'saying'), (24, 'sea'), (25, 'survive'), (26, 'system'), (27, 'topped'), (28, 'tributaries'), (29, 'water')]\n"
     ]
    }
   ],
   "source": [
    "# Check again\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0            1\n",
      "0    0        banks\n",
      "1    1        broke\n",
      "2    2     campaign\n",
      "3    3      clinton\n",
      "4    4     collects\n",
      "5    5       course\n",
      "6    6    dependent\n",
      "7    7      economy\n",
      "8    8      flowing\n",
      "9    9       global\n",
      "10  10        green\n",
      "11  11         grow\n",
      "12  12        grows\n",
      "13  13     homeward\n",
      "14  14       larger\n",
      "15  15        levee\n",
      "16  16      limited\n",
      "17  17      measure\n",
      "18  18       number\n",
      "19  19      refuses\n",
      "20  20    regularly\n",
      "21  21        river\n",
      "22  22       rushes\n",
      "23  23       saying\n",
      "24  24          sea\n",
      "25  25      survive\n",
      "26  26       system\n",
      "27  27       topped\n",
      "28  28  tributaries\n",
      "29  29        water\n"
     ]
    }
   ],
   "source": [
    "# And check\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6247052188788008, 0.4620588181862671, 0.0, 0.0, 0.09686473061048828, 0.09686473061048828, 0.0, 0.05076776195177597, 0.11747860355297193, 0.0, 0.17185198507628535, 0.17185198507628535, 0.09686473061048828, 0.11747860355297193, 0.09686473061048828, 0.19967539374676238, 0.0, 0.0, 0.0, 0.11747860355297193, 0.19967539374676238, 0.6996525364356248, 0.17185198507628535, 0.0, 0.11747860355297193, 0.15525364970917988, 0.19967539374676238, 0.2614772151502125, 0.09686473061048828, 0.3700840630854316] [0.14171045350929856, 0.06209535810168242, 0.19919591161113917, 0.19919591161113917, 0.0, 0.0, 0.5225437281955799, 0.5481362932314325, 0.0, 0.18743980465004675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18743980465004675, 0.31596346419397225, 0.18743980465004675, 0.0, 0.0, 0.0, 0.0, 0.19919591161113917, 0.0, 0.1190647736525464, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "topic_0 = H[0].tolist()\n",
    "topic_1 = H[1].tolist()\n",
    "print(topic_0, topic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Topic 0'] = topic_0\n",
    "df['Topic 1'] = topic_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0            1   Topic 0   Topic 1\n",
      "0    0        banks  0.624705  0.141710\n",
      "1    1        broke  0.462059  0.062095\n",
      "2    2     campaign  0.000000  0.199196\n",
      "3    3      clinton  0.000000  0.199196\n",
      "4    4     collects  0.096865  0.000000\n",
      "5    5       course  0.096865  0.000000\n",
      "6    6    dependent  0.000000  0.522544\n",
      "7    7      economy  0.050768  0.548136\n",
      "8    8      flowing  0.117479  0.000000\n",
      "9    9       global  0.000000  0.187440\n",
      "10  10        green  0.171852  0.000000\n",
      "11  11         grow  0.171852  0.000000\n",
      "12  12        grows  0.096865  0.000000\n",
      "13  13     homeward  0.117479  0.000000\n",
      "14  14       larger  0.096865  0.000000\n",
      "15  15        levee  0.199675  0.000000\n",
      "16  16      limited  0.000000  0.187440\n",
      "17  17      measure  0.000000  0.315963\n",
      "18  18       number  0.000000  0.187440\n",
      "19  19      refuses  0.117479  0.000000\n",
      "20  20    regularly  0.199675  0.000000\n",
      "21  21        river  0.699653  0.000000\n",
      "22  22       rushes  0.171852  0.000000\n",
      "23  23       saying  0.000000  0.199196\n",
      "24  24          sea  0.117479  0.000000\n",
      "25  25      survive  0.155254  0.119065\n",
      "26  26       system  0.199675  0.000000\n",
      "27  27       topped  0.261477  0.000000\n",
      "28  28  tributaries  0.096865  0.000000\n",
      "29  29        water  0.370084  0.000000\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>banks</td>\n",
       "      <td>0.624705</td>\n",
       "      <td>0.141710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>broke</td>\n",
       "      <td>0.462059</td>\n",
       "      <td>0.062095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>campaign</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clinton</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>collects</td>\n",
       "      <td>0.096865</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>course</td>\n",
       "      <td>0.096865</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dependent</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>economy</td>\n",
       "      <td>0.050768</td>\n",
       "      <td>0.548136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>flowing</td>\n",
       "      <td>0.117479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>global</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>green</td>\n",
       "      <td>0.171852</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>grow</td>\n",
       "      <td>0.171852</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>grows</td>\n",
       "      <td>0.096865</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>homeward</td>\n",
       "      <td>0.117479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>larger</td>\n",
       "      <td>0.096865</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>levee</td>\n",
       "      <td>0.199675</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>limited</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>measure</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>number</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>refuses</td>\n",
       "      <td>0.117479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>regularly</td>\n",
       "      <td>0.199675</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>river</td>\n",
       "      <td>0.699653</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rushes</td>\n",
       "      <td>0.171852</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>saying</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sea</td>\n",
       "      <td>0.117479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>survive</td>\n",
       "      <td>0.155254</td>\n",
       "      <td>0.119065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>system</td>\n",
       "      <td>0.199675</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>topped</td>\n",
       "      <td>0.261477</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tributaries</td>\n",
       "      <td>0.096865</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>water</td>\n",
       "      <td>0.370084</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              1   Topic 0   Topic 1\n",
       "0         banks  0.624705  0.141710\n",
       "1         broke  0.462059  0.062095\n",
       "2      campaign  0.000000  0.199196\n",
       "3       clinton  0.000000  0.199196\n",
       "4      collects  0.096865  0.000000\n",
       "5        course  0.096865  0.000000\n",
       "6     dependent  0.000000  0.522544\n",
       "7       economy  0.050768  0.548136\n",
       "8       flowing  0.117479  0.000000\n",
       "9        global  0.000000  0.187440\n",
       "10        green  0.171852  0.000000\n",
       "11         grow  0.171852  0.000000\n",
       "12        grows  0.096865  0.000000\n",
       "13     homeward  0.117479  0.000000\n",
       "14       larger  0.096865  0.000000\n",
       "15        levee  0.199675  0.000000\n",
       "16      limited  0.000000  0.187440\n",
       "17      measure  0.000000  0.315963\n",
       "18       number  0.000000  0.187440\n",
       "19      refuses  0.117479  0.000000\n",
       "20    regularly  0.199675  0.000000\n",
       "21        river  0.699653  0.000000\n",
       "22       rushes  0.171852  0.000000\n",
       "23       saying  0.000000  0.199196\n",
       "24          sea  0.117479  0.000000\n",
       "25      survive  0.155254  0.119065\n",
       "26       system  0.199675  0.000000\n",
       "27       topped  0.261477  0.000000\n",
       "28  tributaries  0.096865  0.000000\n",
       "29        water  0.370084  0.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns={1:'Word'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>banks</td>\n",
       "      <td>0.624705</td>\n",
       "      <td>0.141710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>broke</td>\n",
       "      <td>0.462059</td>\n",
       "      <td>0.062095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>campaign</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clinton</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>collects</td>\n",
       "      <td>0.096865</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>course</td>\n",
       "      <td>0.096865</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dependent</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>economy</td>\n",
       "      <td>0.050768</td>\n",
       "      <td>0.548136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>flowing</td>\n",
       "      <td>0.117479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>global</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>green</td>\n",
       "      <td>0.171852</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>grow</td>\n",
       "      <td>0.171852</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>grows</td>\n",
       "      <td>0.096865</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>homeward</td>\n",
       "      <td>0.117479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>larger</td>\n",
       "      <td>0.096865</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>levee</td>\n",
       "      <td>0.199675</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>limited</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>measure</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>number</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>refuses</td>\n",
       "      <td>0.117479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>regularly</td>\n",
       "      <td>0.199675</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>river</td>\n",
       "      <td>0.699653</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rushes</td>\n",
       "      <td>0.171852</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>saying</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sea</td>\n",
       "      <td>0.117479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>survive</td>\n",
       "      <td>0.155254</td>\n",
       "      <td>0.119065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>system</td>\n",
       "      <td>0.199675</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>topped</td>\n",
       "      <td>0.261477</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tributaries</td>\n",
       "      <td>0.096865</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>water</td>\n",
       "      <td>0.370084</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word   Topic 0   Topic 1\n",
       "0         banks  0.624705  0.141710\n",
       "1         broke  0.462059  0.062095\n",
       "2      campaign  0.000000  0.199196\n",
       "3       clinton  0.000000  0.199196\n",
       "4      collects  0.096865  0.000000\n",
       "5        course  0.096865  0.000000\n",
       "6     dependent  0.000000  0.522544\n",
       "7       economy  0.050768  0.548136\n",
       "8       flowing  0.117479  0.000000\n",
       "9        global  0.000000  0.187440\n",
       "10        green  0.171852  0.000000\n",
       "11         grow  0.171852  0.000000\n",
       "12        grows  0.096865  0.000000\n",
       "13     homeward  0.117479  0.000000\n",
       "14       larger  0.096865  0.000000\n",
       "15        levee  0.199675  0.000000\n",
       "16      limited  0.000000  0.187440\n",
       "17      measure  0.000000  0.315963\n",
       "18       number  0.000000  0.187440\n",
       "19      refuses  0.117479  0.000000\n",
       "20    regularly  0.199675  0.000000\n",
       "21        river  0.699653  0.000000\n",
       "22       rushes  0.171852  0.000000\n",
       "23       saying  0.000000  0.199196\n",
       "24          sea  0.117479  0.000000\n",
       "25      survive  0.155254  0.119065\n",
       "26       system  0.199675  0.000000\n",
       "27       topped  0.261477  0.000000\n",
       "28  tributaries  0.096865  0.000000\n",
       "29        water  0.370084  0.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../outputs/toy_WTT.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This print/display code must be somewhere in the sklearn documentation: both [Alan Riddell][] and [Aneesha Bakharia][] use a version of it.\n",
    "\n",
    "[Alan Riddell]: https://de.dariah.eu/tatom/topic_model_python.html\n",
    "[Aneesha Bakharia]: https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "river banks broke water topped system regularly levee grow rushes\n",
      "Topic 1: \n",
      "economy dependent measure campaign clinton saying limited global number banks\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}: \".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(model, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly what this reveals is that I need to go back and tweak my toy corpus so that the topics can be more clearly seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staying within the `sklearn` ecosystem\n",
    "\n",
    "What if we do all tokenization and normalization in `sklearn`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# This is the bog-standard version from the documentation\n",
    "sk_test_vec = TfidfVectorizer(\n",
    "    lowercase = True, \n",
    "    preprocessor = None, \n",
    "    tokenizer = None, \n",
    "    stop_words = stopwords, \n",
    "    ngram_range = (1, 1), \n",
    "    analyzer = u'word', \n",
    "    max_df = 1.0, \n",
    "    min_df = 1, \n",
    "    max_features = None, \n",
    "    vocabulary = None, \n",
    "    binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7336d43d6f12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msk_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk_test_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/john/Library/Python/3.4/lib/python/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1303\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \"\"\"\n\u001b[0;32m-> 1305\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/john/Library/Python/3.4/lib/python/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/john/Library/Python/3.4/lib/python/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    765\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "sk_test = sk_test_vec.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF(alpha=0.5, beta=1, eta=0.1, init=None, l1_ratio=0.5, max_iter=200,\n",
      "  n_components=2, nls_max_iter=2000, random_state=1, shuffle=False,\n",
      "  solver='cd', sparseness=None, tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "topics = 2\n",
    "sk_test_model = NMF(n_components = topics, \n",
    "                    random_state = 1,\n",
    "                    alpha = 0.5,\n",
    "                    l1_ratio = 0.5,\n",
    "                    solver = 'cd').fit(sk_test_data)\n",
    "print(sk_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = sk_test_model.fit_transform(sk_test_data)\n",
    "H = sk_test_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To my non-mathematical mind, it just seems pretty amazing that you can *see* so clearly the division of the larger matrix by the topics. First, we have the document-topic matrix and we can see the assignment of topics to the documents. (And this is with a terrible set of texts in which it's not clear there are any particular topics -- so, I guess, a lot like life, really.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then here's the words spread across the two topics in a `2 x 31` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "get_feature_names not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-7810f318a0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk_test_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/john/Library/Python/3.4/lib/python/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: get_feature_names not found"
     ]
    }
   ],
   "source": [
    "feature_names = sk_test_data.get_feature_names()\n",
    "print(feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
